{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1d9e12dd-8913-431a-bacb-1c190dffd702",
   "metadata": {},
   "source": [
    "# Video Search App"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee7c83d5-0b90-40eb-9699-eef1192b80e7",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Prerequisites"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a5aef0f-ad9c-4b85-ace2-f438fe6396a2",
   "metadata": {},
   "source": [
    "### Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "248dd395-eb5f-44fb-87eb-157342c80757",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Installation done\n"
     ]
    }
   ],
   "source": [
    "# !pip install -q --upgrade google-cloud-aiplatform \n",
    "!pip install -q --upgrade google-cloud-dlp python-docx --upgrade google-auth\n",
    "!pip install -q --upgrade google-cloud-videointelligence\n",
    "!pip install -q bigframes==0.26\n",
    "!pip install -q --upgrade moviepy\n",
    "!pip install -q --upgrade ffmpeg\n",
    "!pip install -q unidecode\n",
    "\n",
    "!pip install -q --upgrade pytube\n",
    "!pip install -q langchain_google_vertexai\n",
    "!pip install -U -q langchain langchainhub langchain-openai\n",
    "\n",
    "import IPython\n",
    "import time\n",
    "\n",
    "app = IPython.Application.instance()\n",
    "app.kernel.do_shutdown(True)\n",
    "\n",
    "time.sleep(10)\n",
    "\n",
    "print(\"Installation done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68ec4ac1-c0c9-4739-8e9a-16758cfe2ea9",
   "metadata": {},
   "source": [
    "### Once Kernel restarts, run this cell and all below\n",
    "#### Run > Run selected cell and all below"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a932cfa5-9443-4fd1-956c-d462530d38f7",
   "metadata": {},
   "source": [
    "### Setup Environment Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e7c27afe-3cbd-47ad-ae5f-10613999f0a9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain_google_vertexai import VertexAIEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "823d2dd9-21f0-434a-a84c-389e6c46c251",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated property [core/project].\n",
      "Creating gs://jingle-project-414801-formscanner-us-central1/...\n",
      "\u001b[1;31mERROR:\u001b[0m (gcloud.storage.buckets.create) HTTPError 409: Your previous request to create the named bucket succeeded and you already own it.\n",
      "mkdir: cannot create directory ‘content’: File exists\n"
     ]
    }
   ],
   "source": [
    "import socket\n",
    "import re\n",
    "\n",
    "PROJECT_ID = !(gcloud config get-value core/project)\n",
    "PROJECT_ID = PROJECT_ID[0]\n",
    "\n",
    "SVC_ACC = !(gcloud config get-value core/account)\n",
    "SVC_ACC = SVC_ACC[0]\n",
    "\n",
    "PROJECT_NUMBER=str(re.search(r'\\d+', SVC_ACC).group())\n",
    "\n",
    "LOCATION=\"us-central1\"\n",
    "\n",
    "UNIQUE_PREFIX = socket.gethostname()\n",
    "UNIQUE_PREFIX = re.sub('[^A-Za-z0-9]+', '', UNIQUE_PREFIX)\n",
    "\n",
    "UID = UNIQUE_PREFIX\n",
    "\n",
    "BUCKET_NAME = f\"{PROJECT_ID}-{UNIQUE_PREFIX}-{LOCATION}\"\n",
    "\n",
    "BUCKET_URI = f\"gs://{BUCKET_NAME}\"\n",
    "\n",
    "! gcloud config set project $PROJECT_ID\n",
    "! gcloud storage buckets create {BUCKET_URI} --project={PROJECT_ID} --location={LOCATION}\n",
    "\n",
    "!mkdir content\n",
    "!rm -r content/clips/\n",
    "!mkdir content/clips/\n",
    "!rm -r content/frames/\n",
    "!mkdir content/frames/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2829d182-c08a-4493-bd3d-8cbae9b28ded",
   "metadata": {},
   "source": [
    "### Initialize Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d5f14b9a-cc6b-43dd-a3e5-8f5e505154fb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import vertexai\n",
    "from vertexai.language_models import TextEmbeddingModel\n",
    "from vertexai.vision_models import Image as vision_model_Image\n",
    "from vertexai.vision_models import MultiModalEmbeddingModel\n",
    "import glob\n",
    "import os\n",
    "import time\n",
    "from typing import Any, Dict, Iterable, List, Optional, Tuple, Union\n",
    "from IPython.display import display\n",
    "import PIL\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import requests\n",
    "from vertexai.generative_models import (\n",
    "    GenerationConfig,\n",
    "    HarmBlockThreshold,\n",
    "    HarmCategory,\n",
    "    Image,\n",
    ")\n",
    "from vertexai.language_models import TextEmbeddingModel\n",
    "from vertexai.vision_models import Image as vision_model_Image\n",
    "from vertexai.vision_models import MultiModalEmbeddingModel\n",
    "from moviepy.editor import VideoFileClip\n",
    "from google.api_core.client_options import ClientOptions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aa7663a-2782-4f6f-8497-4fa99f86faaa",
   "metadata": {},
   "source": [
    "### Download and upload videos to a bucket"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16d1503e-080b-4748-9dcc-cbe127a778fe",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8ab91cec-29ca-4c02-aff4-048f47f1feaa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pytube\n",
    "from google.cloud import storage\n",
    "import unidecode\n",
    "\n",
    "# -1.1) Fetching from youtube\n",
    "def download_video(video_url):\n",
    "    # Create a YouTube object\n",
    "    yt = YouTube(video_url)\n",
    "    \n",
    "    # Filter to get the highest resolution stream that includes both video and audio\n",
    "    stream = yt.streams.filter(progressive=True, file_extension='mp4').order_by('resolution').desc().first()\n",
    "    \n",
    "    # Download the video\n",
    "    if stream:\n",
    "        print(f\"Going to download {video_url}\")\n",
    "        stream.download()\n",
    "        print(f\"Downloaded into {stream.default_filename}\")\n",
    "    else:\n",
    "        print(\"No suitable stream found\")\n",
    "    return stream.default_filename\n",
    "\n",
    "\n",
    "def upload_file_to_gcs(bucket_name, source_file_path, destination_blob_name):\n",
    "    client = storage.Client()\n",
    "    bucket = client.bucket(bucket_name)\n",
    "    blob = bucket.blob(destination_blob_name)\n",
    "    blob.upload_from_filename(source_file_path)\n",
    "    return blob\n",
    "\n",
    "def upload_file_to_bucket(bucket_name, source_file_path_name): \n",
    "    source_file_path = f\"./{source_file_path_name}\"\n",
    "    destination_blob_name = source_file_path_name.replace(' ', '_')\n",
    "    destination_blob_name = unidecode.unidecode(destination_blob_name)\n",
    "    uploaded_blob         = upload_file_to_gcs(bucket_name, source_file_path, destination_blob_name)\n",
    "    print(f\"File [{source_file_path}] \\nUploaded to [{uploaded_blob.name}] \\nIn bucket [{bucket_name}]\")\n",
    "    return destination_blob_name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab8307e3-dfca-4830-95fd-9e6ae641f35c",
   "metadata": {},
   "source": [
    "### Download and upload Videos from youtube"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b482cf33-c527-49ae-865b-493f31158693",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define the video URL\n",
    "video_url = \"https://www.youtube.com/watch?v=ZBRLpvgcTnM\"\n",
    "\n",
    "# Commented out because Pytube is down (as of Jul 20, 2024). Feel free to uncomment if Pytube is working again\n",
    "# downloaded_video = download_video(video_url)\n",
    "\n",
    "#Comment out below code if Pytube is working:\n",
    "#Change video to your video in content folder\n",
    "downloaded_video = \"./content/googleio.mp4\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85ff44cb-3c98-40c4-b195-8809ce1ed246",
   "metadata": {},
   "source": [
    "### Create csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "15767c04-0e63-48dd-96b0-97064c337e5a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "with open('shots.csv', 'w', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "    field = [\"Id\", \"start_time\", \"end_time\", \"clip_name\", \"frame_name\", \"associated_text\", \"associated_speech\", \"associated_object\", \"description\"]\n",
    "    writer.writerow(field)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62b22119-b765-4145-8c9c-57421bf65836",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Process videos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4391f52b-19ef-4dc4-984e-22fa228d45f0",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Load AI Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ce6a178-0abc-4d0f-8cbe-28e81975ecb8",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8114fb4f-d3cb-4683-a004-03a0b9388bef",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Functions for getting text and image embeddings\n",
    "\n",
    "def get_text_embedding_from_text_embedding_model(\n",
    "    text: str,\n",
    "    return_array: Optional[bool] = False,\n",
    ") -> list:\n",
    "    \"\"\"\n",
    "    Generates a numerical text embedding from a provided text input using a text embedding model.\n",
    "\n",
    "    Args:\n",
    "        text: The input text string to be embedded.\n",
    "        return_array: If True, returns the embedding as a NumPy array.\n",
    "                      If False, returns the embedding as a list. (Default: False)\n",
    "\n",
    "    Returns:\n",
    "        list or numpy.ndarray: A 768-dimensional vector representation of the input text.\n",
    "                               The format (list or NumPy array) depends on the\n",
    "                               value of the 'return_array' parameter.\n",
    "    \"\"\"\n",
    "    embeddings = text_embedding_model.get_embeddings([text])\n",
    "    text_embedding = [embedding.values for embedding in embeddings][0]\n",
    "\n",
    "    if return_array:\n",
    "        text_embedding = np.fromiter(text_embedding, dtype=float)\n",
    "\n",
    "    # returns 768 dimensional array\n",
    "    return text_embedding\n",
    "\n",
    "\n",
    "def get_image_embedding_from_multimodal_embedding_model(\n",
    "    image_uri: str,\n",
    "    embedding_size: int = 512,\n",
    "    text: Optional[str] = None,\n",
    "    return_array: Optional[bool] = False,\n",
    ") -> list:\n",
    "    \"\"\"Extracts an image embedding from a multimodal embedding model.\n",
    "    The function can optionally utilize contextual text to refine the embedding.\n",
    "\n",
    "    Args:\n",
    "        image_uri (str): The URI (Uniform Resource Identifier) of the image to process.\n",
    "        text (Optional[str]): Optional contextual text to guide the embedding generation. Defaults to \"\".\n",
    "        embedding_size (int): The desired dimensionality of the output embedding. Defaults to 512.\n",
    "        return_array (Optional[bool]): If True, returns the embedding as a NumPy array.\n",
    "        Otherwise, returns a list. Defaults to False.\n",
    "\n",
    "    Returns:\n",
    "        list: A list containing the image embedding values. If `return_array` is True, returns a NumPy array instead.\n",
    "    \"\"\"\n",
    "    # image = Image.load_from_file(image_uri)\n",
    "    image = vision_model_Image.load_from_file(image_uri)\n",
    "    embeddings = multimodal_embedding_model.get_embeddings(\n",
    "        image=image, contextual_text=text, dimension=embedding_size\n",
    "    )\n",
    "    image_embedding = embeddings.image_embedding\n",
    "\n",
    "    if return_array:\n",
    "        image_embedding = np.fromiter(image_embedding, dtype=float)\n",
    "\n",
    "    return image_embedding\n",
    "\n",
    "\n",
    "def load_image_bytes(image_path):\n",
    "    \"\"\"Loads an image from a URL or local file path.\n",
    "\n",
    "    Args:\n",
    "        image_uri (str): URL or local file path to the image.\n",
    "\n",
    "    Raises:\n",
    "        ValueError: If `image_uri` is not provided.\n",
    "\n",
    "    Returns:\n",
    "        bytes: Image bytes.\n",
    "    \"\"\"\n",
    "    # Check if the image_uri is provided\n",
    "    if not image_path:\n",
    "        raise ValueError(\"image_uri must be provided.\")\n",
    "\n",
    "    # Load the image from a weblink\n",
    "    if image_path.startswith(\"http://\") or image_path.startswith(\"https://\"):\n",
    "        response = requests.get(image_path, stream=True)\n",
    "        if response.status_code == 200:\n",
    "            return response.content\n",
    "\n",
    "    # Load the image from a local path\n",
    "    else:\n",
    "        return open(image_path, \"rb\").read()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Add colors to the print\n",
    "class Color:\n",
    "    \"\"\"\n",
    "    This class defines a set of color codes that can be used to print text in different colors.\n",
    "    This will be used later to print citations and results to make outputs more readable.\n",
    "    \"\"\"\n",
    "\n",
    "    PURPLE: str = \"\\033[95m\"\n",
    "    CYAN: str = \"\\033[96m\"\n",
    "    DARKCYAN: str = \"\\033[36m\"\n",
    "    BLUE: str = \"\\033[94m\"\n",
    "    GREEN: str = \"\\033[92m\"\n",
    "    YELLOW: str = \"\\033[93m\"\n",
    "    RED: str = \"\\033[91m\"\n",
    "    BOLD: str = \"\\033[1m\"\n",
    "    UNDERLINE: str = \"\\033[4m\"\n",
    "    END: str = \"\\033[0m\"\n",
    "\n",
    "\n",
    "def get_text_overlapping_chunk(\n",
    "    text: str, character_limit: int = 1000, overlap: int = 100\n",
    ") -> dict:\n",
    "    \"\"\"\n",
    "    * Breaks a text document into chunks of a specified size, with an overlap between chunks to preserve context.\n",
    "    * Takes a text document, character limit per chunk, and overlap between chunks as input.\n",
    "    * Returns a dictionary where the keys are chunk numbers and the values are the corresponding text chunks.\n",
    "\n",
    "    Args:\n",
    "        text: The text document to be chunked.\n",
    "        character_limit: Maximum characters per chunk (defaults to 1000).\n",
    "        overlap: Number of overlapping characters between chunks (defaults to 100).\n",
    "\n",
    "    Returns:\n",
    "        A dictionary where keys are chunk numbers and values are the corresponding text chunks.\n",
    "\n",
    "    Raises:\n",
    "        ValueError: If `overlap` is greater than `character_limit`.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    if overlap > character_limit:\n",
    "        raise ValueError(\"Overlap cannot be larger than character limit.\")\n",
    "\n",
    "    # Initialize variables\n",
    "    chunk_number = 1\n",
    "    chunked_text_dict = {}\n",
    "\n",
    "    # Iterate over text with the given limit and overlap\n",
    "    for i in range(0, len(text), character_limit - overlap):\n",
    "        end_index = min(i + character_limit, len(text))\n",
    "        chunk = text[i:end_index]\n",
    "\n",
    "        # Encode and decode for consistent encoding\n",
    "        chunked_text_dict[chunk_number] = chunk.encode(\"ascii\", \"ignore\").decode(\n",
    "            \"utf-8\", \"ignore\"\n",
    "        )\n",
    "\n",
    "        # Increment chunk number\n",
    "        chunk_number += 1\n",
    "\n",
    "    return chunked_text_dict\n",
    "\n",
    "\n",
    "def get_page_text_embedding(text_data: Union[dict, str]) -> dict:\n",
    "    \"\"\"\n",
    "    * Generates embeddings for each text chunk using a specified embedding model.\n",
    "    * Takes a dictionary of text chunks and an embedding size as input.\n",
    "    * Returns a dictionary where the keys are chunk numbers and the values are the corresponding embeddings.\n",
    "\n",
    "    Args:\n",
    "        text_data: Either a dictionary of pre-chunked text or the entire page text.\n",
    "        embedding_size: Size of the embedding vector (defaults to 128).\n",
    "\n",
    "    Returns:\n",
    "        A dictionary where keys are chunk numbers or \"text_embedding\" and values are the corresponding embeddings.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    embeddings_dict = {}\n",
    "\n",
    "    if isinstance(text_data, dict):\n",
    "        # Process each chunk\n",
    "        # print(text_data)\n",
    "        for chunk_number, chunk_value in text_data.items():\n",
    "            text_embd = get_text_embedding_from_text_embedding_model(text=chunk_value)\n",
    "            embeddings_dict[chunk_number] = text_embd\n",
    "    else:\n",
    "        # Process the first 1000 characters of the page text\n",
    "        text_embd = get_text_embedding_from_text_embedding_model(text=text_data)\n",
    "        embeddings_dict[\"text_embedding\"] = text_embd\n",
    "\n",
    "    return embeddings_dict\n",
    "\n",
    "\n",
    "\n",
    "def get_gemini_response(\n",
    "    generative_multimodal_model,\n",
    "    model_input: List[str],\n",
    "    stream: bool = True,\n",
    "    generation_config: Optional[GenerationConfig] = GenerationConfig(\n",
    "        temperature=0.2, max_output_tokens=2048\n",
    "    ),\n",
    "    safety_settings: Optional[dict] = {\n",
    "        HarmCategory.HARM_CATEGORY_HARASSMENT: HarmBlockThreshold.BLOCK_NONE,\n",
    "        HarmCategory.HARM_CATEGORY_HATE_SPEECH: HarmBlockThreshold.BLOCK_NONE,\n",
    "        HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT: HarmBlockThreshold.BLOCK_NONE,\n",
    "        HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT: HarmBlockThreshold.BLOCK_NONE,\n",
    "    },\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    This function generates text in response to a list of model inputs.\n",
    "\n",
    "    Args:\n",
    "        model_input: A list of strings representing the inputs to the model.\n",
    "        stream: Whether to generate the response in a streaming fashion (returning chunks of text at a time) or all at once. Defaults to False.\n",
    "\n",
    "    Returns:\n",
    "        The generated text as a string.\n",
    "    \"\"\"\n",
    "    response = generative_multimodal_model.generate_content(\n",
    "        model_input,\n",
    "        generation_config=generation_config,\n",
    "        stream=stream,\n",
    "        safety_settings=safety_settings,\n",
    "    )\n",
    "    response_list = []\n",
    "\n",
    "    for chunk in response:\n",
    "        try:\n",
    "            response_list.append(chunk.text)\n",
    "        except Exception as e:\n",
    "            print(\n",
    "                \"Exception occurred while calling gemini. Something is wrong. Lower the safety thresholds [safety_settings: BLOCK_NONE ] if not already done. -----\",\n",
    "                e,\n",
    "            )\n",
    "            response_list.append(\"Exception occurred\")\n",
    "            continue\n",
    "    response = \"\".join(response_list)\n",
    "\n",
    "    return response\n",
    "\n",
    "\n",
    "def get_text_metadata_df(\n",
    "    filename: str, text_metadata: Dict[Union[int, str], Dict]\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    This function takes a filename and a text metadata dictionary as input,\n",
    "    iterates over the text metadata dictionary and extracts the text, chunk text,\n",
    "    and chunk embeddings for each page, creates a Pandas DataFrame with the\n",
    "    extracted data, and returns it.\n",
    "\n",
    "    Args:\n",
    "        filename: The filename of the document.\n",
    "        text_metadata: A dictionary containing the text metadata for each page.\n",
    "\n",
    "    Returns:\n",
    "        A Pandas DataFrame with the extracted text, chunk text, and chunk embeddings for each page.\n",
    "    \"\"\"\n",
    "\n",
    "    final_data_text: List[Dict] = []\n",
    "\n",
    "    for key, values in text_metadata.items():\n",
    "        for chunk_number, chunk_text in values[\"chunked_text_dict\"].items():\n",
    "            data: Dict = {}\n",
    "            data[\"file_name\"] = filename\n",
    "            data[\"page_num\"] = int(key) + 1\n",
    "            data[\"text\"] = values[\"text\"]\n",
    "            data[\"text_embedding_page\"] = values[\"page_text_embeddings\"][\n",
    "                \"text_embedding\"\n",
    "            ]\n",
    "            data[\"chunk_number\"] = chunk_number\n",
    "            data[\"chunk_text\"] = chunk_text\n",
    "            data[\"text_embedding_chunk\"] = values[\"chunk_embeddings_dict\"][chunk_number]\n",
    "\n",
    "            final_data_text.append(data)\n",
    "\n",
    "    return_df = pd.DataFrame(final_data_text)\n",
    "    return_df = return_df.reset_index(drop=True)\n",
    "    return return_df\n",
    "\n",
    "\n",
    "def get_image_metadata_df(\n",
    "    filename: str, image_metadata: Dict[Union[int, str], Dict]\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    This function takes a filename and an image metadata dictionary as input,\n",
    "    iterates over the image metadata dictionary and extracts the image path,\n",
    "    image description, and image embeddings for each image, creates a Pandas\n",
    "    DataFrame with the extracted data, and returns it.\n",
    "\n",
    "    Args:\n",
    "        filename: The filename of the document.\n",
    "        image_metadata: A dictionary containing the image metadata for each page.\n",
    "\n",
    "    Returns:\n",
    "        A Pandas DataFrame with the extracted image path, image description, and image embeddings for each image.\n",
    "    \"\"\"\n",
    "\n",
    "    final_data_image: List[Dict] = []\n",
    "    for key, values in image_metadata.items():\n",
    "        for _, image_values in values.items():\n",
    "            data: Dict = {}\n",
    "            data[\"file_name\"] = filename\n",
    "            data[\"page_num\"] = int(key) + 1\n",
    "            data[\"img_num\"] = int(image_values[\"img_num\"])\n",
    "            data[\"img_path\"] = image_values[\"img_path\"]\n",
    "            data[\"img_desc\"] = image_values[\"img_desc\"]\n",
    "            # data[\"mm_embedding_from_text_desc_and_img\"] = image_values[\n",
    "            #     \"mm_embedding_from_text_desc_and_img\"\n",
    "            # ]\n",
    "            data[\"mm_embedding_from_img_only\"] = image_values[\n",
    "                \"mm_embedding_from_img_only\"\n",
    "            ]\n",
    "            data[\"text_embedding_from_image_description\"] = image_values[\n",
    "                \"text_embedding_from_image_description\"\n",
    "            ]\n",
    "            final_data_image.append(data)\n",
    "\n",
    "    return_df = pd.DataFrame(final_data_image).dropna()\n",
    "    return_df = return_df.reset_index(drop=True)\n",
    "    return return_df\n",
    "\n",
    "\n",
    "def get_document_metadata(\n",
    "    generative_multimodal_model,\n",
    "    pdf_folder_path: str,\n",
    "    image_save_dir: str,\n",
    "    image_description_prompt: str,\n",
    "    embedding_size: int = 128,\n",
    "    generation_config: Optional[GenerationConfig] = GenerationConfig(\n",
    "        temperature=0.2, max_output_tokens=2048\n",
    "    ),\n",
    "    safety_settings: Optional[dict] = {\n",
    "        HarmCategory.HARM_CATEGORY_HARASSMENT: HarmBlockThreshold.BLOCK_NONE,\n",
    "        HarmCategory.HARM_CATEGORY_HATE_SPEECH: HarmBlockThreshold.BLOCK_NONE,\n",
    "        HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT: HarmBlockThreshold.BLOCK_NONE,\n",
    "        HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT: HarmBlockThreshold.BLOCK_NONE,\n",
    "    },\n",
    "    add_sleep_after_page: bool = False,\n",
    "    sleep_time_after_page: int = 2,\n",
    ") -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    This function takes a PDF path, an image save directory, an image description prompt, an embedding size, and a text embedding text limit as input.\n",
    "\n",
    "    Args:\n",
    "        pdf_path: The path to the PDF document.\n",
    "        image_save_dir: The directory where extracted images should be saved.\n",
    "        image_description_prompt: A prompt to guide Gemini for generating image descriptions.\n",
    "        embedding_size: The dimensionality of the embedding vectors.\n",
    "        text_emb_text_limit: The maximum number of tokens for text embedding.\n",
    "\n",
    "    Returns:\n",
    "        A tuple containing two DataFrames:\n",
    "            * One DataFrame containing the extracted text metadata for each page of the PDF, including the page text, chunked text dictionaries, and chunk embedding dictionaries.\n",
    "            * Another DataFrame containing the extracted image metadata for each image in the PDF, including the image path, image description, image embeddings (with and without context), and image description text embedding.\n",
    "    \"\"\"\n",
    "\n",
    "    text_metadata_df_final, image_metadata_df_final = pd.DataFrame(), pd.DataFrame()\n",
    "\n",
    "    for pdf_path in glob.glob(pdf_folder_path + \"/*.pdf\"):\n",
    "        print(\n",
    "            \"\\n\\n\",\n",
    "            \"Processing the file: ---------------------------------\",\n",
    "            pdf_path,\n",
    "            \"\\n\\n\",\n",
    "        )\n",
    "\n",
    "        doc, num_pages = get_pdf_doc_object(pdf_path)\n",
    "\n",
    "        file_name = pdf_path.split(\"/\")[-1]\n",
    "\n",
    "        text_metadata: Dict[Union[int, str], Dict] = {}\n",
    "        image_metadata: Dict[Union[int, str], Dict] = {}\n",
    "\n",
    "        for page_num in range(num_pages):\n",
    "            print(f\"Processing page: {page_num + 1}\")\n",
    "\n",
    "            page = doc[page_num]\n",
    "\n",
    "            text = page.get_text()\n",
    "            (\n",
    "                text,\n",
    "                page_text_embeddings_dict,\n",
    "                chunked_text_dict,\n",
    "                chunk_embeddings_dict,\n",
    "            ) = get_chunk_text_metadata(page, embedding_size=embedding_size)\n",
    "\n",
    "            text_metadata[page_num] = {\n",
    "                \"text\": text,\n",
    "                \"page_text_embeddings\": page_text_embeddings_dict,\n",
    "                \"chunked_text_dict\": chunked_text_dict,\n",
    "                \"chunk_embeddings_dict\": chunk_embeddings_dict,\n",
    "            }\n",
    "\n",
    "            images = page.get_images()\n",
    "            image_metadata[page_num] = {}\n",
    "\n",
    "            for image_no, image in enumerate(images):\n",
    "                image_number = int(image_no + 1)\n",
    "                image_metadata[page_num][image_number] = {}\n",
    "\n",
    "                image_for_gemini, image_name = get_image_for_gemini(\n",
    "                    doc, image, image_no, image_save_dir, file_name, page_num\n",
    "                )\n",
    "\n",
    "                print(\n",
    "                    f\"Extracting image from page: {page_num + 1}, saved as: {image_name}\"\n",
    "                )\n",
    "\n",
    "                response = get_gemini_response(\n",
    "                    generative_multimodal_model,\n",
    "                    model_input=[image_description_prompt, image_for_gemini],\n",
    "                    generation_config=generation_config,\n",
    "                    safety_settings=safety_settings,\n",
    "                    stream=True,\n",
    "                )\n",
    "\n",
    "                image_embedding = get_image_embedding_from_multimodal_embedding_model(\n",
    "                    image_uri=image_name,\n",
    "                    embedding_size=embedding_size,\n",
    "                )\n",
    "\n",
    "                image_description_text_embedding = (\n",
    "                    get_text_embedding_from_text_embedding_model(text=response)\n",
    "                )\n",
    "\n",
    "                image_metadata[page_num][image_number] = {\n",
    "                    \"img_num\": image_number,\n",
    "                    \"img_path\": image_name,\n",
    "                    \"img_desc\": response,\n",
    "                    # \"mm_embedding_from_text_desc_and_img\": image_embedding_with_description,\n",
    "                    \"mm_embedding_from_img_only\": image_embedding,\n",
    "                    \"text_embedding_from_image_description\": image_description_text_embedding,\n",
    "                }\n",
    "\n",
    "            # Add sleep to reduce issues with Quota error on API\n",
    "            if add_sleep_after_page:\n",
    "                time.sleep(sleep_time_after_page)\n",
    "                print(\n",
    "                    \"Sleeping for \",\n",
    "                    sleep_time_after_page,\n",
    "                    \"\"\" sec before processing the next page to avoid quota issues. You can disable it: \"add_sleep_after_page = False\"  \"\"\",\n",
    "                )\n",
    "\n",
    "        text_metadata_df = get_text_metadata_df(file_name, text_metadata)\n",
    "        image_metadata_df = get_image_metadata_df(file_name, image_metadata)\n",
    "\n",
    "        text_metadata_df_final = pd.concat(\n",
    "            [text_metadata_df_final, text_metadata_df], axis=0\n",
    "        )\n",
    "        image_metadata_df_final = pd.concat(\n",
    "            [\n",
    "                image_metadata_df_final,\n",
    "                image_metadata_df.drop_duplicates(subset=[\"img_desc\"]),\n",
    "            ],\n",
    "            axis=0,\n",
    "        )\n",
    "\n",
    "        text_metadata_df_final = text_metadata_df_final.reset_index(drop=True)\n",
    "        image_metadata_df_final = image_metadata_df_final.reset_index(drop=True)\n",
    "\n",
    "    return text_metadata_df_final, image_metadata_df_final\n",
    "\n",
    "\n",
    "# Helper Functions\n",
    "\n",
    "\n",
    "def get_user_query_text_embeddings(user_query: str) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Extracts text embeddings for the user query using a text embedding model.\n",
    "\n",
    "    Args:\n",
    "        user_query: The user query text.\n",
    "        embedding_size: The desired embedding size.\n",
    "\n",
    "    Returns:\n",
    "        A NumPy array representing the user query text embedding.\n",
    "    \"\"\"\n",
    "\n",
    "    return get_text_embedding_from_text_embedding_model(user_query)\n",
    "\n",
    "\n",
    "def get_user_query_image_embeddings(\n",
    "    image_query_path: str, embedding_size: int\n",
    ") -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Extracts image embeddings for the user query image using a multimodal embedding model.\n",
    "\n",
    "    Args:\n",
    "        image_query_path: The path to the user query image.\n",
    "        embedding_size: The desired embedding size.\n",
    "\n",
    "    Returns:\n",
    "        A NumPy array representing the user query image embedding.\n",
    "    \"\"\"\n",
    "\n",
    "    return get_image_embedding_from_multimodal_embedding_model(\n",
    "        image_uri=image_query_path, embedding_size=embedding_size\n",
    "    )\n",
    "\n",
    "\n",
    "def get_cosine_score(\n",
    "    dataframe: pd.DataFrame, column_name: str, input_text_embd: np.ndarray\n",
    ") -> float:\n",
    "    \"\"\"\n",
    "    Calculates the cosine similarity between the user query embedding and the dataframe embedding for a specific column.\n",
    "\n",
    "    Args:\n",
    "        dataframe: The pandas DataFrame containing the data to compare against.\n",
    "        column_name: The name of the column containing the embeddings to compare with.\n",
    "        input_text_embd: The NumPy array representing the user query embedding.\n",
    "\n",
    "    Returns:\n",
    "        The cosine similarity score (rounded to two decimal places) between the user query embedding and the dataframe embedding.\n",
    "    \"\"\"\n",
    "\n",
    "    text_cosine_score = round(np.dot(dataframe[column_name], input_text_embd), 2)\n",
    "    return text_cosine_score\n",
    "\n",
    "\n",
    "def print_text_to_image_citation(\n",
    "    final_images: Dict[int, Dict[str, Any]], print_top: bool = True\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Prints a formatted citation for each matched image in a dictionary.\n",
    "\n",
    "    Args:\n",
    "        final_images: A dictionary containing information about matched images,\n",
    "                    with keys as image number and values as dictionaries containing\n",
    "                    image path, page number, page text, cosine similarity score, and image description.\n",
    "        print_top: A boolean flag indicating whether to only print the first citation (True) or all citations (False).\n",
    "\n",
    "    Returns:\n",
    "        None (prints formatted citations to the console).\n",
    "    \"\"\"\n",
    "\n",
    "    color = Color()\n",
    "\n",
    "    # Iterate through the matched image citations\n",
    "    for imageno, image_dict in final_images.items():\n",
    "        # Print the citation header\n",
    "        print(\n",
    "            color.RED + f\"Citation {imageno + 1}:\",\n",
    "            \"Matched image path, page number and page text: \\n\" + color.END,\n",
    "        )\n",
    "\n",
    "        # Print the cosine similarity score\n",
    "        print(color.BLUE + \"score: \" + color.END, image_dict[\"cosine_score\"])\n",
    "\n",
    "        # Print the file_name\n",
    "        print(color.BLUE + \"file_name: \" + color.END, image_dict[\"file_name\"])\n",
    "\n",
    "        # Print the image path\n",
    "        print(color.BLUE + \"path: \" + color.END, image_dict[\"img_path\"])\n",
    "\n",
    "        # Print the page number\n",
    "        print(color.BLUE + \"page number: \" + color.END, image_dict[\"page_num\"])\n",
    "\n",
    "        # Print the page text\n",
    "        print(\n",
    "            color.BLUE + \"page text: \" + color.END, \"\\n\".join(image_dict[\"page_text\"])\n",
    "        )\n",
    "\n",
    "        # Print the image description\n",
    "        print(\n",
    "            color.BLUE + \"image description: \" + color.END,\n",
    "            image_dict[\"image_description\"],\n",
    "        )\n",
    "\n",
    "        # Only print the first citation if print_top is True\n",
    "        if print_top and imageno == 0:\n",
    "            break\n",
    "\n",
    "\n",
    "def print_text_to_text_citation(\n",
    "    final_text: Dict[int, Dict[str, Any]],\n",
    "    print_top: bool = True,\n",
    "    chunk_text: bool = True,\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Prints a formatted citation for each matched text in a dictionary.\n",
    "\n",
    "    Args:\n",
    "        final_text: A dictionary containing information about matched text passages,\n",
    "                    with keys as text number and values as dictionaries containing\n",
    "                    page number, cosine similarity score, chunk number (optional),\n",
    "                    chunk text (optional), and page text (optional).\n",
    "        print_top: A boolean flag indicating whether to only print the first citation (True) or all citations (False).\n",
    "        chunk_text: A boolean flag indicating whether to print individual text chunks (True) or the entire page text (False).\n",
    "\n",
    "    Returns:\n",
    "        None (prints formatted citations to the console).\n",
    "    \"\"\"\n",
    "\n",
    "    color = Color()\n",
    "\n",
    "    # Iterate through the matched text citations\n",
    "    for textno, text_dict in final_text.items():\n",
    "        # Print the citation header\n",
    "        print(color.RED + f\"Citation {textno + 1}:\", \"Matched text: \\n\" + color.END)\n",
    "\n",
    "        # Print the cosine similarity score\n",
    "        print(color.BLUE + \"score: \" + color.END, text_dict[\"cosine_score\"])\n",
    "\n",
    "        # Print the file_name\n",
    "        print(color.BLUE + \"file_name: \" + color.END, text_dict[\"file_name\"])\n",
    "\n",
    "        # Print the page number\n",
    "        print(color.BLUE + \"page_number: \" + color.END, text_dict[\"page_num\"])\n",
    "\n",
    "        # Print the matched text based on the chunk_text argument\n",
    "        if chunk_text:\n",
    "            # Print chunk number and chunk text\n",
    "            print(color.BLUE + \"chunk_number: \" + color.END, text_dict[\"chunk_number\"])\n",
    "            print(color.BLUE + \"chunk_text: \" + color.END, text_dict[\"chunk_text\"])\n",
    "        else:\n",
    "            # Print page text\n",
    "            print(color.BLUE + \"page text: \" + color.END, text_dict[\"page_text\"])\n",
    "\n",
    "        # Only print the first citation if print_top is True\n",
    "        if print_top and textno == 0:\n",
    "            break\n",
    "\n",
    "\n",
    "def get_similar_image_from_query(\n",
    "    text_metadata_df: pd.DataFrame,\n",
    "    image_metadata_df: pd.DataFrame,\n",
    "    query: str = \"\",\n",
    "    image_query_path: str = \"\",\n",
    "    column_name: str = \"\",\n",
    "    image_emb: bool = True,\n",
    "    top_n: int = 3,\n",
    "    embedding_size: int = 128,\n",
    ") -> Dict[int, Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Finds the top N most similar images from a metadata DataFrame based on a text query or an image query.\n",
    "\n",
    "    Args:\n",
    "        text_metadata_df: A Pandas DataFrame containing text metadata associated with the images.\n",
    "        image_metadata_df: A Pandas DataFrame containing image metadata (paths, descriptions, etc.).\n",
    "        query: The text query used for finding similar images (if image_emb is False).\n",
    "        image_query_path: The path to the image used for finding similar images (if image_emb is True).\n",
    "        column_name: The column name in the image_metadata_df containing the image embeddings or captions.\n",
    "        image_emb: Whether to use image embeddings (True) or text captions (False) for comparisons.\n",
    "        top_n: The number of most similar images to return.\n",
    "        embedding_size: The dimensionality of the image embeddings (only used if image_emb is True).\n",
    "\n",
    "    Returns:\n",
    "        A dictionary containing information about the top N most similar images, including cosine scores, image objects, paths, page numbers, text excerpts, and descriptions.\n",
    "    \"\"\"\n",
    "    # Check if image embedding is used\n",
    "    if image_emb:\n",
    "        # Calculate cosine similarity between query image and metadata images\n",
    "        user_query_image_embedding = get_user_query_image_embeddings(\n",
    "            image_query_path, embedding_size\n",
    "        )\n",
    "        cosine_scores = image_metadata_df.apply(\n",
    "            lambda x: get_cosine_score(x, column_name, user_query_image_embedding),\n",
    "            axis=1,\n",
    "        )\n",
    "    else:\n",
    "        # Calculate cosine similarity between query text and metadata image captions\n",
    "        user_query_text_embedding = get_user_query_text_embeddings(query)\n",
    "        cosine_scores = image_metadata_df.apply(\n",
    "            lambda x: get_cosine_score(x, column_name, user_query_text_embedding),\n",
    "            axis=1,\n",
    "        )\n",
    "\n",
    "    # Remove same image comparison score when user image is matched exactly with metadata image\n",
    "    cosine_scores = cosine_scores[cosine_scores < 1.0]\n",
    "\n",
    "    # Get top N cosine scores and their indices\n",
    "    top_n_cosine_scores = cosine_scores.nlargest(top_n).index.tolist()\n",
    "    top_n_cosine_values = cosine_scores.nlargest(top_n).values.tolist()\n",
    "\n",
    "    # Create a dictionary to store matched images and their information\n",
    "    final_images: Dict[int, Dict[str, Any]] = {}\n",
    "\n",
    "    for matched_imageno, indexvalue in enumerate(top_n_cosine_scores):\n",
    "        # Create a sub-dictionary for each matched image\n",
    "        final_images[matched_imageno] = {}\n",
    "\n",
    "        # Store cosine score\n",
    "        final_images[matched_imageno][\"cosine_score\"] = top_n_cosine_values[\n",
    "            matched_imageno\n",
    "        ]\n",
    "\n",
    "        # Load image from file\n",
    "        final_images[matched_imageno][\"image_object\"] = Image.load_from_file(\n",
    "            image_metadata_df.iloc[indexvalue][\"img_path\"]\n",
    "        )\n",
    "\n",
    "        # Add file name\n",
    "        final_images[matched_imageno][\"file_name\"] = image_metadata_df.iloc[indexvalue][\n",
    "            \"file_name\"\n",
    "        ]\n",
    "\n",
    "        # Store image path\n",
    "        final_images[matched_imageno][\"img_path\"] = image_metadata_df.iloc[indexvalue][\n",
    "            \"img_path\"\n",
    "        ]\n",
    "\n",
    "        # Store page number\n",
    "        final_images[matched_imageno][\"page_num\"] = image_metadata_df.iloc[indexvalue][\n",
    "            \"page_num\"\n",
    "        ]\n",
    "\n",
    "        final_images[matched_imageno][\"page_text\"] = np.unique(\n",
    "            text_metadata_df[\n",
    "                (\n",
    "                    text_metadata_df[\"page_num\"].isin(\n",
    "                        [final_images[matched_imageno][\"page_num\"]]\n",
    "                    )\n",
    "                )\n",
    "                & (\n",
    "                    text_metadata_df[\"file_name\"].isin(\n",
    "                        [final_images[matched_imageno][\"file_name\"]]\n",
    "                    )\n",
    "                )\n",
    "            ][\"text\"].values\n",
    "        )\n",
    "\n",
    "        # Store image description\n",
    "        final_images[matched_imageno][\"image_description\"] = image_metadata_df.iloc[\n",
    "            indexvalue\n",
    "        ][\"img_desc\"]\n",
    "\n",
    "    return final_images\n",
    "\n",
    "\n",
    "def get_similar_text_from_query(\n",
    "    query: str,\n",
    "    text_metadata_df: pd.DataFrame,\n",
    "    column_name: str = \"\",\n",
    "    top_n: int = 3,\n",
    "    chunk_text: bool = True,\n",
    "    print_citation: bool = False,\n",
    ") -> Dict[int, Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Finds the top N most similar text passages from a metadata DataFrame based on a text query.\n",
    "\n",
    "    Args:\n",
    "        query: The text query used for finding similar passages.\n",
    "        text_metadata_df: A Pandas DataFrame containing the text metadata to search.\n",
    "        column_name: The column name in the text_metadata_df containing the text embeddings or text itself.\n",
    "        top_n: The number of most similar text passages to return.\n",
    "        embedding_size: The dimensionality of the text embeddings (only used if text embeddings are stored in the column specified by `column_name`).\n",
    "        chunk_text: Whether to return individual text chunks (True) or the entire page text (False).\n",
    "        print_citation: Whether to immediately print formatted citations for the matched text passages (True) or just return the dictionary (False).\n",
    "\n",
    "    Returns:\n",
    "        A dictionary containing information about the top N most similar text passages, including cosine scores, page numbers, chunk numbers (optional), and chunk text or page text (depending on `chunk_text`).\n",
    "\n",
    "    Raises:\n",
    "        KeyError: If the specified `column_name` is not present in the `text_metadata_df`.\n",
    "    \"\"\"\n",
    "\n",
    "    if column_name not in text_metadata_df.columns:\n",
    "        raise KeyError(f\"Column '{column_name}' not found in the 'text_metadata_df'\")\n",
    "\n",
    "    query_vector = get_user_query_text_embeddings(query)\n",
    "\n",
    "\n",
    "    # Calculate cosine similarity between query text and metadata text\n",
    "    cosine_scores = text_metadata_df.apply(\n",
    "        lambda row: get_cosine_score(\n",
    "            row,\n",
    "            column_name,\n",
    "            query_vector,\n",
    "        ),\n",
    "        axis=1,\n",
    "    )\n",
    "\n",
    "    # Get top N cosine scores and their indices\n",
    "    top_n_indices = cosine_scores.nlargest(top_n).index.tolist()\n",
    "    top_n_scores = cosine_scores.nlargest(top_n).values.tolist()\n",
    "\n",
    "    # Create a dictionary to store matched text and their information\n",
    "    final_text: Dict[int, Dict[str, Any]] = {}\n",
    "\n",
    "    for matched_textno, index in enumerate(top_n_indices):\n",
    "        # Create a sub-dictionary for each matched text\n",
    "        final_text[matched_textno] = {}\n",
    "\n",
    "        #Code subIndex  Desc Longer Desc\n",
    "\n",
    "        # Store page number\n",
    "        final_text[matched_textno][\"clip_name\"] = text_metadata_df.iloc[index][\n",
    "            \"clip_name\"\n",
    "        ]\n",
    "\n",
    "        # Store page number\n",
    "        final_text[matched_textno][\"description\t\"] = text_metadata_df.iloc[index][\n",
    "            \"description\"\n",
    "        ]\n",
    "\n",
    "        # Store cosine score\n",
    "        final_text[matched_textno][\"cosine_score\"] = top_n_scores[matched_textno]\n",
    "\n",
    "\n",
    "    return final_text\n",
    "\n",
    "\n",
    "def display_images(\n",
    "    images: Iterable[Union[str, PIL.Image.Image]], resize_ratio: float = 0.5\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Displays a series of images provided as paths or PIL Image objects.\n",
    "\n",
    "    Args:\n",
    "        images: An iterable of image paths or PIL Image objects.\n",
    "        resize_ratio: The factor by which to resize each image (default 0.5).\n",
    "\n",
    "    Returns:\n",
    "        None (displays images using IPython or Jupyter notebook).\n",
    "    \"\"\"\n",
    "\n",
    "    # Convert paths to PIL images if necessary\n",
    "    pil_images = []\n",
    "    for image in images:\n",
    "        if isinstance(image, str):\n",
    "            pil_images.append(PIL.Image.open(image))\n",
    "        else:\n",
    "            pil_images.append(image)\n",
    "\n",
    "    # Resize and display each image\n",
    "    for img in pil_images:\n",
    "        original_width, original_height = img.size\n",
    "        new_width = int(original_width * resize_ratio)\n",
    "        new_height = int(original_height * resize_ratio)\n",
    "        resized_img = img.resize((new_width, new_height))\n",
    "        display(resized_img)\n",
    "        print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9124340e-795e-41fd-a85e-326adef6811a",
   "metadata": {},
   "source": [
    "## Cut the video into subclips of specified duration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "034d4183-2240-4cd1-ad36-8987c9f7da02",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content/clips/clip_0.mp4 has been processed\n",
      "content/clips/clip_1.mp4 has been processed\n",
      "content/clips/clip_2.mp4 has been processed\n",
      "content/clips/clip_3.mp4 has been processed\n",
      "content/clips/clip_4.mp4 has been processed\n",
      "content/clips/clip_5.mp4 has been processed\n",
      "content/clips/clip_6.mp4 has been processed\n",
      "content/clips/clip_7.mp4 has been processed\n",
      "content/clips/clip_8.mp4 has been processed\n",
      "content/clips/clip_9.mp4 has been processed\n",
      "\n",
      "10 clips have been processed.\n"
     ]
    }
   ],
   "source": [
    "import moviepy.editor as mpe\n",
    "import pandas as pd\n",
    "import math\n",
    "\n",
    "# Load the video file\n",
    "clip = mpe.VideoFileClip(downloaded_video)\n",
    "\n",
    "# first result is retrieved because a single video was processed\n",
    "shots_list = []\n",
    "\n",
    "shots_df = pd.read_csv('shots.csv')\n",
    "\n",
    "# e.g. Cut into 60s clips (Change clip_duration to set the desired duration of subclips)\n",
    "clip_duration = 60\n",
    "clip_no = math.ceil(clip.duration/clip_duration)\n",
    "clip_no = int(clip_no)\n",
    "\n",
    "unnamed_id_list = []\n",
    "start_time_list = []\n",
    "end_time_list = []\n",
    "clip_name_list = []\n",
    "frame_name_list = []\n",
    "\n",
    "for i in range(clip_no):\n",
    "    start_time = i * clip_duration\n",
    "    end_time = min(start_time + clip_duration, clip.duration)\n",
    "\n",
    "    text = \"\"\n",
    "    clip_name = \"content/clips/clip_\"+str(i)+\".mp4\"\n",
    "    clip.subclip(start_time, end_time).write_videofile(clip_name, logger=None)\n",
    "    time = (start_time  + end_time)/2\n",
    "    print(f\"{clip_name} has been processed\")\n",
    "   # saving a frame at 2 second\n",
    "    frame_name = \"content/frames/frame_\"+str(i)+\".png\"\n",
    "    clip.save_frame(frame_name, t = float(time))\n",
    "\n",
    "    unnamed_id_list.append(i-0)\n",
    "    start_time_list.append(start_time)\n",
    "    end_time_list.append(end_time)\n",
    "    clip_name_list.append(clip_name)\n",
    "    frame_name_list.append(frame_name)\n",
    "      \n",
    "print(f\"\\n{clip_no} clips have been processed.\")\n",
    "        \n",
    "shots_df['id'] = unnamed_id_list\n",
    "shots_df['start_time'] = start_time_list\n",
    "shots_df['end_time'] = end_time_list\n",
    "shots_df['clip_name'] = clip_name_list\n",
    "shots_df['frame_name'] = frame_name_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b37e7bd0-e978-40b0-9ff6-856bca402dc1",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af9ba5a1-149a-4c52-9389-aaecadd18169",
   "metadata": {},
   "source": [
    "### Load the Video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "165aa04e-2884-48b6-bd18-5ba87a3d7c5f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import base64\n",
    "\n",
    "from vertexai.preview.generative_models import GenerativeModel, Part\n",
    "import base64\n",
    "\n",
    "\n",
    "text = \"\"\n",
    "with open(\"content/clips/clip_0.mp4\", \"rb\") as videoFile:\n",
    "    text = base64.b64encode(videoFile.read())\n",
    "    print(text)\n",
    "\n",
    "video1 = Part.from_data(data=base64.b64decode(text), mime_type=\"video/mp4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24c29443-dbb0-48d7-a753-d1b079954e5e",
   "metadata": {},
   "source": [
    "### Helper Functions to generate embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c06441bc-faa0-44a4-934a-220ecf55d859",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"At Google we are fully in our Gemini era Today all of our 2 billion user products use Gemini Gemini 1.5 Pro is available today in Workspace Labs Let's see how this comes to life with Google Workspace People are always searching their emails in Gmail We're working to make it much more powerful with Gemini Now you can ask Gemini to summarize all recent emails from the school Maybe you were traveling this week and you couldn't make the PTA meeting The recording of the meeting is an hour long If it's from Google Meet you can ask Gemini to give you the highlights People love using photos to search across their life With Gemini we're making that a whole lot easier And Ask Photos can also help you search your memories in a deeper way For example you might be reminiscing \\n\""
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import base64\n",
    "import vertexai\n",
    "import vertexai.preview.generative_models as generative_models\n",
    "\n",
    "\n",
    "#Generate video description using AI Model\n",
    "#Model used is customisable (default = gemini-1.0-pro)\n",
    "\n",
    "def generate(video):\n",
    "    vertexai.init(project=PROJECT_ID, location=\"us-central1\")\n",
    "    model = GenerativeModel(\"gemini-1.5-pro-001\")\n",
    "    responses = model.generate_content(\n",
    "    [\"\"\"You are provided with a short video. First, watch the whole video in full. Then, \n",
    "    describe the contents of the video in chronological order. Register everything that is said in the video as well.\n",
    "    Then, provide detailed descriptions of each subject present in the video,\n",
    "    include their appearance, emotion, actions if possible. Include any names if mentioned.\n",
    "    Go into as much detail as possible.\"\"\", video1],\n",
    "    generation_config={\n",
    "        \"max_output_tokens\": 2048,\n",
    "        \"temperature\": 0,\n",
    "        \"top_p\": 0.4,\n",
    "        \"top_k\": 32\n",
    "    },\n",
    "    safety_settings={\n",
    "          generative_models.HarmCategory.HARM_CATEGORY_HATE_SPEECH: generative_models.HarmBlockThreshold.BLOCK_MEDIUM_AND_ABOVE,\n",
    "          generative_models.HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT: generative_models.HarmBlockThreshold.BLOCK_MEDIUM_AND_ABOVE,\n",
    "          generative_models.HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT: generative_models.HarmBlockThreshold.BLOCK_MEDIUM_AND_ABOVE,\n",
    "          generative_models.HarmCategory.HARM_CATEGORY_HARASSMENT: generative_models.HarmBlockThreshold.BLOCK_MEDIUM_AND_ABOVE,\n",
    "    },\n",
    "    stream=False,\n",
    "    )\n",
    "\n",
    "  #print(responses.text, end=\"\")\n",
    "    return responses.text\n",
    "\n",
    "def generateText(video):\n",
    "    vertexai.init(project=PROJECT_ID, location=\"us-central1\")\n",
    "    model = GenerativeModel(\"gemini-1.5-pro-001\")\n",
    "    responses = model.generate_content(\n",
    "    [\"\"\"You are provided with a short video. Register everything visible in chronological order.\n",
    "    Do not include audio. Only include what you can see visually.\"\"\", video1],\n",
    "    generation_config={\n",
    "        \"max_output_tokens\": 2048,\n",
    "        \"temperature\": 0,\n",
    "        \"top_p\": 0.4,\n",
    "        \"top_k\": 32\n",
    "    },\n",
    "    safety_settings={\n",
    "          generative_models.HarmCategory.HARM_CATEGORY_HATE_SPEECH: generative_models.HarmBlockThreshold.BLOCK_MEDIUM_AND_ABOVE,\n",
    "          generative_models.HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT: generative_models.HarmBlockThreshold.BLOCK_MEDIUM_AND_ABOVE,\n",
    "          generative_models.HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT: generative_models.HarmBlockThreshold.BLOCK_MEDIUM_AND_ABOVE,\n",
    "          generative_models.HarmCategory.HARM_CATEGORY_HARASSMENT: generative_models.HarmBlockThreshold.BLOCK_MEDIUM_AND_ABOVE,\n",
    "    },\n",
    "    stream=False,\n",
    "    )\n",
    "\n",
    "  #print(responses.text, end=\"\")\n",
    "    return responses.text\n",
    "\n",
    "\n",
    "\n",
    "def generateSpeech(video):\n",
    "    \n",
    "    \n",
    "    vertexai.init(project=PROJECT_ID, location=\"us-central1\")\n",
    "    model = GenerativeModel(\"gemini-1.5-pro-001\")\n",
    "    responses = model.generate_content(\n",
    "    [\"\"\"You are provided with a short video. Watch the video in full, and listen to the audio.\n",
    "    Register everything you hear in chronological order. Convert everything into text, word for word.\"\"\", video1],\n",
    "    generation_config={\n",
    "        \"max_output_tokens\": 2048,\n",
    "        \"temperature\": 0,\n",
    "        \"top_p\": 0.4,\n",
    "        \"top_k\": 32\n",
    "    },\n",
    "    safety_settings={\n",
    "          generative_models.HarmCategory.HARM_CATEGORY_HATE_SPEECH: generative_models.HarmBlockThreshold.BLOCK_MEDIUM_AND_ABOVE,\n",
    "          generative_models.HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT: generative_models.HarmBlockThreshold.BLOCK_MEDIUM_AND_ABOVE,\n",
    "          generative_models.HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT: generative_models.HarmBlockThreshold.BLOCK_MEDIUM_AND_ABOVE,\n",
    "          generative_models.HarmCategory.HARM_CATEGORY_HARASSMENT: generative_models.HarmBlockThreshold.BLOCK_MEDIUM_AND_ABOVE,\n",
    "    },\n",
    "    stream=False,\n",
    "    )\n",
    "\n",
    "  #print(responses.text, end=\"\")\n",
    "    return responses.text\n",
    "\n",
    "def generateObjects(video):\n",
    "    vertexai.init(project=PROJECT_ID, location=\"us-central1\")\n",
    "    model = GenerativeModel(\"gemini-1.5-pro-001\")\n",
    "    responses = model.generate_content(\n",
    "    [\"\"\"You are provided with a short video. Describe all the different people or animals that appear throughout the video. Associate them with\n",
    "    a name if mentioned.\n",
    "    Include their appearance, emotions, actions if possible. Do so in great detail.\"\"\", video1],\n",
    "    generation_config={\n",
    "        \"max_output_tokens\": 2048,\n",
    "        \"temperature\": 0,\n",
    "        \"top_p\": 0.4,\n",
    "        \"top_k\": 32\n",
    "    },\n",
    "    safety_settings={\n",
    "          generative_models.HarmCategory.HARM_CATEGORY_HATE_SPEECH: generative_models.HarmBlockThreshold.BLOCK_MEDIUM_AND_ABOVE,\n",
    "          generative_models.HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT: generative_models.HarmBlockThreshold.BLOCK_MEDIUM_AND_ABOVE,\n",
    "          generative_models.HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT: generative_models.HarmBlockThreshold.BLOCK_MEDIUM_AND_ABOVE,\n",
    "          generative_models.HarmCategory.HARM_CATEGORY_HARASSMENT: generative_models.HarmBlockThreshold.BLOCK_MEDIUM_AND_ABOVE,\n",
    "    },\n",
    "    stream=False,\n",
    "    )\n",
    "\n",
    "  #print(responses.text, end=\"\")\n",
    "    return responses.text\n",
    "\n",
    "generateSpeech(video1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e53b6c49-1796-4a68-aa1b-76afd29a6d20",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>start_time</th>\n",
       "      <th>end_time</th>\n",
       "      <th>clip_name</th>\n",
       "      <th>frame_name</th>\n",
       "      <th>associated_text</th>\n",
       "      <th>associated_speech</th>\n",
       "      <th>associated_object</th>\n",
       "      <th>description</th>\n",
       "      <th>id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>content/clips/clip_0.mp4</td>\n",
       "      <td>content/frames/frame_0.png</td>\n",
       "      <td>The video starts with a white screen with a re...</td>\n",
       "      <td>At Google we are fully in our Gemini era Today...</td>\n",
       "      <td>The video features Sundar Pichai, CEO of Googl...</td>\n",
       "      <td>The video starts with a white background with ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>60</td>\n",
       "      <td>120.0</td>\n",
       "      <td>content/clips/clip_1.mp4</td>\n",
       "      <td>content/frames/frame_1.png</td>\n",
       "      <td>The video starts with a man with glasses, dres...</td>\n",
       "      <td>about your daughter Lucia's early milestones Y...</td>\n",
       "      <td>The video features a single speaker, Sundar Pi...</td>\n",
       "      <td>The video starts with a man standing on a stag...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>120</td>\n",
       "      <td>180.0</td>\n",
       "      <td>content/clips/clip_2.mp4</td>\n",
       "      <td>content/frames/frame_2.png</td>\n",
       "      <td>The video starts with a man with glasses and a...</td>\n",
       "      <td>the opportunities we see with AI agents I thin...</td>\n",
       "      <td>The video features two people.\\n\\nThe first pe...</td>\n",
       "      <td>The video starts with Sundar Pichai, CEO of Go...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NaN</td>\n",
       "      <td>180</td>\n",
       "      <td>240.0</td>\n",
       "      <td>content/clips/clip_3.mp4</td>\n",
       "      <td>content/frames/frame_3.png</td>\n",
       "      <td>The video starts with a person holding a phone...</td>\n",
       "      <td>vector IV Do you remember where you saw my gla...</td>\n",
       "      <td>The video starts with a pair of hands holding ...</td>\n",
       "      <td>The video starts with a person holding a smart...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NaN</td>\n",
       "      <td>240</td>\n",
       "      <td>300.0</td>\n",
       "      <td>content/clips/clip_4.mp4</td>\n",
       "      <td>content/frames/frame_4.png</td>\n",
       "      <td>The video starts with a black screen and five ...</td>\n",
       "      <td>video prompts It can capture the details of yo...</td>\n",
       "      <td>The video starts with a fast-tracking shot thr...</td>\n",
       "      <td>The video starts with a black screen with the ...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Id  start_time  end_time                 clip_name  \\\n",
       "0  NaN           0      60.0  content/clips/clip_0.mp4   \n",
       "1  NaN          60     120.0  content/clips/clip_1.mp4   \n",
       "2  NaN         120     180.0  content/clips/clip_2.mp4   \n",
       "3  NaN         180     240.0  content/clips/clip_3.mp4   \n",
       "4  NaN         240     300.0  content/clips/clip_4.mp4   \n",
       "\n",
       "                   frame_name  \\\n",
       "0  content/frames/frame_0.png   \n",
       "1  content/frames/frame_1.png   \n",
       "2  content/frames/frame_2.png   \n",
       "3  content/frames/frame_3.png   \n",
       "4  content/frames/frame_4.png   \n",
       "\n",
       "                                     associated_text  \\\n",
       "0  The video starts with a white screen with a re...   \n",
       "1  The video starts with a man with glasses, dres...   \n",
       "2  The video starts with a man with glasses and a...   \n",
       "3  The video starts with a person holding a phone...   \n",
       "4  The video starts with a black screen and five ...   \n",
       "\n",
       "                                   associated_speech  \\\n",
       "0  At Google we are fully in our Gemini era Today...   \n",
       "1  about your daughter Lucia's early milestones Y...   \n",
       "2  the opportunities we see with AI agents I thin...   \n",
       "3  vector IV Do you remember where you saw my gla...   \n",
       "4  video prompts It can capture the details of yo...   \n",
       "\n",
       "                                   associated_object  \\\n",
       "0  The video features Sundar Pichai, CEO of Googl...   \n",
       "1  The video features a single speaker, Sundar Pi...   \n",
       "2  The video features two people.\\n\\nThe first pe...   \n",
       "3  The video starts with a pair of hands holding ...   \n",
       "4  The video starts with a fast-tracking shot thr...   \n",
       "\n",
       "                                         description  id  \n",
       "0  The video starts with a white background with ...   0  \n",
       "1  The video starts with a man standing on a stag...   1  \n",
       "2  The video starts with Sundar Pichai, CEO of Go...   2  \n",
       "3  The video starts with a person holding a smart...   3  \n",
       "4  The video starts with a black screen with the ...   4  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "associated_desc=[]\n",
    "associated_speech=[]\n",
    "associated_text=[]\n",
    "associated_object=[]\n",
    "\n",
    "\n",
    "\n",
    "for i in range(clip_no):\n",
    "    clip_name = f\"clip_{i}\"\n",
    "    text = \"\"\n",
    "    with open(f\"content/clips/{clip_name}.mp4\", \"rb\") as videoFile:\n",
    "        text = base64.b64encode(videoFile.read())\n",
    "\n",
    "        video1 = Part.from_data(data=base64.b64decode(text), mime_type=\"video/mp4\")\n",
    "        desc = generate(video1)\n",
    "        associated_desc.append(desc)\n",
    "        speechDesc = generateSpeech(video1)\n",
    "        associated_speech.append(speechDesc)\n",
    "        textDesc = generateText(video1)\n",
    "        associated_text.append(textDesc)\n",
    "        objDesc = generateObjects(video1)\n",
    "        associated_object.append(objDesc)\n",
    "        \n",
    "        \n",
    "for i in range(clip_no):\n",
    "    clip_name = f\"clip_{i}\"\n",
    "    text = \"\"\n",
    "    with open(f\"content/clips/{clip_name}.mp4\", \"rb\") as videoFile:\n",
    "        text = base64.b64encode(videoFile.read())\n",
    "\n",
    "        video1 = Part.from_data(data=base64.b64decode(text), mime_type=\"video/mp4\")\n",
    "        \n",
    "\n",
    "shots_df['description'] = associated_desc\n",
    "shots_df['associated_speech'] = associated_speech\n",
    "shots_df['associated_text'] = associated_text\n",
    "shots_df['associated_object'] = associated_object\n",
    "shots_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "963b997d-98b7-4967-8811-e4f40356301f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from vertexai.language_models import TextEmbeddingModel\n",
    "text_embedding_model = TextEmbeddingModel.from_pretrained(\"textembedding-gecko@003\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "36a420c9-b1e9-4679-8756-bbbe629f0ebb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "embeddingsList = []\n",
    "for i in range(clip_no):\n",
    "    string = shots_df['description'][i] + \"\\n\" + shots_df['associated_speech'][i] + \"\\n\" + shots_df['associated_text'][i] + \"\\n\" +  shots_df['associated_object'][i]\n",
    "    embeddings = text_embedding_model.get_embeddings([string])\n",
    "    text_embedding = [embedding.values for embedding in embeddings][0]\n",
    "    embeddingsList.append(text_embedding)\n",
    "shots_df[\"embedding\"] = embeddingsList"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7027e27f-785d-4ced-916f-962df4a44fe2",
   "metadata": {},
   "source": [
    "### Preview Dataframe "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f72e79b8-b71c-4784-8cbd-39b6147a5ef9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>start_time</th>\n",
       "      <th>end_time</th>\n",
       "      <th>clip_name</th>\n",
       "      <th>frame_name</th>\n",
       "      <th>associated_text</th>\n",
       "      <th>associated_speech</th>\n",
       "      <th>associated_object</th>\n",
       "      <th>description</th>\n",
       "      <th>id</th>\n",
       "      <th>embedding</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>content/clips/clip_0.mp4</td>\n",
       "      <td>content/frames/frame_0.png</td>\n",
       "      <td>The video starts with a white screen with a re...</td>\n",
       "      <td>At Google we are fully in our Gemini era Today...</td>\n",
       "      <td>The video features Sundar Pichai, CEO of Googl...</td>\n",
       "      <td>The video starts with a white background with ...</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.024182597175240517, 0.029580209404230118, -...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>60</td>\n",
       "      <td>120.0</td>\n",
       "      <td>content/clips/clip_1.mp4</td>\n",
       "      <td>content/frames/frame_1.png</td>\n",
       "      <td>The video starts with a man with glasses, dres...</td>\n",
       "      <td>about your daughter Lucia's early milestones Y...</td>\n",
       "      <td>The video features a single speaker, Sundar Pi...</td>\n",
       "      <td>The video starts with a man standing on a stag...</td>\n",
       "      <td>1</td>\n",
       "      <td>[0.0018262257799506187, 0.03788071125745773, -...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>120</td>\n",
       "      <td>180.0</td>\n",
       "      <td>content/clips/clip_2.mp4</td>\n",
       "      <td>content/frames/frame_2.png</td>\n",
       "      <td>The video starts with a man with glasses and a...</td>\n",
       "      <td>the opportunities we see with AI agents I thin...</td>\n",
       "      <td>The video features two people.\\n\\nThe first pe...</td>\n",
       "      <td>The video starts with Sundar Pichai, CEO of Go...</td>\n",
       "      <td>2</td>\n",
       "      <td>[0.0018594925059005618, -0.001524347229860723,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NaN</td>\n",
       "      <td>180</td>\n",
       "      <td>240.0</td>\n",
       "      <td>content/clips/clip_3.mp4</td>\n",
       "      <td>content/frames/frame_3.png</td>\n",
       "      <td>The video starts with a person holding a phone...</td>\n",
       "      <td>vector IV Do you remember where you saw my gla...</td>\n",
       "      <td>The video starts with a pair of hands holding ...</td>\n",
       "      <td>The video starts with a person holding a smart...</td>\n",
       "      <td>3</td>\n",
       "      <td>[0.015710072591900826, -0.02628205716609955, -...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NaN</td>\n",
       "      <td>240</td>\n",
       "      <td>300.0</td>\n",
       "      <td>content/clips/clip_4.mp4</td>\n",
       "      <td>content/frames/frame_4.png</td>\n",
       "      <td>The video starts with a black screen and five ...</td>\n",
       "      <td>video prompts It can capture the details of yo...</td>\n",
       "      <td>The video starts with a fast-tracking shot thr...</td>\n",
       "      <td>The video starts with a black screen with the ...</td>\n",
       "      <td>4</td>\n",
       "      <td>[0.017501749098300934, -0.01367997843772173, -...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Id  start_time  end_time                 clip_name  \\\n",
       "0  NaN           0      60.0  content/clips/clip_0.mp4   \n",
       "1  NaN          60     120.0  content/clips/clip_1.mp4   \n",
       "2  NaN         120     180.0  content/clips/clip_2.mp4   \n",
       "3  NaN         180     240.0  content/clips/clip_3.mp4   \n",
       "4  NaN         240     300.0  content/clips/clip_4.mp4   \n",
       "\n",
       "                   frame_name  \\\n",
       "0  content/frames/frame_0.png   \n",
       "1  content/frames/frame_1.png   \n",
       "2  content/frames/frame_2.png   \n",
       "3  content/frames/frame_3.png   \n",
       "4  content/frames/frame_4.png   \n",
       "\n",
       "                                     associated_text  \\\n",
       "0  The video starts with a white screen with a re...   \n",
       "1  The video starts with a man with glasses, dres...   \n",
       "2  The video starts with a man with glasses and a...   \n",
       "3  The video starts with a person holding a phone...   \n",
       "4  The video starts with a black screen and five ...   \n",
       "\n",
       "                                   associated_speech  \\\n",
       "0  At Google we are fully in our Gemini era Today...   \n",
       "1  about your daughter Lucia's early milestones Y...   \n",
       "2  the opportunities we see with AI agents I thin...   \n",
       "3  vector IV Do you remember where you saw my gla...   \n",
       "4  video prompts It can capture the details of yo...   \n",
       "\n",
       "                                   associated_object  \\\n",
       "0  The video features Sundar Pichai, CEO of Googl...   \n",
       "1  The video features a single speaker, Sundar Pi...   \n",
       "2  The video features two people.\\n\\nThe first pe...   \n",
       "3  The video starts with a pair of hands holding ...   \n",
       "4  The video starts with a fast-tracking shot thr...   \n",
       "\n",
       "                                         description  id  \\\n",
       "0  The video starts with a white background with ...   0   \n",
       "1  The video starts with a man standing on a stag...   1   \n",
       "2  The video starts with Sundar Pichai, CEO of Go...   2   \n",
       "3  The video starts with a person holding a smart...   3   \n",
       "4  The video starts with a black screen with the ...   4   \n",
       "\n",
       "                                           embedding  \n",
       "0  [0.024182597175240517, 0.029580209404230118, -...  \n",
       "1  [0.0018262257799506187, 0.03788071125745773, -...  \n",
       "2  [0.0018594925059005618, -0.001524347229860723,...  \n",
       "3  [0.015710072591900826, -0.02628205716609955, -...  \n",
       "4  [0.017501749098300934, -0.01367997843772173, -...  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shots_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "39b69218-7bf9-4586-9823-c45227159e5c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copying file://content/shots.csv [Content-Type=text/csv]...\n",
      "- [1 files][238.6 KiB/238.6 KiB]                                                \n",
      "Operation completed over 1 objects/238.6 KiB.                                    \n"
     ]
    }
   ],
   "source": [
    "shots_df.to_csv('content/shots.csv')\n",
    "# Copy the file to our new bucket.\n",
    "!gsutil cp content/shots.csv {BUCKET_URI}/shots.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e3b1956-0c36-4a87-8a81-715fad307411",
   "metadata": {},
   "source": [
    "### Convert the Dataframe into a JSON file, which will be uploaded into Vector Search Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6152d8e8-6b82-41ba-9a0e-de72d1f4fcd8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "\n",
    "BUCKET_URI_ME=f\"{BUCKET_URI}/embeddings/\"\n",
    "LOCATION = 'us-central1'\n",
    "\n",
    "from google.cloud import aiplatform\n",
    "\n",
    "aiplatform.init(project=PROJECT_ID, location=LOCATION)\n",
    "\n",
    "jsonl_string = shots_df[['id','start_time','end_time','clip_name', 'frame_name', \"description\", \"embedding\"]].to_json(orient=\"records\", lines=True)\n",
    "with open(f\"./videodata.json\", \"w\") as f:\n",
    "    f.write(jsonl_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "683ba9bf-2079-41c0-a51b-2e11e9bca8ac",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copying file://videodata.json [Content-Type=application/json]...\n",
      "- [1 files][134.1 KiB/134.1 KiB]                                                \n",
      "Operation completed over 1 objects/134.1 KiB.                                    \n"
     ]
    }
   ],
   "source": [
    "! gsutil cp videodata.json {BUCKET_URI_ME}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf914184-1944-4996-9de9-f7f1150b129e",
   "metadata": {},
   "source": [
    "## Creating Vector Search Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a7fba175-9018-4dea-b4f1-19ea07180066",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating MatchingEngineIndex\n",
      "Create MatchingEngineIndex backing LRO: projects/777458322107/locations/us-central1/indexes/2911695906350825472/operations/4010764052983709696\n",
      "MatchingEngineIndex created. Resource name: projects/777458322107/locations/us-central1/indexes/2911695906350825472\n",
      "To use this MatchingEngineIndex in another session:\n",
      "index = aiplatform.MatchingEngineIndex('projects/777458322107/locations/us-central1/indexes/2911695906350825472')\n"
     ]
    }
   ],
   "source": [
    "# create Index\n",
    "my_index = aiplatform.MatchingEngineIndex.create_tree_ah_index(\n",
    "    display_name=f\"vs_index_{UID}\",\n",
    "    contents_delta_uri=BUCKET_URI_ME,\n",
    "    dimensions=768,\n",
    "    approximate_neighbors_count=10,\n",
    "    project = PROJECT_ID\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab66e05f-22c0-4225-bb12-06286e016252",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Create Index Endpoint and deploy the Index\n",
    "To use the Index, you need to create an Index Endpoint. It works as a server instance accepting query requests for your Index.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "72c81bf3-11f8-4536-ad33-ad3ad6ebfdd3",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating MatchingEngineIndexEndpoint\n",
      "Create MatchingEngineIndexEndpoint backing LRO: projects/777458322107/locations/us-central1/indexEndpoints/3553529221995298816/operations/3589677487824568320\n",
      "MatchingEngineIndexEndpoint created. Resource name: projects/777458322107/locations/us-central1/indexEndpoints/3553529221995298816\n",
      "To use this MatchingEngineIndexEndpoint in another session:\n",
      "index_endpoint = aiplatform.MatchingEngineIndexEndpoint('projects/777458322107/locations/us-central1/indexEndpoints/3553529221995298816')\n"
     ]
    }
   ],
   "source": [
    "# create IndexEndpoint\n",
    "my_index_endpoint = aiplatform.MatchingEngineIndexEndpoint.create(\n",
    "    display_name=f\"vs_endpoint_{UID}\", public_endpoint_enabled=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77a5f274-a090-4477-b296-5fa2f02f8b35",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deploying index MatchingEngineIndexEndpoint index_endpoint: projects/777458322107/locations/us-central1/indexEndpoints/3553529221995298816\n",
      "Deploy index MatchingEngineIndexEndpoint index_endpoint backing LRO: projects/777458322107/locations/us-central1/indexEndpoints/3553529221995298816/operations/8311701697122533376\n",
      "MatchingEngineIndexEndpoint index_endpoint Deployed index. Resource name: projects/777458322107/locations/us-central1/indexEndpoints/3553529221995298816\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<google.cloud.aiplatform.matching_engine.matching_engine_index_endpoint.MatchingEngineIndexEndpoint object at 0x7fb35ba94a60> \n",
       "resource name: projects/777458322107/locations/us-central1/indexEndpoints/3553529221995298816"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DEPLOYED_INDEX_ID = f\"vs_deployed_{UID}\"\n",
    "# deploy the Index to the Index Endpoint\n",
    "my_index_endpoint.deploy_index(index=my_index, deployed_index_id=DEPLOYED_INDEX_ID)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc16288d-f9f0-4f29-898c-5100fb83284a",
   "metadata": {},
   "source": [
    "#### Go to your Vertex AI console and check that the index is CREATED successfully "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a3ba04c-a5ec-432b-a098-1205d937647f",
   "metadata": {},
   "source": [
    "### Get an existing Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "86531ab4-261a-4f34-924c-cce05d4cb0b9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import tqdm\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "fb63a10d-c8ab-455e-a9af-65c609f280d3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "REGION = LOCATION = \"us-central1\"\n",
    "\n",
    "aiplatform.init(project=PROJECT_ID, location=LOCATION)\n",
    "vertexai.init(project=PROJECT_ID, location=LOCATION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "bbe54c90-9902-4e70-84a1-33f7ef9636c8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Uncomment below to manually insert Index \n",
    "# my_index_id = \"8368299436119425024\"\n",
    "# my_index = aiplatform.MatchingEngineIndex(my_index_id)\n",
    "\n",
    "my_index_name = my_index._gca_resource.name\n",
    "my_index_display_name = my_index.display_name\n",
    "my_index_id = my_index.name.split('/')[-1]\n",
    "\n",
    "my_index_endpoint_name = my_index_endpoint._gca_resource.name\n",
    "my_index_endpoint_display_name = my_index_endpoint.display_name\n",
    "my_index_endpoint_id = my_index_endpoint.name.split('/')[-1]\n",
    "my_index_endpoint_public_domain = my_index_endpoint.public_endpoint_domain_name\n",
    "\n",
    "my_index = aiplatform.MatchingEngineIndex(my_index_name)\n",
    "\n",
    "my_index_endpoint = aiplatform.MatchingEngineIndexEndpoint(my_index_endpoint_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "2829325d-9680-445e-9ad1-4eefd4029f8b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Set variables for the current deployed index.\n",
    "API_ENDPOINT=my_index_endpoint_public_domain\n",
    "INDEX_ENDPOINT=my_index_endpoint_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d4b9293b-ad30-414f-8e6b-b9009475f2a1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain_google_vertexai import VertexAIEmbeddings\n",
    "from google.cloud import aiplatform_v1\n",
    "\n",
    "embeddings = VertexAIEmbeddings(model=\"textembedding-gecko@003\")\n",
    "\n",
    "text_embedding_model = embeddings #TextEmbeddingModel.from_pretrained(\"textembedding-gecko@003\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2df9812c-c1ee-423a-af56-7235da1e1e9a",
   "metadata": {},
   "source": [
    "### Querying using Vector Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "949c0248-1299-4e33-b0dc-f6f34155ec24",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Set your query\n",
    "query = \"Give me the clips where there are animals present\"\n",
    "\n",
    "#Set the number of results you want\n",
    "neighbor_count = 3\n",
    "\n",
    "test_embeddings = embeddings.embed_query(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "24599c7d-0abc-46cb-997a-93c6c7c54c78",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'937796134.us-central1-777458322107.vdb.vertexai.goog'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "API_ENDPOINT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "1e74441a-4107-49a3-987f-e8918df3d45b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "neighbor_count 3\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>start_time</th>\n",
       "      <th>end_time</th>\n",
       "      <th>clip_name</th>\n",
       "      <th>frame_name</th>\n",
       "      <th>associated_text</th>\n",
       "      <th>associated_speech</th>\n",
       "      <th>associated_object</th>\n",
       "      <th>description</th>\n",
       "      <th>id</th>\n",
       "      <th>embedding</th>\n",
       "      <th>distance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>NaN</td>\n",
       "      <td>480</td>\n",
       "      <td>540.00</td>\n",
       "      <td>content/clips/clip_8.mp4</td>\n",
       "      <td>content/frames/frame_8.png</td>\n",
       "      <td>The video starts with a man in a grey shirt an...</td>\n",
       "      <td>what's possible with our latest model Gemini N...</td>\n",
       "      <td>The video features three speakers at a confere...</td>\n",
       "      <td>The video is about Google's new AI technology....</td>\n",
       "      <td>8</td>\n",
       "      <td>[-0.014479124918580055, -0.016115542501211166,...</td>\n",
       "      <td>0.609874</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>NaN</td>\n",
       "      <td>540</td>\n",
       "      <td>598.05</td>\n",
       "      <td>content/clips/clip_9.mp4</td>\n",
       "      <td>content/frames/frame_9.png</td>\n",
       "      <td>The video starts with a wide shot of a stage w...</td>\n",
       "      <td>for people and society We're improving our mod...</td>\n",
       "      <td>The video starts with a man with dark skin giv...</td>\n",
       "      <td>The video starts with a man standing on a stag...</td>\n",
       "      <td>9</td>\n",
       "      <td>[0.005041585303843021, -0.02762093022465706, -...</td>\n",
       "      <td>0.611545</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NaN</td>\n",
       "      <td>180</td>\n",
       "      <td>240.00</td>\n",
       "      <td>content/clips/clip_3.mp4</td>\n",
       "      <td>content/frames/frame_3.png</td>\n",
       "      <td>The video starts with a person holding a phone...</td>\n",
       "      <td>vector IV Do you remember where you saw my gla...</td>\n",
       "      <td>The video starts with a pair of hands holding ...</td>\n",
       "      <td>The video starts with a person holding a smart...</td>\n",
       "      <td>3</td>\n",
       "      <td>[0.015710072591900826, -0.02628205716609955, -...</td>\n",
       "      <td>0.647064</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Id  start_time  end_time                 clip_name  \\\n",
       "8  NaN         480    540.00  content/clips/clip_8.mp4   \n",
       "9  NaN         540    598.05  content/clips/clip_9.mp4   \n",
       "3  NaN         180    240.00  content/clips/clip_3.mp4   \n",
       "\n",
       "                   frame_name  \\\n",
       "8  content/frames/frame_8.png   \n",
       "9  content/frames/frame_9.png   \n",
       "3  content/frames/frame_3.png   \n",
       "\n",
       "                                     associated_text  \\\n",
       "8  The video starts with a man in a grey shirt an...   \n",
       "9  The video starts with a wide shot of a stage w...   \n",
       "3  The video starts with a person holding a phone...   \n",
       "\n",
       "                                   associated_speech  \\\n",
       "8  what's possible with our latest model Gemini N...   \n",
       "9  for people and society We're improving our mod...   \n",
       "3  vector IV Do you remember where you saw my gla...   \n",
       "\n",
       "                                   associated_object  \\\n",
       "8  The video features three speakers at a confere...   \n",
       "9  The video starts with a man with dark skin giv...   \n",
       "3  The video starts with a pair of hands holding ...   \n",
       "\n",
       "                                         description  id  \\\n",
       "8  The video is about Google's new AI technology....   8   \n",
       "9  The video starts with a man standing on a stag...   9   \n",
       "3  The video starts with a person holding a smart...   3   \n",
       "\n",
       "                                           embedding  distance  \n",
       "8  [-0.014479124918580055, -0.016115542501211166,...  0.609874  \n",
       "9  [0.005041585303843021, -0.02762093022465706, -...  0.611545  \n",
       "3  [0.015710072591900826, -0.02628205716609955, -...  0.647064  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "# Configure Vector Search client\n",
    "client_options = {\n",
    "  \"api_endpoint\": API_ENDPOINT\n",
    "}\n",
    "vector_search_client = aiplatform_v1.MatchServiceClient(\n",
    "  client_options=client_options,\n",
    ")\n",
    "# Build FindNeighborsRequest object\n",
    "datapoint = aiplatform_v1.IndexDatapoint(\n",
    "  feature_vector=test_embeddings\n",
    ")\n",
    "\n",
    "query = aiplatform_v1.FindNeighborsRequest.Query(\n",
    "  datapoint=datapoint,\n",
    "  # The number of nearest neighbors to be retrieved\n",
    "  neighbor_count=neighbor_count\n",
    ")\n",
    "\n",
    "request = aiplatform_v1.FindNeighborsRequest(\n",
    "  index_endpoint=INDEX_ENDPOINT,\n",
    "  deployed_index_id=DEPLOYED_INDEX_ID,\n",
    "  # Request can have multiple queries\n",
    "  queries=[query],\n",
    "  return_full_datapoint=False,\n",
    ")\n",
    "\n",
    "# Execute the request\n",
    "response = vector_search_client.find_neighbors(request)\n",
    "\n",
    "df_new = pd.DataFrame()\n",
    "print('neighbor_count', neighbor_count)\n",
    "\n",
    "shots_df['distance'] = None\n",
    "\n",
    "for i in range(0,neighbor_count):\n",
    "    x=response.nearest_neighbors[0]\n",
    "    \n",
    "    df_match = shots_df.loc[shots_df['id'] == int(x.neighbors[i].datapoint.datapoint_id) ]\n",
    "    df_match['distance'] = x.neighbors[i].distance\n",
    "\n",
    "    # Append the matching rows to the new DataFrame\n",
    "    df_new = pd.concat([df_new, df_match])\n",
    "    \n",
    "\n",
    "# Print the new DataFrame\n",
    "df_sorted = df_new.sort_values(by=\"distance\", ascending=True)\n",
    "print(display(df_sorted))\n",
    "\n",
    "#Export DataFrame to CSV file for reference\n",
    "df_new.to_csv(\"results.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "3e4bf09e-68de-4c77-aad6-b6be9ced11fb",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "results = []\n",
    "\n",
    "for result in response.nearest_neighbors:\n",
    "    for neighbor in result.neighbors:\n",
    "        clip_id = int(neighbor.datapoint.datapoint_id)\n",
    "        distance = neighbor.distance\n",
    "        df_match = shots_df.loc[shots_df.index == clip_id]\n",
    "        if not df_match.empty:\n",
    "            match_info = df_match.iloc[0].to_dict()\n",
    "            match_info['distance'] = distance\n",
    "            results.append(match_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "d2a62998-8b5e-48e5-a951-2ac47c8bccbe",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['content/clips/clip_8.mp4', 'content/clips/clip_9.mp4', 'content/clips/clip_3.mp4']\n"
     ]
    }
   ],
   "source": [
    "clipNames = []\n",
    "\n",
    "for i in df_sorted['clip_name']:\n",
    "    clipNames.append(i)\n",
    "    \n",
    "#Display the clips     \n",
    "print(clipNames)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c206cddf-9022-43a1-b0e3-bbbad439bb88",
   "metadata": {},
   "source": [
    "### Display Top 3 Relevant Clips"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "56918019-cd58-4329-b6bb-4eea1bfc224b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<video src=\"content/clips/clip_8.mp4\" controls  width=\"600\" >\n",
       "      Your browser does not support the <code>video</code> element.\n",
       "    </video>"
      ],
      "text/plain": [
       "<IPython.core.display.Video object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<video src=\"content/clips/clip_9.mp4\" controls  width=\"600\" >\n",
       "      Your browser does not support the <code>video</code> element.\n",
       "    </video>"
      ],
      "text/plain": [
       "<IPython.core.display.Video object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<video src=\"content/clips/clip_3.mp4\" controls  width=\"600\" >\n",
       "      Your browser does not support the <code>video</code> element.\n",
       "    </video>"
      ],
      "text/plain": [
       "<IPython.core.display.Video object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import IPython\n",
    "\n",
    "for i in clipNames:\n",
    "    IPython.display.display(IPython.display.Video(i, width=600))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "45df467d-ff6a-47e6-a84d-7908c2af07bd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "PROJECT_ID = !(gcloud config get-value core/project)\n",
    "PROJECT_ID = PROJECT_ID[0]\n",
    "\n",
    "SVC_ACC = !(gcloud config get-value core/account)\n",
    "SVC_ACC = SVC_ACC[0]\n",
    "\n",
    "PROJECT_NUMBER=str(re.search(r'\\d+', SVC_ACC).group())\n",
    "\n",
    "FOLDER_NAME=\".\"\n",
    "\n",
    "dataset_id = \"video_rag_dataset\"\n",
    "table_id = \"video_embeddings\"\n",
    "LOCATION=\"us-central1\"\n",
    "# full_table_id = '.'.join([PROJECT_ID,dataset_id,table_id])\n",
    "file_path = \"videodata.json\"\n",
    "\n",
    "\n",
    "from google.cloud import bigquery\n",
    "client = bigquery.Client(project=PROJECT_ID)\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "acf91bbd-ee84-4ad2-bf74-ccee45f66811",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def create_bigquery_dataset():\n",
    "    dataset = client.dataset(dataset_id)\n",
    "    dataset = client.create_dataset(dataset, timeout=30)  # Make an API request.\n",
    "    print(\"Created dataset {}.{}\".format(client.project, dataset.dataset_id))\n",
    "\n",
    "def create_bigquery_table():    \n",
    "    # Define the schema for the table\n",
    "    schema = [\n",
    "        bigquery.SchemaField(\"id\", \"STRING\"),\n",
    "        bigquery.SchemaField(\"start_time\", \"STRING\"),\n",
    "        bigquery.SchemaField(\"end_time\", \"STRING\"),\n",
    "        bigquery.SchemaField(\"clip_name\", \"STRING\"),\n",
    "        bigquery.SchemaField(\"frame_name\", \"STRING\"),\n",
    "        bigquery.SchemaField(\"description\", \"STRING\"),\n",
    "        bigquery.SchemaField(\"embedding\", \"FLOAT\", mode=\"REPEATED\")\n",
    "    ]\n",
    "    \n",
    "    # Define the table reference\n",
    "    table_ref = client.dataset(dataset_id).table(table_id)\n",
    "    \n",
    "    # Define the table object\n",
    "    table = bigquery.Table(table_ref, schema=schema)\n",
    "    \n",
    "    # Create the table in BigQuery\n",
    "    try:\n",
    "        client.create_table(table)\n",
    "        print(\"Table created successfully.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error creating table: {e}\")\n",
    "\n",
    "def load_data_into_bigquery_table():\n",
    "    job_config = bigquery.LoadJobConfig(\n",
    "        source_format=bigquery.SourceFormat.NEWLINE_DELIMITED_JSON, autodetect=False)\n",
    "    \n",
    "    table_ref = client.dataset(dataset_id).table(table_id)\n",
    "    \n",
    "    with open(file_path, \"rb\") as source_file:\n",
    "        job = client.load_table_from_file(source_file, table_ref, job_config=job_config)\n",
    "    \n",
    "    job.result()  # Waits for the job to complete.\n",
    "    \n",
    "    table = client.get_table(table_ref)  # Make an API request.\n",
    "    print(\"Loaded {} rows and {} columns to {}\".format(\n",
    "        table.num_rows, len(table.schema), table_ref))\n",
    "    \n",
    "def read_bq_table_dataframe():  \n",
    "    table_ref = client.dataset(dataset_id).table(table_id)\n",
    "    dataframe = client.list_rows(table_ref).to_dataframe(create_bqstorage_client=True)\n",
    "    \n",
    "    return dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "f55fb03b-7402-415d-b2b0-e4042d854487",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created dataset jingle-project-414801.video_rag_dataset\n",
      "Table created successfully.\n",
      "Loaded 10 rows and 7 columns to jingle-project-414801.video_rag_dataset.video_embeddings\n"
     ]
    }
   ],
   "source": [
    "# Call the function to create the table\n",
    "create_bigquery_dataset()\n",
    "create_bigquery_table()\n",
    "load_data_into_bigquery_table()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "ea69d483-fd8d-42af-aec1-5eb75238653d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>start_time</th>\n",
       "      <th>end_time</th>\n",
       "      <th>clip_name</th>\n",
       "      <th>frame_name</th>\n",
       "      <th>description</th>\n",
       "      <th>embedding</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>content/clips/clip_0.mp4</td>\n",
       "      <td>content/frames/frame_0.png</td>\n",
       "      <td>The video starts with a white background with ...</td>\n",
       "      <td>[0.0241825972, 0.0295802094, -0.0274775736, 0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>60</td>\n",
       "      <td>120.0</td>\n",
       "      <td>content/clips/clip_1.mp4</td>\n",
       "      <td>content/frames/frame_1.png</td>\n",
       "      <td>The video starts with a man standing on a stag...</td>\n",
       "      <td>[0.0018262258, 0.0378807113, -0.0280327685, 0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>120</td>\n",
       "      <td>180.0</td>\n",
       "      <td>content/clips/clip_2.mp4</td>\n",
       "      <td>content/frames/frame_2.png</td>\n",
       "      <td>The video starts with Sundar Pichai, CEO of Go...</td>\n",
       "      <td>[0.0018594925, -0.0015243472, -0.0319734663, -...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>180</td>\n",
       "      <td>240.0</td>\n",
       "      <td>content/clips/clip_3.mp4</td>\n",
       "      <td>content/frames/frame_3.png</td>\n",
       "      <td>The video starts with a person holding a smart...</td>\n",
       "      <td>[0.0157100726, -0.0262820572, -0.029469328, 0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>240</td>\n",
       "      <td>300.0</td>\n",
       "      <td>content/clips/clip_4.mp4</td>\n",
       "      <td>content/frames/frame_4.png</td>\n",
       "      <td>The video starts with a black screen with the ...</td>\n",
       "      <td>[0.0175017491, -0.0136799784, -0.0314220563, 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>300</td>\n",
       "      <td>360.0</td>\n",
       "      <td>content/clips/clip_5.mp4</td>\n",
       "      <td>content/frames/frame_5.png</td>\n",
       "      <td>The video starts with a woman with short curly...</td>\n",
       "      <td>[0.0262935739, -0.0310534034, -0.0107499557, 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>360</td>\n",
       "      <td>420.0</td>\n",
       "      <td>content/clips/clip_6.mp4</td>\n",
       "      <td>content/frames/frame_6.png</td>\n",
       "      <td>The video begins with a woman standing on a st...</td>\n",
       "      <td>[0.0550979152, -0.0187123474, -0.0080388971, 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>420</td>\n",
       "      <td>480.0</td>\n",
       "      <td>content/clips/clip_7.mp4</td>\n",
       "      <td>content/frames/frame_7.png</td>\n",
       "      <td>The video starts with a woman standing on a st...</td>\n",
       "      <td>[0.0145515744, -0.0049185813, -0.0101476833, 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>480</td>\n",
       "      <td>540.0</td>\n",
       "      <td>content/clips/clip_8.mp4</td>\n",
       "      <td>content/frames/frame_8.png</td>\n",
       "      <td>The video is about Google's new AI technology....</td>\n",
       "      <td>[-0.0144791249, -0.0161155425, -0.0128399599, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>540</td>\n",
       "      <td>598.05</td>\n",
       "      <td>content/clips/clip_9.mp4</td>\n",
       "      <td>content/frames/frame_9.png</td>\n",
       "      <td>The video starts with a man standing on a stag...</td>\n",
       "      <td>[0.0050415853, -0.0276209302, -0.0140419062, 0...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  id start_time end_time                 clip_name  \\\n",
       "0  0          0     60.0  content/clips/clip_0.mp4   \n",
       "1  1         60    120.0  content/clips/clip_1.mp4   \n",
       "2  2        120    180.0  content/clips/clip_2.mp4   \n",
       "3  3        180    240.0  content/clips/clip_3.mp4   \n",
       "4  4        240    300.0  content/clips/clip_4.mp4   \n",
       "5  5        300    360.0  content/clips/clip_5.mp4   \n",
       "6  6        360    420.0  content/clips/clip_6.mp4   \n",
       "7  7        420    480.0  content/clips/clip_7.mp4   \n",
       "8  8        480    540.0  content/clips/clip_8.mp4   \n",
       "9  9        540   598.05  content/clips/clip_9.mp4   \n",
       "\n",
       "                   frame_name  \\\n",
       "0  content/frames/frame_0.png   \n",
       "1  content/frames/frame_1.png   \n",
       "2  content/frames/frame_2.png   \n",
       "3  content/frames/frame_3.png   \n",
       "4  content/frames/frame_4.png   \n",
       "5  content/frames/frame_5.png   \n",
       "6  content/frames/frame_6.png   \n",
       "7  content/frames/frame_7.png   \n",
       "8  content/frames/frame_8.png   \n",
       "9  content/frames/frame_9.png   \n",
       "\n",
       "                                         description  \\\n",
       "0  The video starts with a white background with ...   \n",
       "1  The video starts with a man standing on a stag...   \n",
       "2  The video starts with Sundar Pichai, CEO of Go...   \n",
       "3  The video starts with a person holding a smart...   \n",
       "4  The video starts with a black screen with the ...   \n",
       "5  The video starts with a woman with short curly...   \n",
       "6  The video begins with a woman standing on a st...   \n",
       "7  The video starts with a woman standing on a st...   \n",
       "8  The video is about Google's new AI technology....   \n",
       "9  The video starts with a man standing on a stag...   \n",
       "\n",
       "                                           embedding  \n",
       "0  [0.0241825972, 0.0295802094, -0.0274775736, 0....  \n",
       "1  [0.0018262258, 0.0378807113, -0.0280327685, 0....  \n",
       "2  [0.0018594925, -0.0015243472, -0.0319734663, -...  \n",
       "3  [0.0157100726, -0.0262820572, -0.029469328, 0....  \n",
       "4  [0.0175017491, -0.0136799784, -0.0314220563, 0...  \n",
       "5  [0.0262935739, -0.0310534034, -0.0107499557, 0...  \n",
       "6  [0.0550979152, -0.0187123474, -0.0080388971, 0...  \n",
       "7  [0.0145515744, -0.0049185813, -0.0101476833, 0...  \n",
       "8  [-0.0144791249, -0.0161155425, -0.0128399599, ...  \n",
       "9  [0.0050415853, -0.0276209302, -0.0140419062, 0...  "
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataframe = read_bq_table_dataframe()\n",
    "dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc537471-3163-44d3-83b2-3212977e7419",
   "metadata": {},
   "source": [
    "## Querying using Gemini Model instead of Vector Search"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f1db7cd-19c1-4134-bc3c-28647eac4501",
   "metadata": {},
   "source": [
    "### Defining Functions for Gemini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "56839314-719c-4171-8c36-0baaa1e20905",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import vertexai\n",
    "from vertexai.preview.generative_models import GenerativeModel, Part\n",
    "from vertexai.preview.language_models import TextGenerationModel\n",
    "\n",
    "def generate(input_prompt):\n",
    "    model = GenerativeModel(\"gemini-ultra\")\n",
    "    responses = model.generate_content(\n",
    "        input_prompt ,\n",
    "    generation_config={\n",
    "        \"max_output_tokens\": 2048,\n",
    "        \"temperature\": 0.2,\n",
    "        \"top_p\": 1,\n",
    "        \"top_k\": 32\n",
    "    },\n",
    "        safety_settings=[],\n",
    "        stream=True,\n",
    "    )\n",
    "    \n",
    "    all_response  = []\n",
    "    \n",
    "    for response in responses:\n",
    "        # print(response.text, end=\"\")\n",
    "        all_response.append(response.text)\n",
    "    \n",
    "    return(\" \".join(all_response))\n",
    "    \n",
    "\n",
    "def generate_pro(input_prompt):\n",
    "    model = GenerativeModel(\"gemini-pro\")\n",
    "    responses = model.generate_content(\n",
    "    input_prompt,\n",
    "    generation_config={\n",
    "        \"max_output_tokens\": 2048,\n",
    "        \"temperature\": 0.2,\n",
    "        \"top_p\": 1\n",
    "    },stream=True,)\n",
    "    \n",
    "    all_response  = []\n",
    "    \n",
    "    for response in responses:\n",
    "        all_response.append(response.text)\n",
    "\n",
    "    return(\" \".join(all_response))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7df08a0c-434b-4040-a830-b3d775ce774a",
   "metadata": {},
   "source": [
    "### Query using Gemini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "ed3ccae4-1c2f-4a67-b11d-ac7dc50fa206",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Insert with your own query\n",
    "query = \"In which clips does Sundar Pichai appear?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "7a7a30fb-e0eb-4847-997e-065114ebca8b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def combine_column_to_string(df, column_name):\n",
    "\n",
    "    column_values = df[column_name].tolist()\n",
    "    combined_string = ', '.join(column_values)\n",
    "    return combined_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "8df35b3e-6305-4d23-8ee2-b4bc75ae0a0f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Id</th>\n",
       "      <th>start_time</th>\n",
       "      <th>end_time</th>\n",
       "      <th>clip_name</th>\n",
       "      <th>frame_name</th>\n",
       "      <th>associated_text</th>\n",
       "      <th>associated_speech</th>\n",
       "      <th>associated_object</th>\n",
       "      <th>description</th>\n",
       "      <th>id</th>\n",
       "      <th>embedding</th>\n",
       "      <th>distance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>180</td>\n",
       "      <td>240.00</td>\n",
       "      <td>content/clips/clip_3.mp4</td>\n",
       "      <td>content/frames/frame_3.png</td>\n",
       "      <td>The video starts with a person holding a phone...</td>\n",
       "      <td>vector IV Do you remember where you saw my gla...</td>\n",
       "      <td>The video starts with a pair of hands holding ...</td>\n",
       "      <td>The video starts with a person holding a smart...</td>\n",
       "      <td>3</td>\n",
       "      <td>[0.015710072591900826, -0.02628205716609955, -...</td>\n",
       "      <td>0.647064</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9</td>\n",
       "      <td>NaN</td>\n",
       "      <td>540</td>\n",
       "      <td>598.05</td>\n",
       "      <td>content/clips/clip_9.mp4</td>\n",
       "      <td>content/frames/frame_9.png</td>\n",
       "      <td>The video starts with a wide shot of a stage w...</td>\n",
       "      <td>for people and society We're improving our mod...</td>\n",
       "      <td>The video starts with a man with dark skin giv...</td>\n",
       "      <td>The video starts with a man standing on a stag...</td>\n",
       "      <td>9</td>\n",
       "      <td>[0.005041585303843021, -0.02762093022465706, -...</td>\n",
       "      <td>0.611545</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8</td>\n",
       "      <td>NaN</td>\n",
       "      <td>480</td>\n",
       "      <td>540.00</td>\n",
       "      <td>content/clips/clip_8.mp4</td>\n",
       "      <td>content/frames/frame_8.png</td>\n",
       "      <td>The video starts with a man in a grey shirt an...</td>\n",
       "      <td>what's possible with our latest model Gemini N...</td>\n",
       "      <td>The video features three speakers at a confere...</td>\n",
       "      <td>The video is about Google's new AI technology....</td>\n",
       "      <td>8</td>\n",
       "      <td>[-0.014479124918580055, -0.016115542501211166,...</td>\n",
       "      <td>0.609874</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  Id  start_time  end_time                 clip_name  \\\n",
       "0           3 NaN         180    240.00  content/clips/clip_3.mp4   \n",
       "1           9 NaN         540    598.05  content/clips/clip_9.mp4   \n",
       "2           8 NaN         480    540.00  content/clips/clip_8.mp4   \n",
       "\n",
       "                   frame_name  \\\n",
       "0  content/frames/frame_3.png   \n",
       "1  content/frames/frame_9.png   \n",
       "2  content/frames/frame_8.png   \n",
       "\n",
       "                                     associated_text  \\\n",
       "0  The video starts with a person holding a phone...   \n",
       "1  The video starts with a wide shot of a stage w...   \n",
       "2  The video starts with a man in a grey shirt an...   \n",
       "\n",
       "                                   associated_speech  \\\n",
       "0  vector IV Do you remember where you saw my gla...   \n",
       "1  for people and society We're improving our mod...   \n",
       "2  what's possible with our latest model Gemini N...   \n",
       "\n",
       "                                   associated_object  \\\n",
       "0  The video starts with a pair of hands holding ...   \n",
       "1  The video starts with a man with dark skin giv...   \n",
       "2  The video features three speakers at a confere...   \n",
       "\n",
       "                                         description  id  \\\n",
       "0  The video starts with a person holding a smart...   3   \n",
       "1  The video starts with a man standing on a stag...   9   \n",
       "2  The video is about Google's new AI technology....   8   \n",
       "\n",
       "                                           embedding  distance  \n",
       "0  [0.015710072591900826, -0.02628205716609955, -...  0.647064  \n",
       "1  [0.005041585303843021, -0.02762093022465706, -...  0.611545  \n",
       "2  [-0.014479124918580055, -0.016115542501211166,...  0.609874  "
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "shots_df = pd.read_csv('./results.csv') \n",
    "shots_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "0737729f-cbff-4d4c-9f1a-186ffc6d14a7",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The video starts with a person holding a smartphone in their hands. The phone is in a video call with a large language model named Gemini. The person asks Gemini if it remembers where they saw their glasses. Gemini responds that it does and that the glasses were on the desk near a red apple. The person then laughs and puts the phone down. The video then cuts to a different scene where a person is petting a golden retriever. The person asks Gemini for a band name for the dog and a tiger plushie. Gemini responds with \"Golden Stripes\". The person thanks Gemini and the video cuts to a man in a blue sweater standing in front of a blue and white background. The man is talking about Gemini 1.5 Flash, a new large language model that is faster and more cost-efficient than its predecessor. The video then cuts to a man in a black shirt standing on a stage in front of a large screen. The screen shows the Gemini 1.5 Flash logo. The man is talking about the features of Gemini 1.5 Flash, which include speed and efficiency, multimodal reasoning, and breakthrough long context. The video then cuts back to the man in the blue sweater. He is talking about the progress that has been made in generative video. The video then cuts back to the man in the black shirt. He is announcing a new generative video model called Veo. The video then shows a montage of different videos that were created using Veo. The videos include a cityscape at night, a mountain lake, a busy street, a satellite in space, and an alpaca.\n",
      "\n",
      "**Detailed descriptions of each subject:**\n",
      "\n",
      "**Person 1:**\n",
      "* Appearance: Only hands are visible. Fair skin tone.\n",
      "* Emotion: Neutral at the start, then amused.\n",
      "* Actions: Holds a smartphone, asks Gemini a question, laughs, puts the phone down.\n",
      "\n",
      "**Gemini (voice only):**\n",
      "* Emotion: Helpful and responsive.\n",
      "* Actions: Answers questions accurately, provides creative suggestions.\n",
      "\n",
      "**Person 2:**\n",
      "* Appearance: Only hands and forearms are visible. Fair skin tone.\n",
      "* Emotion: Playful and appreciative.\n",
      "* Actions: Pets a dog, holds a tiger plushie, interacts with Gemini.\n",
      "\n",
      "**Golden Retriever:**\n",
      "* Appearance: Golden fur, large size, friendly face.\n",
      "* Emotion: Calm and happy.\n",
      "* Actions: Sits patiently, enjoys being petted.\n",
      "\n",
      "**Tiger Plushie:**\n",
      "* Appearance: Orange and black stripes, small size, smiling face.\n",
      "\n",
      "**Man in Blue Sweater:**\n",
      "* Name: Not mentioned.\n",
      "* Appearance: Bald head, glasses, short beard, wearing a blue sweater and a watch.\n",
      "* Emotion: Excited and enthusiastic.\n",
      "* Actions: Speaks to the camera, uses hand gestures.\n",
      "\n",
      "**Man in Black Shirt:**\n",
      "* Name: Not mentioned.\n",
      "* Appearance: Short hair, glasses, wearing a black shirt and black pants.\n",
      "* Emotion: Excited and proud.\n",
      "* Actions: Stands on a stage, speaks to an audience, gestures towards a screen.\n",
      "\n",
      "**Veo (AI model):**\n",
      "* Actions: Creates high-quality 1080p videos from text, image, and video prompts.\n",
      "\n",
      "**Video Montage:**\n",
      "* Content: Shows various high-quality videos generated by Veo, including a cityscape at night, a mountain lake, a busy street, a satellite in space, and an alpaca. \n",
      ", The video starts with a man standing on a stage in front of a large screen and a large audience. The screen displays the words \"Building AI responsibly\".\n",
      "\n",
      "The scene changes to the man standing in front of a blue and white background. He says: \"for people and society. We're improving our models with an industry standard practice called red teaming,\". The scene changes back to the man on stage. The screen behind him now displays \"Red Teaming\". The man continues: \"in which we test our own models and try to break them to identify weaknesses.\". The scene changes to a white grid with three squares highlighted in a gradient of blue and red.\n",
      "\n",
      "The scene changes back to the man in front of the blue and white background. He says: \"I'm excited to introduce LearnAlam,\". The scene changes back to the man on stage. The screen behind him now displays \"LearnLM\". The man continues: \"our new family of models based on Gemini and fine-tuned for learning.\".\n",
      "\n",
      "The scene changes back to the man in front of the blue and white background. He says: \"Another example is a new feature in YouTube that uses LearnAlam to make educational videos more interactive, allowing you to ask a clarifying question, get a helpful explanation, or take a quiz.\". The scene changes back to the man on stage. The screen behind him now displays a phone with a YouTube video playing. A chat box is open on the phone, showing a conversation about the video. The chat box offers options to \"Summarize the video\", \"Tell me why this topic is important\", and \"Quiz me about this topic\". The user selects \"Quiz me about this topic\" and a multiple choice question appears.\n",
      "\n",
      "The scene changes to a different man in front of the blue and white background. He says: \"All of this shows the important progress we have made as we take a bold and responsible approach to making AI helpful for everyone. To everyone here in Shoreline and the millions more watching around the world, here's to the possibilities ahead and creating them together. Thank you.\". The man waves and smiles. The scene changes to a wide shot of the stage, showing the large audience.\n",
      "\n",
      "The scene changes back to the second man in front of the blue and white background. He smiles and waves again.\n",
      "\n",
      "The Google logo is displayed in the center of the screen.\n",
      "\n",
      "## Subject Descriptions:\n",
      "\n",
      "**Subject 1:**\n",
      "\n",
      "* **Name:** Not mentioned\n",
      "* **Appearance:** Dark-skinned male with a shaved head. He is wearing a light brown button-down shirt over a black t-shirt, and dark-colored pants. He is wearing glasses with a black frame.\n",
      "* **Emotion:** He appears to be enthusiastic and passionate about the topic he is presenting. He speaks with a smile and uses hand gestures to emphasize his points.\n",
      "* **Actions:** He is standing on a stage, presenting information about AI. He gestures with his hands while speaking.\n",
      "\n",
      "**Subject 2:**\n",
      "\n",
      "* **Name:** Not mentioned\n",
      "* **Appearance:** Darker-skinned male with short, dark hair and a trimmed beard. He is wearing a gray button-down shirt and glasses with a black frame.\n",
      "* **Emotion:** He appears to be proud of the progress made and optimistic about the future. He speaks with a smile and uses hand gestures to emphasize his points.\n",
      "* **Actions:** He is standing in front of a blue and white background, addressing the audience. He gestures with his hands while speaking. \n",
      ", The video is about Google's new AI technology. It features two speakers on a stage at a Google event.\n",
      "\n",
      "The first speaker announces the Gemini Nano model with multimodality, which allows phones to understand the world through text, sights, sounds, and spoken language.\n",
      "\n",
      "The second speaker talks about Gemma, a family of open models crucial for driving AI innovation and responsibility. He announces the release of PaLiGemma, their first vision language open model, and Gemma 2, the next generation of Gemma, which will be available in June with a new 27 billion parameter model.\n",
      "\n",
      "**Speaker 1:**\n",
      "\n",
      "* **Appearance:** A man with short brown hair and a light beard. He is wearing a brown long-sleeved shirt and dark blue jeans. He has a smartwatch on his left wrist and a ring on his left ring finger.\n",
      "* **Emotion:** He appears enthusiastic and passionate about the technology he is presenting.\n",
      "* **Actions:** He is standing on a stage, gesturing with his hands as he speaks.\n",
      "* **Words:** \"Imagine what's possible with our latest model, Gemini Nano with multimodality. This means your phone can understand the world the way you understand it. So not just through text input, but also through sights, sounds, and spoken language.\"\n",
      "\n",
      "**Speaker 2:**\n",
      "\n",
      "* **Appearance:** A man with short blonde hair and glasses. He is wearing a dark gray hoodie and dark blue jeans.\n",
      "* **Emotion:** He appears excited and proud to announce the new AI models.\n",
      "* **Actions:** He is standing on a stage, clapping his hands together and gesturing as he speaks.\n",
      "* **Words:** \"Now, let's shift gears and talk about Gemma, our family of open models which are crucial for driving AI innovation and responsibility. Today's newest member, PaLiGemma, our first vision language open model and it's available right now. I'm also excited to announce that we have Gemma 2 coming. It's the next generation of Gemma and it will be available in June. So in a few weeks, we'll be adding a new 27 billion parameter model to Gemma 2.\"\n",
      "\n",
      "**Other:**\n",
      "\n",
      "* The video shows a large screen behind the speakers, displaying visuals related to the topics being discussed.\n",
      "* The audience is visible in some shots, clapping and looking at the speakers. \n",
      "\n",
      "Your prompt: \n",
      " You are an expert in screening video descriptions and understanding the context and contents of the video.\n",
      "Only answer based on the description of the video provided here: \n",
      " The video starts with a person holding a smartphone in their hands. The phone is in a video call with a large language model named Gemini. The person asks Gemini if it remembers where they saw their glasses. Gemini responds that it does and that the glasses were on the desk near a red apple. The person then laughs and puts the phone down. The video then cuts to a different scene where a person is petting a golden retriever. The person asks Gemini for a band name for the dog and a tiger plushie. Gemini responds with \"Golden Stripes\". The person thanks Gemini and the video cuts to a man in a blue sweater standing in front of a blue and white background. The man is talking about Gemini 1.5 Flash, a new large language model that is faster and more cost-efficient than its predecessor. The video then cuts to a man in a black shirt standing on a stage in front of a large screen. The screen shows the Gemini 1.5 Flash logo. The man is talking about the features of Gemini 1.5 Flash, which include speed and efficiency, multimodal reasoning, and breakthrough long context. The video then cuts back to the man in the blue sweater. He is talking about the progress that has been made in generative video. The video then cuts back to the man in the black shirt. He is announcing a new generative video model called Veo. The video then shows a montage of different videos that were created using Veo. The videos include a cityscape at night, a mountain lake, a busy street, a satellite in space, and an alpaca.\n",
      "\n",
      "**Detailed descriptions of each subject:**\n",
      "\n",
      "**Person 1:**\n",
      "* Appearance: Only hands are visible. Fair skin tone.\n",
      "* Emotion: Neutral at the start, then amused.\n",
      "* Actions: Holds a smartphone, asks Gemini a question, laughs, puts the phone down.\n",
      "\n",
      "**Gemini (voice only):**\n",
      "* Emotion: Helpful and responsive.\n",
      "* Actions: Answers questions accurately, provides creative suggestions.\n",
      "\n",
      "**Person 2:**\n",
      "* Appearance: Only hands and forearms are visible. Fair skin tone.\n",
      "* Emotion: Playful and appreciative.\n",
      "* Actions: Pets a dog, holds a tiger plushie, interacts with Gemini.\n",
      "\n",
      "**Golden Retriever:**\n",
      "* Appearance: Golden fur, large size, friendly face.\n",
      "* Emotion: Calm and happy.\n",
      "* Actions: Sits patiently, enjoys being petted.\n",
      "\n",
      "**Tiger Plushie:**\n",
      "* Appearance: Orange and black stripes, small size, smiling face.\n",
      "\n",
      "**Man in Blue Sweater:**\n",
      "* Name: Not mentioned.\n",
      "* Appearance: Bald head, glasses, short beard, wearing a blue sweater and a watch.\n",
      "* Emotion: Excited and enthusiastic.\n",
      "* Actions: Speaks to the camera, uses hand gestures.\n",
      "\n",
      "**Man in Black Shirt:**\n",
      "* Name: Not mentioned.\n",
      "* Appearance: Short hair, glasses, wearing a black shirt and black pants.\n",
      "* Emotion: Excited and proud.\n",
      "* Actions: Stands on a stage, speaks to an audience, gestures towards a screen.\n",
      "\n",
      "**Veo (AI model):**\n",
      "* Actions: Creates high-quality 1080p videos from text, image, and video prompts.\n",
      "\n",
      "**Video Montage:**\n",
      "* Content: Shows various high-quality videos generated by Veo, including a cityscape at night, a mountain lake, a busy street, a satellite in space, and an alpaca. \n",
      ", The video starts with a man standing on a stage in front of a large screen and a large audience. The screen displays the words \"Building AI responsibly\".\n",
      "\n",
      "The scene changes to the man standing in front of a blue and white background. He says: \"for people and society. We're improving our models with an industry standard practice called red teaming,\". The scene changes back to the man on stage. The screen behind him now displays \"Red Teaming\". The man continues: \"in which we test our own models and try to break them to identify weaknesses.\". The scene changes to a white grid with three squares highlighted in a gradient of blue and red.\n",
      "\n",
      "The scene changes back to the man in front of the blue and white background. He says: \"I'm excited to introduce LearnAlam,\". The scene changes back to the man on stage. The screen behind him now displays \"LearnLM\". The man continues: \"our new family of models based on Gemini and fine-tuned for learning.\".\n",
      "\n",
      "The scene changes back to the man in front of the blue and white background. He says: \"Another example is a new feature in YouTube that uses LearnAlam to make educational videos more interactive, allowing you to ask a clarifying question, get a helpful explanation, or take a quiz.\". The scene changes back to the man on stage. The screen behind him now displays a phone with a YouTube video playing. A chat box is open on the phone, showing a conversation about the video. The chat box offers options to \"Summarize the video\", \"Tell me why this topic is important\", and \"Quiz me about this topic\". The user selects \"Quiz me about this topic\" and a multiple choice question appears.\n",
      "\n",
      "The scene changes to a different man in front of the blue and white background. He says: \"All of this shows the important progress we have made as we take a bold and responsible approach to making AI helpful for everyone. To everyone here in Shoreline and the millions more watching around the world, here's to the possibilities ahead and creating them together. Thank you.\". The man waves and smiles. The scene changes to a wide shot of the stage, showing the large audience.\n",
      "\n",
      "The scene changes back to the second man in front of the blue and white background. He smiles and waves again.\n",
      "\n",
      "The Google logo is displayed in the center of the screen.\n",
      "\n",
      "## Subject Descriptions:\n",
      "\n",
      "**Subject 1:**\n",
      "\n",
      "* **Name:** Not mentioned\n",
      "* **Appearance:** Dark-skinned male with a shaved head. He is wearing a light brown button-down shirt over a black t-shirt, and dark-colored pants. He is wearing glasses with a black frame.\n",
      "* **Emotion:** He appears to be enthusiastic and passionate about the topic he is presenting. He speaks with a smile and uses hand gestures to emphasize his points.\n",
      "* **Actions:** He is standing on a stage, presenting information about AI. He gestures with his hands while speaking.\n",
      "\n",
      "**Subject 2:**\n",
      "\n",
      "* **Name:** Not mentioned\n",
      "* **Appearance:** Darker-skinned male with short, dark hair and a trimmed beard. He is wearing a gray button-down shirt and glasses with a black frame.\n",
      "* **Emotion:** He appears to be proud of the progress made and optimistic about the future. He speaks with a smile and uses hand gestures to emphasize his points.\n",
      "* **Actions:** He is standing in front of a blue and white background, addressing the audience. He gestures with his hands while speaking. \n",
      ", The video is about Google's new AI technology. It features two speakers on a stage at a Google event.\n",
      "\n",
      "The first speaker announces the Gemini Nano model with multimodality, which allows phones to understand the world through text, sights, sounds, and spoken language.\n",
      "\n",
      "The second speaker talks about Gemma, a family of open models crucial for driving AI innovation and responsibility. He announces the release of PaLiGemma, their first vision language open model, and Gemma 2, the next generation of Gemma, which will be available in June with a new 27 billion parameter model.\n",
      "\n",
      "**Speaker 1:**\n",
      "\n",
      "* **Appearance:** A man with short brown hair and a light beard. He is wearing a brown long-sleeved shirt and dark blue jeans. He has a smartwatch on his left wrist and a ring on his left ring finger.\n",
      "* **Emotion:** He appears enthusiastic and passionate about the technology he is presenting.\n",
      "* **Actions:** He is standing on a stage, gesturing with his hands as he speaks.\n",
      "* **Words:** \"Imagine what's possible with our latest model, Gemini Nano with multimodality. This means your phone can understand the world the way you understand it. So not just through text input, but also through sights, sounds, and spoken language.\"\n",
      "\n",
      "**Speaker 2:**\n",
      "\n",
      "* **Appearance:** A man with short blonde hair and glasses. He is wearing a dark gray hoodie and dark blue jeans.\n",
      "* **Emotion:** He appears excited and proud to announce the new AI models.\n",
      "* **Actions:** He is standing on a stage, clapping his hands together and gesturing as he speaks.\n",
      "* **Words:** \"Now, let's shift gears and talk about Gemma, our family of open models which are crucial for driving AI innovation and responsibility. Today's newest member, PaLiGemma, our first vision language open model and it's available right now. I'm also excited to announce that we have Gemma 2 coming. It's the next generation of Gemma and it will be available in June. So in a few weeks, we'll be adding a new 27 billion parameter model to Gemma 2.\"\n",
      "\n",
      "**Other:**\n",
      "\n",
      "* The video shows a large screen behind the speakers, displaying visuals related to the topics being discussed.\n",
      "* The audience is visible in some shots, clapping and looking at the speakers. \n",
      "The video starts with a person holding a phone in their hands. The phone is in a video call and the person is showing the other person on the call around the room. The person then puts the phone down on a desk. The video then cuts to a wide shot of an office. The camera moves around the office and then focuses on a dog. The video then cuts to a man in a blue sweater standing in front of a blue and white background. He is wearing glasses and has a microphone clipped to his sweater. He is speaking. The video then cuts to a man in a black shirt and black pants standing on a stage in front of a large screen. The screen is black with the words \"Gemini 1.5 Flash\" in white letters. The man is speaking. The screen then changes to show a blue and white logo with the words \"Speed and efficiency\" underneath. The man is still speaking. The screen then changes to show the same logo with the words \"Multimodal reasoning\" underneath. The man is still speaking. The video then cuts back to the man in the blue sweater. He is still speaking. The video then cuts back to the man on the stage. The screen behind him is now showing a picture of a city at night. The man is still speaking. The screen then changes to show a picture of a lake in the mountains. The man is still speaking. The screen then changes to show a collage of six different videos. The videos are of a city at night, a lake in the mountains, a llama, a space shuttle, a sunset, and a city street. The word \"Veo\" is superimposed over the videos. The video then ends., The video starts with a wide shot of a stage with a large screen that reads \"Building AI responsibly\". A Black man in a tan shirt and black pants is standing on the stage in front of an audience. The screen changes to black with the words \"Red Teaming\" in white. The man continues speaking. The screen changes to white with the words \"LearnLM\" in blue and green. The screen changes to show a smartphone with a chat window open. The chat window reads \"Ask about this video\" and has several options to choose from. The screen changes back to the man speaking. The screen changes to a wide shot of a different stage with a large screen that shows a video of the man speaking. The man is now wearing a gray shirt. He waves to the audience and the video ends with the Google logo. \n",
      ", The video starts with a man in a grey shirt and black pants standing on a stage in front of a large screen. The screen displays the text \"Gemini Nano with Multimodality\" and \"Coming to Pixel later this year\". The man is gesturing with his hands as he speaks.\n",
      "\n",
      "The screen then changes to show four black rectangles. The rectangles are arranged in a 2x2 grid. The top left rectangle has the letters \"Aa\" in it. The top right rectangle has a camera icon in it. The bottom left rectangle has a sound wave icon in it. The bottom right rectangle has a microphone icon in it.\n",
      "\n",
      "The screen then changes to show a grid of images and text. The images include a person standing in front of a body of water, a guitar, and a person standing in front of a building. The text is in a foreign language.\n",
      "\n",
      "The man in the grey shirt is no longer visible. A man in a black sweater and blue jeans is now standing on the stage. He is wearing glasses and has short, light brown hair. The screen behind him is black.\n",
      "\n",
      "The screen then displays the text \"Gemma\" in blue letters. The man in the black sweater claps his hands together once.\n",
      "\n",
      "The screen then displays the text \"PaliGemma\" in blue letters. The man in the black sweater is still standing on the stage.\n",
      "\n",
      "The screen then displays four images and the text \"PaliGemma\". The images are of a microscopic image, a dog, some flowers, and an aerial view of a city. The man in the black sweater is still standing on the stage.\n",
      "\n",
      "The screen behind the man is now black.\n",
      "\n",
      "The screen then displays the text \"Gemma 2\". The text is in white letters and is centered on the screen. Below the text is a diagram of a neural network. The diagram is in white lines and is also centered on the screen.\n",
      "\n",
      "The screen then displays the text \"Gemma 2\" and \"27B parameters\". The man in the black sweater is still standing on the stage.\n",
      "\n",
      "The audience, a large group of people of various races and genders, is shown clapping. They are seated in rows of chairs.\n",
      "\n",
      "The man in the black sweater is no longer visible. A man in a tan shirt and black pants is now standing on a stage in front of a blue wall. He is wearing glasses and has short, dark hair. The man is gesturing with his hands as he speaks.\n",
      "\n",
      "The screen behind the man is now white and displays the text \"Building AI responsibly\" in blue letters. The man in the tan shirt is still standing on the stage.\n",
      "vector IV Do you remember where you saw my glasses Yes I do Your glasses were on the desk near a red apple Golden Stripes Nice Thanks Gemini Today we're introducing Gemini 1.5 Flash Flash is a lighter weight model compared to Pro It's designed to be fast and cost-efficient to serve at scale while still featuring multimodal reasoning capabilities and breakthrough long context There's one more area I'm really excited to share with you Our teams have made some incredible progress in generative video Today I'm excited to announce our newest most capable generative video model called Veo Veo creates high quality 1080p videos from text image and , for people and society We're improving our models with an industry standard practice called red teaming in which we test our own models and try to break them to identify weaknesses I'm excited to introduce Learn Alam our new family of models based on Gemini and fine-tuned for learning Another example is a new feature in YouTube that uses Learn Alam to make educational videos more interactive allowing you to ask a clarifying question get a helpful explanation or take a quiz All of this shows the important progress we have made as we take a bold and responsible approach to making AI helpful for everyone To everyone here in Shoreline and the millions more watching around the world Here's to the possibilities ahead and creating them together Thank you , what's possible with our latest model Gemini Nano with multimodality This means your phone can understand the world the way you understand it So not just through text input but also through sights sounds and spoken language Now let's shift gears and talk about Gemma our family of open models which are crucial for driving AI innovation and responsibility Today's newest member PaLiGemma our first vision language open model and it's available right now I'm also excited to announce that we have Gemma 2 coming It's the next generation of Gemma and it will be available in June So in a few weeks we'll be adding a new 27 billion parameter model to Gemma 2 To us building AI responsibly means both addressing the risks and maximizing the benefits \n",
      "The video starts with a pair of hands holding a smartphone. The hands are pale and appear to belong to a light-skinned person. They are holding the phone in front of them, slightly angled downwards. The person is wearing a blue shirt. The hands are moving the phone around, as if the person is looking for something. The person speaks, asking \"Do you remember where you saw my glasses?\". A voice identified as Gemini responds \"Yes, I do. Your glasses were on the desk near a red apple.\" The hands then move the phone again, locating the glasses on the desk. The person then says \"huh\" and places the phone down on the desk. \n",
      "\n",
      "The video then cuts to a woman sitting at a desk in an office. She is wearing glasses, a black shirt, and has dark hair. She is typing on a laptop and appears to be working. The camera angle is from a low perspective, as if being held by someone sitting on the floor. A golden retriever dog walks up to the camera and sniffs it. The dog has a white bandage on its right front leg. The person holding the camera reaches out and pets the dog on the head. The person asks \"Give me a band name for this duo.\" The camera focuses on the dog's face. The dog is looking at the camera with its tongue slightly out. The voice identified as Gemini responds \"Golden Stripes.\" The person laughs and thanks Gemini. The dog walks away.\n",
      "\n",
      "The video then cuts to a man with glasses and a shaved head standing in front of a blue and white background. He is wearing a blue sweater and a watch. He is speaking in a clear and concise voice. He says \"Today, we're introducing Gemini 1.5 Flash.\" The video then cuts to a man with glasses and short hair standing on a stage in front of a large screen. He is wearing a black shirt and black pants. The screen behind him shows the words \"Gemini 1.5 Flash\" in white letters on a black background. The man is speaking in a clear and concise voice. He says \"Flash is a lighter-weight model compared to Pro. It's designed to be fast and cost-efficient to serve at scale, while still featuring multimodal reasoning capabilities and breakthrough long context.\" The screen behind him changes to show a diagram of a network of nodes, with the words \"Speed and efficiency\" underneath. On the right side of the screen, there are four squares arranged in a two-by-two grid. The top left square contains the letters \"Aa\", the top right square contains a picture of a landscape, the bottom left square contains a musical note, and the bottom right square contains a video camera icon. The words \"Multimodal reasoning\" are displayed underneath the squares. The man continues to speak, but his words are not included in the provided text.\n",
      "\n",
      "The video then cuts back to the man in the blue sweater. He says \"There's one more area I'm really excited to share with you. Our teams have made some incredible progress in generative video.\" The video then cuts back to the man on stage. He says \"Today, I'm excited to announce our newest, most capable generative video model, called Veo.\" The screen behind him changes to show a variety of video clips, including a nighttime cityscape, a mountain lake, a busy city street, a satellite in space, and a llama. The word \"Veo\" is superimposed over the clips in large, white letters. The man continues to speak, saying \"Veo creates high-quality 1080p videos from text, image, and...\" The video ends before he can finish his sentence. \n",
      ", The video starts with a man with dark skin giving a presentation on a stage. He is wearing a beige overshirt over a black shirt, black pants, and black shoes. He has short, dark hair and is wearing glasses. He is standing in the middle of the stage, with a large screen behind him. He is gesturing with his hands as he speaks. He seems to be passionate about the topic he is presenting.\n",
      "\n",
      "The man on the stage introduces a new feature on YouTube that uses AI to make educational videos more interactive. The video then cuts to a close-up of a smartphone screen showing the new feature. The screen shows a video of a man with dark hair and a beard, wearing a black t-shirt, who is writing code on a whiteboard. The video is paused, and a chat window is open on top of it. The chat window says: \"Ask about this video. Hello! Want to learn about what you're watching? I'm here to help. Not sure what to ask? Choose something. Summarize the video. Tell me why this topic is important. Quiz me about this topic.\"\n",
      "\n",
      "The video then cuts back to the first man on stage. He is still gesturing with his hands as he speaks. He seems to be excited about the new feature.\n",
      "\n",
      "The video then cuts to another man with dark hair and a beard, wearing glasses and a gray shirt. He is speaking to the camera in front of a blue and white background. He is Sundar Pichai, CEO of Google. He is clasping his hands together in front of him. He is speaking about the progress Google has made in AI. He seems to be proud of the work Google is doing.\n",
      "\n",
      "The video then cuts to a wide shot of an outdoor amphitheater. There is a large stage with a screen in the background. Sundar Pichai is standing on the stage, giving a presentation to a large audience. He is wearing a dark suit and a light shirt. He is gesturing with his hands as he speaks. He seems to be enthusiastic about the future of AI.\n",
      "\n",
      "The video then cuts back to Sundar Pichai speaking to the camera in front of the blue and white background. He is smiling and waving to the camera. He seems to be happy to be sharing Google's work with the world.\n",
      ", The video features three speakers at a conference.\n",
      "\n",
      "The first speaker is a man with short brown hair and a light beard. He is wearing a brown long-sleeved shirt and a smartwatch on his left wrist. He seems to be passionate about the topic he is presenting, using hand gestures to emphasize his points. He is standing on a stage in front of a large screen with a Google-themed design. The screen displays information about \"Gemini Nano with Multimodality\". The audience is not visible in this shot.\n",
      "\n",
      "The second speaker is also a man, this time with short blond hair and wearing glasses. He is wearing a dark gray hoodie and dark blue jeans. He, too, uses hand gestures while speaking and appears enthusiastic about the \"Gemma\" family of open models. He announces \"PaliGemma\" and \"Gemma 2\". The background changes between a shot of him on stage in front of a large screen with the audience sitting in the background and a close-up shot where only a blue and white wall is visible.\n",
      "\n",
      "The third speaker is a dark-skinned man with short, dark hair and a thin beard. He is wearing rectangular glasses, a light brown button-up shirt over a black t-shirt. He is speaking about building AI responsibly. The background is similar to the previous speaker's close-up shot, but with a slightly different shade of blue. The audience is not visible in this shot. \n",
      "  -- Based on the video description provided, answer the following query as accurately as possible:\n",
      "In which clips does Sundar Pichai appear?\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "System_Prompts = \"\"\" You are an expert in screening video descriptions and understanding the context and contents of the video.\n",
    "Only answer based on the description of the video provided here: \n",
    "\"\"\"\n",
    "\n",
    "Question_Prompts = \"\"\" -- Based on the video description provided, answer the following query as accurately as possible:\n",
    "\"\"\"\n",
    "print(combine_column_to_string(shots_df,'description'))\n",
    "\n",
    "videoDesc = combine_column_to_string(shots_df,'description') + combine_column_to_string(shots_df,'associated_text') + combine_column_to_string(shots_df,'associated_speech') + combine_column_to_string(shots_df,'associated_object')\n",
    "\n",
    "combined_prompt = System_Prompts + ' ' + videoDesc + ' ' + Question_Prompts + query\n",
    "\n",
    "\n",
    "print(\"Your prompt: \\n\" + combined_prompt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdd553d4-4b21-4f5b-a2f5-5b402e760397",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(\"Response from AI model: \\n\")\n",
    "print(generate_pro(combined_prompt))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c286cd01-4c9c-4187-9ee5-2aad75b236b4",
   "metadata": {},
   "source": [
    "## The End"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-env-pytorch-pytorch",
   "name": "workbench-notebooks.m124",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m124"
  },
  "kernelspec": {
   "display_name": "PyTorch 1-13 (Local)",
   "language": "python",
   "name": "conda-env-pytorch-pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
