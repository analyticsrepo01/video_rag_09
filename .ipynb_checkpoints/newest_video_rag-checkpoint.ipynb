{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1d9e12dd-8913-431a-bacb-1c190dffd702",
   "metadata": {},
   "source": [
    "# Video Search App"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee7c83d5-0b90-40eb-9699-eef1192b80e7",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Prerequisites"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a5aef0f-ad9c-4b85-ace2-f438fe6396a2",
   "metadata": {},
   "source": [
    "### Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "248dd395-eb5f-44fb-87eb-157342c80757",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Ignoring invalid distribution -ryptography (/opt/conda/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -ryptography (/opt/conda/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "google-cloud-documentai-toolbox 0.13.0a0 requires grpc-google-iam-v1<0.13dev,>=0.12.6, but you have grpc-google-iam-v1 0.13.0 which is incompatible.\n",
      "google-cloud-translate 2.0.1 requires google-api-core[grpc]<2.0.0dev,>=1.15.0, but you have google-api-core 2.19.1 which is incompatible.\n",
      "google-cloud-translate 2.0.1 requires google-cloud-core<2.0dev,>=1.1.0, but you have google-cloud-core 2.4.1 which is incompatible.\n",
      "kfp 2.4.0 requires kubernetes<27,>=8.0.0, but you have kubernetes 29.0.0 which is incompatible.\n",
      "kfp 2.4.0 requires requests-toolbelt<1,>=0.8.0, but you have requests-toolbelt 1.0.0 which is incompatible.\n",
      "kfp 2.4.0 requires urllib3<2.0.0, but you have urllib3 2.2.1 which is incompatible.\n",
      "langchain-google-genai 1.0.3 requires google-generativeai<0.6.0,>=0.5.2, but you have google-generativeai 0.7.1 which is incompatible.\n",
      "langchain-google-genai 1.0.3 requires langchain-core<0.2,>=0.1.45, but you have langchain-core 0.2.28 which is incompatible.\n",
      "scrapegraphai 1.5.2 requires langchain==0.1.15, but you have langchain 0.2.12 which is incompatible.\n",
      "scrapegraphai 1.5.2 requires langchain-openai==0.1.6, but you have langchain-openai 0.1.7 which is incompatible.\n",
      "scrapegraphai 1.5.2 requires pandas==2.2.2, but you have pandas 2.1.3 which is incompatible.\n",
      "scrapegraphai 1.5.2 requires python-dotenv==1.0.1, but you have python-dotenv 1.0.0 which is incompatible.\n",
      "scrapegraphai 1.5.2 requires tiktoken==0.6.0, but you have tiktoken 0.7.0 which is incompatible.\n",
      "vertexai 1.49.0 requires google-cloud-aiplatform[all]==1.49.0, but you have google-cloud-aiplatform 1.65.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.1.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -ryptography (/opt/conda/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -ryptography (/opt/conda/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.1.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -ryptography (/opt/conda/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -ryptography (/opt/conda/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.1.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -ryptography (/opt/conda/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -ryptography (/opt/conda/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.1.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -ryptography (/opt/conda/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -ryptography (/opt/conda/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.1.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -ryptography (/opt/conda/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -ryptography (/opt/conda/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.1.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# !pip install -q --upgrade google-cloud-aiplatform \n",
    "!pip install -q --upgrade google-cloud-dlp python-docx --upgrade google-auth\n",
    "!pip install -q --upgrade google-cloud-videointelligence\n",
    "!pip install -q bigframes==0.26\n",
    "!pip install -q --upgrade moviepy\n",
    "!pip install -q unidecode\n",
    "\n",
    "!pip install -q --upgrade pytube\n",
    "# !pip install -q --upgrade google-cloud-discoveryengine\n",
    "!pip install -q langchain_google_vertexai"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a932cfa5-9443-4fd1-956c-d462530d38f7",
   "metadata": {},
   "source": [
    "### Setup Environment Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e7c27afe-3cbd-47ad-ae5f-10613999f0a9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain_google_vertexai import VertexAIEmbeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "823d2dd9-21f0-434a-a84c-389e6c46c251",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated property [core/project].\n",
      "Creating gs://my-project-0004-346516-pytorch112kagglewbi-us-central1/...\n",
      "\u001b[1;31mERROR:\u001b[0m (gcloud.storage.buckets.create) HTTPError 409: Your previous request to create the named bucket succeeded and you already own it.\n",
      "mkdir: cannot create directory ‘content’: File exists\n"
     ]
    }
   ],
   "source": [
    "import socket\n",
    "import re\n",
    "\n",
    "PROJECT_ID = !(gcloud config get-value core/project)\n",
    "PROJECT_ID = PROJECT_ID[0]\n",
    "\n",
    "SVC_ACC = !(gcloud config get-value core/account)\n",
    "SVC_ACC = SVC_ACC[0]\n",
    "\n",
    "PROJECT_NUMBER=str(re.search(r'\\d+', SVC_ACC).group())\n",
    "\n",
    "LOCATION=\"us-central1\"\n",
    "\n",
    "UNIQUE_PREFIX = socket.gethostname()\n",
    "UNIQUE_PREFIX = re.sub('[^A-Za-z0-9]+', '', UNIQUE_PREFIX)\n",
    "\n",
    "UID = UNIQUE_PREFIX\n",
    "\n",
    "BUCKET_NAME = f\"{PROJECT_ID}-{UNIQUE_PREFIX}-{LOCATION}\"\n",
    "\n",
    "BUCKET_URI = f\"gs://{BUCKET_NAME}\"\n",
    "\n",
    "! gcloud config set project $PROJECT_ID\n",
    "! gcloud storage buckets create {BUCKET_URI} --project={PROJECT_ID} --location={LOCATION}\n",
    "\n",
    "!mkdir content\n",
    "!rm -r content/clips/\n",
    "!mkdir content/clips/\n",
    "!rm -r content/frames/\n",
    "!mkdir content/frames/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2829d182-c08a-4493-bd3d-8cbae9b28ded",
   "metadata": {},
   "source": [
    "### Initialize Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d5f14b9a-cc6b-43dd-a3e5-8f5e505154fb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import vertexai\n",
    "from vertexai.language_models import TextEmbeddingModel\n",
    "from vertexai.vision_models import Image as vision_model_Image\n",
    "from vertexai.vision_models import MultiModalEmbeddingModel\n",
    "import glob\n",
    "import os\n",
    "import time\n",
    "from typing import Any, Dict, Iterable, List, Optional, Tuple, Union\n",
    "from IPython.display import display\n",
    "import PIL\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import requests\n",
    "from vertexai.generative_models import (\n",
    "    GenerationConfig,\n",
    "    HarmBlockThreshold,\n",
    "    HarmCategory,\n",
    "    Image,\n",
    ")\n",
    "from vertexai.language_models import TextEmbeddingModel\n",
    "from vertexai.vision_models import Image as vision_model_Image\n",
    "from vertexai.vision_models import MultiModalEmbeddingModel\n",
    "from moviepy.editor import VideoFileClip\n",
    "from google.api_core.client_options import ClientOptions\n",
    "from google.cloud import discoveryengine"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aa7663a-2782-4f6f-8497-4fa99f86faaa",
   "metadata": {},
   "source": [
    "### Download and upload videos to a bucket"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16d1503e-080b-4748-9dcc-cbe127a778fe",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8ab91cec-29ca-4c02-aff4-048f47f1feaa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pytube\n",
    "from google.cloud import storage\n",
    "import unidecode\n",
    "\n",
    "# -1.1) Fetching from youtube\n",
    "def download_video(video_url):\n",
    "    # Create a YouTube object\n",
    "    yt = YouTube(video_url)\n",
    "    \n",
    "    # Filter to get the highest resolution stream that includes both video and audio\n",
    "    stream = yt.streams.filter(progressive=True, file_extension='mp4').order_by('resolution').desc().first()\n",
    "    \n",
    "    # Download the video\n",
    "    if stream:\n",
    "        print(f\"Going to download {video_url}\")\n",
    "        stream.download()\n",
    "        print(f\"Downloaded into {stream.default_filename}\")\n",
    "    else:\n",
    "        print(\"No suitable stream found\")\n",
    "    return stream.default_filename\n",
    "\n",
    "\n",
    "def upload_file_to_gcs(bucket_name, source_file_path, destination_blob_name):\n",
    "    client = storage.Client()\n",
    "    bucket = client.bucket(bucket_name)\n",
    "    blob = bucket.blob(destination_blob_name)\n",
    "    blob.upload_from_filename(source_file_path)\n",
    "    return blob\n",
    "\n",
    "def upload_file_to_bucket(bucket_name, source_file_path_name): \n",
    "    source_file_path = f\"./{source_file_path_name}\"\n",
    "    destination_blob_name = source_file_path_name.replace(' ', '_')\n",
    "    destination_blob_name = unidecode.unidecode(destination_blob_name)\n",
    "    uploaded_blob         = upload_file_to_gcs(bucket_name, source_file_path, destination_blob_name)\n",
    "    print(f\"File [{source_file_path}] \\nUploaded to [{uploaded_blob.name}] \\nIn bucket [{bucket_name}]\")\n",
    "    return destination_blob_name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab8307e3-dfca-4830-95fd-9e6ae641f35c",
   "metadata": {},
   "source": [
    "### Download and upload Videos from youtube"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b482cf33-c527-49ae-865b-493f31158693",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define the video URL\n",
    "video_url = \"https://www.youtube.com/watch?v=ZBRLpvgcTnM\"\n",
    "\n",
    "# Commented out because Pytube is down (as of Jul 20, 2024). Feel free to uncomment if Pytube is working again\n",
    "# downloaded_video = download_video(video_url)\n",
    "\n",
    "#Comment out below code if Pytube is working:\n",
    "#Change video to your video in content folder\n",
    "downloaded_video = \"./content/googleio.mp4\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85ff44cb-3c98-40c4-b195-8809ce1ed246",
   "metadata": {},
   "source": [
    "### Create csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "15767c04-0e63-48dd-96b0-97064c337e5a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "with open('shots.csv', 'w', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "    field = [\"Id\", \"start_time\", \"end_time\", \"clip_name\", \"frame_name\", \"associated_text\", \"associated_speech\", \"associated_object\", \"description\"]\n",
    "    writer.writerow(field)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62b22119-b765-4145-8c9c-57421bf65836",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Process videos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4391f52b-19ef-4dc4-984e-22fa228d45f0",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Load AI Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ce6a178-0abc-4d0f-8cbe-28e81975ecb8",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8114fb4f-d3cb-4683-a004-03a0b9388bef",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Functions for getting text and image embeddings\n",
    "\n",
    "def get_text_embedding_from_text_embedding_model(\n",
    "    text: str,\n",
    "    return_array: Optional[bool] = False,\n",
    ") -> list:\n",
    "    \"\"\"\n",
    "    Generates a numerical text embedding from a provided text input using a text embedding model.\n",
    "\n",
    "    Args:\n",
    "        text: The input text string to be embedded.\n",
    "        return_array: If True, returns the embedding as a NumPy array.\n",
    "                      If False, returns the embedding as a list. (Default: False)\n",
    "\n",
    "    Returns:\n",
    "        list or numpy.ndarray: A 768-dimensional vector representation of the input text.\n",
    "                               The format (list or NumPy array) depends on the\n",
    "                               value of the 'return_array' parameter.\n",
    "    \"\"\"\n",
    "    embeddings = text_embedding_model.get_embeddings([text])\n",
    "    text_embedding = [embedding.values for embedding in embeddings][0]\n",
    "\n",
    "    if return_array:\n",
    "        text_embedding = np.fromiter(text_embedding, dtype=float)\n",
    "\n",
    "    # returns 768 dimensional array\n",
    "    return text_embedding\n",
    "\n",
    "\n",
    "def get_image_embedding_from_multimodal_embedding_model(\n",
    "    image_uri: str,\n",
    "    embedding_size: int = 512,\n",
    "    text: Optional[str] = None,\n",
    "    return_array: Optional[bool] = False,\n",
    ") -> list:\n",
    "    \"\"\"Extracts an image embedding from a multimodal embedding model.\n",
    "    The function can optionally utilize contextual text to refine the embedding.\n",
    "\n",
    "    Args:\n",
    "        image_uri (str): The URI (Uniform Resource Identifier) of the image to process.\n",
    "        text (Optional[str]): Optional contextual text to guide the embedding generation. Defaults to \"\".\n",
    "        embedding_size (int): The desired dimensionality of the output embedding. Defaults to 512.\n",
    "        return_array (Optional[bool]): If True, returns the embedding as a NumPy array.\n",
    "        Otherwise, returns a list. Defaults to False.\n",
    "\n",
    "    Returns:\n",
    "        list: A list containing the image embedding values. If `return_array` is True, returns a NumPy array instead.\n",
    "    \"\"\"\n",
    "    # image = Image.load_from_file(image_uri)\n",
    "    image = vision_model_Image.load_from_file(image_uri)\n",
    "    embeddings = multimodal_embedding_model.get_embeddings(\n",
    "        image=image, contextual_text=text, dimension=embedding_size\n",
    "    )\n",
    "    image_embedding = embeddings.image_embedding\n",
    "\n",
    "    if return_array:\n",
    "        image_embedding = np.fromiter(image_embedding, dtype=float)\n",
    "\n",
    "    return image_embedding\n",
    "\n",
    "\n",
    "def load_image_bytes(image_path):\n",
    "    \"\"\"Loads an image from a URL or local file path.\n",
    "\n",
    "    Args:\n",
    "        image_uri (str): URL or local file path to the image.\n",
    "\n",
    "    Raises:\n",
    "        ValueError: If `image_uri` is not provided.\n",
    "\n",
    "    Returns:\n",
    "        bytes: Image bytes.\n",
    "    \"\"\"\n",
    "    # Check if the image_uri is provided\n",
    "    if not image_path:\n",
    "        raise ValueError(\"image_uri must be provided.\")\n",
    "\n",
    "    # Load the image from a weblink\n",
    "    if image_path.startswith(\"http://\") or image_path.startswith(\"https://\"):\n",
    "        response = requests.get(image_path, stream=True)\n",
    "        if response.status_code == 200:\n",
    "            return response.content\n",
    "\n",
    "    # Load the image from a local path\n",
    "    else:\n",
    "        return open(image_path, \"rb\").read()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Add colors to the print\n",
    "class Color:\n",
    "    \"\"\"\n",
    "    This class defines a set of color codes that can be used to print text in different colors.\n",
    "    This will be used later to print citations and results to make outputs more readable.\n",
    "    \"\"\"\n",
    "\n",
    "    PURPLE: str = \"\\033[95m\"\n",
    "    CYAN: str = \"\\033[96m\"\n",
    "    DARKCYAN: str = \"\\033[36m\"\n",
    "    BLUE: str = \"\\033[94m\"\n",
    "    GREEN: str = \"\\033[92m\"\n",
    "    YELLOW: str = \"\\033[93m\"\n",
    "    RED: str = \"\\033[91m\"\n",
    "    BOLD: str = \"\\033[1m\"\n",
    "    UNDERLINE: str = \"\\033[4m\"\n",
    "    END: str = \"\\033[0m\"\n",
    "\n",
    "\n",
    "def get_text_overlapping_chunk(\n",
    "    text: str, character_limit: int = 1000, overlap: int = 100\n",
    ") -> dict:\n",
    "    \"\"\"\n",
    "    * Breaks a text document into chunks of a specified size, with an overlap between chunks to preserve context.\n",
    "    * Takes a text document, character limit per chunk, and overlap between chunks as input.\n",
    "    * Returns a dictionary where the keys are chunk numbers and the values are the corresponding text chunks.\n",
    "\n",
    "    Args:\n",
    "        text: The text document to be chunked.\n",
    "        character_limit: Maximum characters per chunk (defaults to 1000).\n",
    "        overlap: Number of overlapping characters between chunks (defaults to 100).\n",
    "\n",
    "    Returns:\n",
    "        A dictionary where keys are chunk numbers and values are the corresponding text chunks.\n",
    "\n",
    "    Raises:\n",
    "        ValueError: If `overlap` is greater than `character_limit`.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    if overlap > character_limit:\n",
    "        raise ValueError(\"Overlap cannot be larger than character limit.\")\n",
    "\n",
    "    # Initialize variables\n",
    "    chunk_number = 1\n",
    "    chunked_text_dict = {}\n",
    "\n",
    "    # Iterate over text with the given limit and overlap\n",
    "    for i in range(0, len(text), character_limit - overlap):\n",
    "        end_index = min(i + character_limit, len(text))\n",
    "        chunk = text[i:end_index]\n",
    "\n",
    "        # Encode and decode for consistent encoding\n",
    "        chunked_text_dict[chunk_number] = chunk.encode(\"ascii\", \"ignore\").decode(\n",
    "            \"utf-8\", \"ignore\"\n",
    "        )\n",
    "\n",
    "        # Increment chunk number\n",
    "        chunk_number += 1\n",
    "\n",
    "    return chunked_text_dict\n",
    "\n",
    "\n",
    "def get_page_text_embedding(text_data: Union[dict, str]) -> dict:\n",
    "    \"\"\"\n",
    "    * Generates embeddings for each text chunk using a specified embedding model.\n",
    "    * Takes a dictionary of text chunks and an embedding size as input.\n",
    "    * Returns a dictionary where the keys are chunk numbers and the values are the corresponding embeddings.\n",
    "\n",
    "    Args:\n",
    "        text_data: Either a dictionary of pre-chunked text or the entire page text.\n",
    "        embedding_size: Size of the embedding vector (defaults to 128).\n",
    "\n",
    "    Returns:\n",
    "        A dictionary where keys are chunk numbers or \"text_embedding\" and values are the corresponding embeddings.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    embeddings_dict = {}\n",
    "\n",
    "    if isinstance(text_data, dict):\n",
    "        # Process each chunk\n",
    "        # print(text_data)\n",
    "        for chunk_number, chunk_value in text_data.items():\n",
    "            text_embd = get_text_embedding_from_text_embedding_model(text=chunk_value)\n",
    "            embeddings_dict[chunk_number] = text_embd\n",
    "    else:\n",
    "        # Process the first 1000 characters of the page text\n",
    "        text_embd = get_text_embedding_from_text_embedding_model(text=text_data)\n",
    "        embeddings_dict[\"text_embedding\"] = text_embd\n",
    "\n",
    "    return embeddings_dict\n",
    "\n",
    "\n",
    "\n",
    "def get_gemini_response(\n",
    "    generative_multimodal_model,\n",
    "    model_input: List[str],\n",
    "    stream: bool = True,\n",
    "    generation_config: Optional[GenerationConfig] = GenerationConfig(\n",
    "        temperature=0.2, max_output_tokens=2048\n",
    "    ),\n",
    "    safety_settings: Optional[dict] = {\n",
    "        HarmCategory.HARM_CATEGORY_HARASSMENT: HarmBlockThreshold.BLOCK_NONE,\n",
    "        HarmCategory.HARM_CATEGORY_HATE_SPEECH: HarmBlockThreshold.BLOCK_NONE,\n",
    "        HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT: HarmBlockThreshold.BLOCK_NONE,\n",
    "        HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT: HarmBlockThreshold.BLOCK_NONE,\n",
    "    },\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    This function generates text in response to a list of model inputs.\n",
    "\n",
    "    Args:\n",
    "        model_input: A list of strings representing the inputs to the model.\n",
    "        stream: Whether to generate the response in a streaming fashion (returning chunks of text at a time) or all at once. Defaults to False.\n",
    "\n",
    "    Returns:\n",
    "        The generated text as a string.\n",
    "    \"\"\"\n",
    "    response = generative_multimodal_model.generate_content(\n",
    "        model_input,\n",
    "        generation_config=generation_config,\n",
    "        stream=stream,\n",
    "        safety_settings=safety_settings,\n",
    "    )\n",
    "    response_list = []\n",
    "\n",
    "    for chunk in response:\n",
    "        try:\n",
    "            response_list.append(chunk.text)\n",
    "        except Exception as e:\n",
    "            print(\n",
    "                \"Exception occurred while calling gemini. Something is wrong. Lower the safety thresholds [safety_settings: BLOCK_NONE ] if not already done. -----\",\n",
    "                e,\n",
    "            )\n",
    "            response_list.append(\"Exception occurred\")\n",
    "            continue\n",
    "    response = \"\".join(response_list)\n",
    "\n",
    "    return response\n",
    "\n",
    "\n",
    "def get_text_metadata_df(\n",
    "    filename: str, text_metadata: Dict[Union[int, str], Dict]\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    This function takes a filename and a text metadata dictionary as input,\n",
    "    iterates over the text metadata dictionary and extracts the text, chunk text,\n",
    "    and chunk embeddings for each page, creates a Pandas DataFrame with the\n",
    "    extracted data, and returns it.\n",
    "\n",
    "    Args:\n",
    "        filename: The filename of the document.\n",
    "        text_metadata: A dictionary containing the text metadata for each page.\n",
    "\n",
    "    Returns:\n",
    "        A Pandas DataFrame with the extracted text, chunk text, and chunk embeddings for each page.\n",
    "    \"\"\"\n",
    "\n",
    "    final_data_text: List[Dict] = []\n",
    "\n",
    "    for key, values in text_metadata.items():\n",
    "        for chunk_number, chunk_text in values[\"chunked_text_dict\"].items():\n",
    "            data: Dict = {}\n",
    "            data[\"file_name\"] = filename\n",
    "            data[\"page_num\"] = int(key) + 1\n",
    "            data[\"text\"] = values[\"text\"]\n",
    "            data[\"text_embedding_page\"] = values[\"page_text_embeddings\"][\n",
    "                \"text_embedding\"\n",
    "            ]\n",
    "            data[\"chunk_number\"] = chunk_number\n",
    "            data[\"chunk_text\"] = chunk_text\n",
    "            data[\"text_embedding_chunk\"] = values[\"chunk_embeddings_dict\"][chunk_number]\n",
    "\n",
    "            final_data_text.append(data)\n",
    "\n",
    "    return_df = pd.DataFrame(final_data_text)\n",
    "    return_df = return_df.reset_index(drop=True)\n",
    "    return return_df\n",
    "\n",
    "\n",
    "def get_image_metadata_df(\n",
    "    filename: str, image_metadata: Dict[Union[int, str], Dict]\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    This function takes a filename and an image metadata dictionary as input,\n",
    "    iterates over the image metadata dictionary and extracts the image path,\n",
    "    image description, and image embeddings for each image, creates a Pandas\n",
    "    DataFrame with the extracted data, and returns it.\n",
    "\n",
    "    Args:\n",
    "        filename: The filename of the document.\n",
    "        image_metadata: A dictionary containing the image metadata for each page.\n",
    "\n",
    "    Returns:\n",
    "        A Pandas DataFrame with the extracted image path, image description, and image embeddings for each image.\n",
    "    \"\"\"\n",
    "\n",
    "    final_data_image: List[Dict] = []\n",
    "    for key, values in image_metadata.items():\n",
    "        for _, image_values in values.items():\n",
    "            data: Dict = {}\n",
    "            data[\"file_name\"] = filename\n",
    "            data[\"page_num\"] = int(key) + 1\n",
    "            data[\"img_num\"] = int(image_values[\"img_num\"])\n",
    "            data[\"img_path\"] = image_values[\"img_path\"]\n",
    "            data[\"img_desc\"] = image_values[\"img_desc\"]\n",
    "            # data[\"mm_embedding_from_text_desc_and_img\"] = image_values[\n",
    "            #     \"mm_embedding_from_text_desc_and_img\"\n",
    "            # ]\n",
    "            data[\"mm_embedding_from_img_only\"] = image_values[\n",
    "                \"mm_embedding_from_img_only\"\n",
    "            ]\n",
    "            data[\"text_embedding_from_image_description\"] = image_values[\n",
    "                \"text_embedding_from_image_description\"\n",
    "            ]\n",
    "            final_data_image.append(data)\n",
    "\n",
    "    return_df = pd.DataFrame(final_data_image).dropna()\n",
    "    return_df = return_df.reset_index(drop=True)\n",
    "    return return_df\n",
    "\n",
    "\n",
    "def get_document_metadata(\n",
    "    generative_multimodal_model,\n",
    "    pdf_folder_path: str,\n",
    "    image_save_dir: str,\n",
    "    image_description_prompt: str,\n",
    "    embedding_size: int = 128,\n",
    "    generation_config: Optional[GenerationConfig] = GenerationConfig(\n",
    "        temperature=0.2, max_output_tokens=2048\n",
    "    ),\n",
    "    safety_settings: Optional[dict] = {\n",
    "        HarmCategory.HARM_CATEGORY_HARASSMENT: HarmBlockThreshold.BLOCK_NONE,\n",
    "        HarmCategory.HARM_CATEGORY_HATE_SPEECH: HarmBlockThreshold.BLOCK_NONE,\n",
    "        HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT: HarmBlockThreshold.BLOCK_NONE,\n",
    "        HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT: HarmBlockThreshold.BLOCK_NONE,\n",
    "    },\n",
    "    add_sleep_after_page: bool = False,\n",
    "    sleep_time_after_page: int = 2,\n",
    ") -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    This function takes a PDF path, an image save directory, an image description prompt, an embedding size, and a text embedding text limit as input.\n",
    "\n",
    "    Args:\n",
    "        pdf_path: The path to the PDF document.\n",
    "        image_save_dir: The directory where extracted images should be saved.\n",
    "        image_description_prompt: A prompt to guide Gemini for generating image descriptions.\n",
    "        embedding_size: The dimensionality of the embedding vectors.\n",
    "        text_emb_text_limit: The maximum number of tokens for text embedding.\n",
    "\n",
    "    Returns:\n",
    "        A tuple containing two DataFrames:\n",
    "            * One DataFrame containing the extracted text metadata for each page of the PDF, including the page text, chunked text dictionaries, and chunk embedding dictionaries.\n",
    "            * Another DataFrame containing the extracted image metadata for each image in the PDF, including the image path, image description, image embeddings (with and without context), and image description text embedding.\n",
    "    \"\"\"\n",
    "\n",
    "    text_metadata_df_final, image_metadata_df_final = pd.DataFrame(), pd.DataFrame()\n",
    "\n",
    "    for pdf_path in glob.glob(pdf_folder_path + \"/*.pdf\"):\n",
    "        print(\n",
    "            \"\\n\\n\",\n",
    "            \"Processing the file: ---------------------------------\",\n",
    "            pdf_path,\n",
    "            \"\\n\\n\",\n",
    "        )\n",
    "\n",
    "        doc, num_pages = get_pdf_doc_object(pdf_path)\n",
    "\n",
    "        file_name = pdf_path.split(\"/\")[-1]\n",
    "\n",
    "        text_metadata: Dict[Union[int, str], Dict] = {}\n",
    "        image_metadata: Dict[Union[int, str], Dict] = {}\n",
    "\n",
    "        for page_num in range(num_pages):\n",
    "            print(f\"Processing page: {page_num + 1}\")\n",
    "\n",
    "            page = doc[page_num]\n",
    "\n",
    "            text = page.get_text()\n",
    "            (\n",
    "                text,\n",
    "                page_text_embeddings_dict,\n",
    "                chunked_text_dict,\n",
    "                chunk_embeddings_dict,\n",
    "            ) = get_chunk_text_metadata(page, embedding_size=embedding_size)\n",
    "\n",
    "            text_metadata[page_num] = {\n",
    "                \"text\": text,\n",
    "                \"page_text_embeddings\": page_text_embeddings_dict,\n",
    "                \"chunked_text_dict\": chunked_text_dict,\n",
    "                \"chunk_embeddings_dict\": chunk_embeddings_dict,\n",
    "            }\n",
    "\n",
    "            images = page.get_images()\n",
    "            image_metadata[page_num] = {}\n",
    "\n",
    "            for image_no, image in enumerate(images):\n",
    "                image_number = int(image_no + 1)\n",
    "                image_metadata[page_num][image_number] = {}\n",
    "\n",
    "                image_for_gemini, image_name = get_image_for_gemini(\n",
    "                    doc, image, image_no, image_save_dir, file_name, page_num\n",
    "                )\n",
    "\n",
    "                print(\n",
    "                    f\"Extracting image from page: {page_num + 1}, saved as: {image_name}\"\n",
    "                )\n",
    "\n",
    "                response = get_gemini_response(\n",
    "                    generative_multimodal_model,\n",
    "                    model_input=[image_description_prompt, image_for_gemini],\n",
    "                    generation_config=generation_config,\n",
    "                    safety_settings=safety_settings,\n",
    "                    stream=True,\n",
    "                )\n",
    "\n",
    "                image_embedding = get_image_embedding_from_multimodal_embedding_model(\n",
    "                    image_uri=image_name,\n",
    "                    embedding_size=embedding_size,\n",
    "                )\n",
    "\n",
    "                image_description_text_embedding = (\n",
    "                    get_text_embedding_from_text_embedding_model(text=response)\n",
    "                )\n",
    "\n",
    "                image_metadata[page_num][image_number] = {\n",
    "                    \"img_num\": image_number,\n",
    "                    \"img_path\": image_name,\n",
    "                    \"img_desc\": response,\n",
    "                    # \"mm_embedding_from_text_desc_and_img\": image_embedding_with_description,\n",
    "                    \"mm_embedding_from_img_only\": image_embedding,\n",
    "                    \"text_embedding_from_image_description\": image_description_text_embedding,\n",
    "                }\n",
    "\n",
    "            # Add sleep to reduce issues with Quota error on API\n",
    "            if add_sleep_after_page:\n",
    "                time.sleep(sleep_time_after_page)\n",
    "                print(\n",
    "                    \"Sleeping for \",\n",
    "                    sleep_time_after_page,\n",
    "                    \"\"\" sec before processing the next page to avoid quota issues. You can disable it: \"add_sleep_after_page = False\"  \"\"\",\n",
    "                )\n",
    "\n",
    "        text_metadata_df = get_text_metadata_df(file_name, text_metadata)\n",
    "        image_metadata_df = get_image_metadata_df(file_name, image_metadata)\n",
    "\n",
    "        text_metadata_df_final = pd.concat(\n",
    "            [text_metadata_df_final, text_metadata_df], axis=0\n",
    "        )\n",
    "        image_metadata_df_final = pd.concat(\n",
    "            [\n",
    "                image_metadata_df_final,\n",
    "                image_metadata_df.drop_duplicates(subset=[\"img_desc\"]),\n",
    "            ],\n",
    "            axis=0,\n",
    "        )\n",
    "\n",
    "        text_metadata_df_final = text_metadata_df_final.reset_index(drop=True)\n",
    "        image_metadata_df_final = image_metadata_df_final.reset_index(drop=True)\n",
    "\n",
    "    return text_metadata_df_final, image_metadata_df_final\n",
    "\n",
    "\n",
    "# Helper Functions\n",
    "\n",
    "\n",
    "def get_user_query_text_embeddings(user_query: str) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Extracts text embeddings for the user query using a text embedding model.\n",
    "\n",
    "    Args:\n",
    "        user_query: The user query text.\n",
    "        embedding_size: The desired embedding size.\n",
    "\n",
    "    Returns:\n",
    "        A NumPy array representing the user query text embedding.\n",
    "    \"\"\"\n",
    "\n",
    "    return get_text_embedding_from_text_embedding_model(user_query)\n",
    "\n",
    "\n",
    "def get_user_query_image_embeddings(\n",
    "    image_query_path: str, embedding_size: int\n",
    ") -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Extracts image embeddings for the user query image using a multimodal embedding model.\n",
    "\n",
    "    Args:\n",
    "        image_query_path: The path to the user query image.\n",
    "        embedding_size: The desired embedding size.\n",
    "\n",
    "    Returns:\n",
    "        A NumPy array representing the user query image embedding.\n",
    "    \"\"\"\n",
    "\n",
    "    return get_image_embedding_from_multimodal_embedding_model(\n",
    "        image_uri=image_query_path, embedding_size=embedding_size\n",
    "    )\n",
    "\n",
    "\n",
    "def get_cosine_score(\n",
    "    dataframe: pd.DataFrame, column_name: str, input_text_embd: np.ndarray\n",
    ") -> float:\n",
    "    \"\"\"\n",
    "    Calculates the cosine similarity between the user query embedding and the dataframe embedding for a specific column.\n",
    "\n",
    "    Args:\n",
    "        dataframe: The pandas DataFrame containing the data to compare against.\n",
    "        column_name: The name of the column containing the embeddings to compare with.\n",
    "        input_text_embd: The NumPy array representing the user query embedding.\n",
    "\n",
    "    Returns:\n",
    "        The cosine similarity score (rounded to two decimal places) between the user query embedding and the dataframe embedding.\n",
    "    \"\"\"\n",
    "\n",
    "    text_cosine_score = round(np.dot(dataframe[column_name], input_text_embd), 2)\n",
    "    return text_cosine_score\n",
    "\n",
    "\n",
    "def print_text_to_image_citation(\n",
    "    final_images: Dict[int, Dict[str, Any]], print_top: bool = True\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Prints a formatted citation for each matched image in a dictionary.\n",
    "\n",
    "    Args:\n",
    "        final_images: A dictionary containing information about matched images,\n",
    "                    with keys as image number and values as dictionaries containing\n",
    "                    image path, page number, page text, cosine similarity score, and image description.\n",
    "        print_top: A boolean flag indicating whether to only print the first citation (True) or all citations (False).\n",
    "\n",
    "    Returns:\n",
    "        None (prints formatted citations to the console).\n",
    "    \"\"\"\n",
    "\n",
    "    color = Color()\n",
    "\n",
    "    # Iterate through the matched image citations\n",
    "    for imageno, image_dict in final_images.items():\n",
    "        # Print the citation header\n",
    "        print(\n",
    "            color.RED + f\"Citation {imageno + 1}:\",\n",
    "            \"Matched image path, page number and page text: \\n\" + color.END,\n",
    "        )\n",
    "\n",
    "        # Print the cosine similarity score\n",
    "        print(color.BLUE + \"score: \" + color.END, image_dict[\"cosine_score\"])\n",
    "\n",
    "        # Print the file_name\n",
    "        print(color.BLUE + \"file_name: \" + color.END, image_dict[\"file_name\"])\n",
    "\n",
    "        # Print the image path\n",
    "        print(color.BLUE + \"path: \" + color.END, image_dict[\"img_path\"])\n",
    "\n",
    "        # Print the page number\n",
    "        print(color.BLUE + \"page number: \" + color.END, image_dict[\"page_num\"])\n",
    "\n",
    "        # Print the page text\n",
    "        print(\n",
    "            color.BLUE + \"page text: \" + color.END, \"\\n\".join(image_dict[\"page_text\"])\n",
    "        )\n",
    "\n",
    "        # Print the image description\n",
    "        print(\n",
    "            color.BLUE + \"image description: \" + color.END,\n",
    "            image_dict[\"image_description\"],\n",
    "        )\n",
    "\n",
    "        # Only print the first citation if print_top is True\n",
    "        if print_top and imageno == 0:\n",
    "            break\n",
    "\n",
    "\n",
    "def print_text_to_text_citation(\n",
    "    final_text: Dict[int, Dict[str, Any]],\n",
    "    print_top: bool = True,\n",
    "    chunk_text: bool = True,\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Prints a formatted citation for each matched text in a dictionary.\n",
    "\n",
    "    Args:\n",
    "        final_text: A dictionary containing information about matched text passages,\n",
    "                    with keys as text number and values as dictionaries containing\n",
    "                    page number, cosine similarity score, chunk number (optional),\n",
    "                    chunk text (optional), and page text (optional).\n",
    "        print_top: A boolean flag indicating whether to only print the first citation (True) or all citations (False).\n",
    "        chunk_text: A boolean flag indicating whether to print individual text chunks (True) or the entire page text (False).\n",
    "\n",
    "    Returns:\n",
    "        None (prints formatted citations to the console).\n",
    "    \"\"\"\n",
    "\n",
    "    color = Color()\n",
    "\n",
    "    # Iterate through the matched text citations\n",
    "    for textno, text_dict in final_text.items():\n",
    "        # Print the citation header\n",
    "        print(color.RED + f\"Citation {textno + 1}:\", \"Matched text: \\n\" + color.END)\n",
    "\n",
    "        # Print the cosine similarity score\n",
    "        print(color.BLUE + \"score: \" + color.END, text_dict[\"cosine_score\"])\n",
    "\n",
    "        # Print the file_name\n",
    "        print(color.BLUE + \"file_name: \" + color.END, text_dict[\"file_name\"])\n",
    "\n",
    "        # Print the page number\n",
    "        print(color.BLUE + \"page_number: \" + color.END, text_dict[\"page_num\"])\n",
    "\n",
    "        # Print the matched text based on the chunk_text argument\n",
    "        if chunk_text:\n",
    "            # Print chunk number and chunk text\n",
    "            print(color.BLUE + \"chunk_number: \" + color.END, text_dict[\"chunk_number\"])\n",
    "            print(color.BLUE + \"chunk_text: \" + color.END, text_dict[\"chunk_text\"])\n",
    "        else:\n",
    "            # Print page text\n",
    "            print(color.BLUE + \"page text: \" + color.END, text_dict[\"page_text\"])\n",
    "\n",
    "        # Only print the first citation if print_top is True\n",
    "        if print_top and textno == 0:\n",
    "            break\n",
    "\n",
    "\n",
    "def get_similar_image_from_query(\n",
    "    text_metadata_df: pd.DataFrame,\n",
    "    image_metadata_df: pd.DataFrame,\n",
    "    query: str = \"\",\n",
    "    image_query_path: str = \"\",\n",
    "    column_name: str = \"\",\n",
    "    image_emb: bool = True,\n",
    "    top_n: int = 3,\n",
    "    embedding_size: int = 128,\n",
    ") -> Dict[int, Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Finds the top N most similar images from a metadata DataFrame based on a text query or an image query.\n",
    "\n",
    "    Args:\n",
    "        text_metadata_df: A Pandas DataFrame containing text metadata associated with the images.\n",
    "        image_metadata_df: A Pandas DataFrame containing image metadata (paths, descriptions, etc.).\n",
    "        query: The text query used for finding similar images (if image_emb is False).\n",
    "        image_query_path: The path to the image used for finding similar images (if image_emb is True).\n",
    "        column_name: The column name in the image_metadata_df containing the image embeddings or captions.\n",
    "        image_emb: Whether to use image embeddings (True) or text captions (False) for comparisons.\n",
    "        top_n: The number of most similar images to return.\n",
    "        embedding_size: The dimensionality of the image embeddings (only used if image_emb is True).\n",
    "\n",
    "    Returns:\n",
    "        A dictionary containing information about the top N most similar images, including cosine scores, image objects, paths, page numbers, text excerpts, and descriptions.\n",
    "    \"\"\"\n",
    "    # Check if image embedding is used\n",
    "    if image_emb:\n",
    "        # Calculate cosine similarity between query image and metadata images\n",
    "        user_query_image_embedding = get_user_query_image_embeddings(\n",
    "            image_query_path, embedding_size\n",
    "        )\n",
    "        cosine_scores = image_metadata_df.apply(\n",
    "            lambda x: get_cosine_score(x, column_name, user_query_image_embedding),\n",
    "            axis=1,\n",
    "        )\n",
    "    else:\n",
    "        # Calculate cosine similarity between query text and metadata image captions\n",
    "        user_query_text_embedding = get_user_query_text_embeddings(query)\n",
    "        cosine_scores = image_metadata_df.apply(\n",
    "            lambda x: get_cosine_score(x, column_name, user_query_text_embedding),\n",
    "            axis=1,\n",
    "        )\n",
    "\n",
    "    # Remove same image comparison score when user image is matched exactly with metadata image\n",
    "    cosine_scores = cosine_scores[cosine_scores < 1.0]\n",
    "\n",
    "    # Get top N cosine scores and their indices\n",
    "    top_n_cosine_scores = cosine_scores.nlargest(top_n).index.tolist()\n",
    "    top_n_cosine_values = cosine_scores.nlargest(top_n).values.tolist()\n",
    "\n",
    "    # Create a dictionary to store matched images and their information\n",
    "    final_images: Dict[int, Dict[str, Any]] = {}\n",
    "\n",
    "    for matched_imageno, indexvalue in enumerate(top_n_cosine_scores):\n",
    "        # Create a sub-dictionary for each matched image\n",
    "        final_images[matched_imageno] = {}\n",
    "\n",
    "        # Store cosine score\n",
    "        final_images[matched_imageno][\"cosine_score\"] = top_n_cosine_values[\n",
    "            matched_imageno\n",
    "        ]\n",
    "\n",
    "        # Load image from file\n",
    "        final_images[matched_imageno][\"image_object\"] = Image.load_from_file(\n",
    "            image_metadata_df.iloc[indexvalue][\"img_path\"]\n",
    "        )\n",
    "\n",
    "        # Add file name\n",
    "        final_images[matched_imageno][\"file_name\"] = image_metadata_df.iloc[indexvalue][\n",
    "            \"file_name\"\n",
    "        ]\n",
    "\n",
    "        # Store image path\n",
    "        final_images[matched_imageno][\"img_path\"] = image_metadata_df.iloc[indexvalue][\n",
    "            \"img_path\"\n",
    "        ]\n",
    "\n",
    "        # Store page number\n",
    "        final_images[matched_imageno][\"page_num\"] = image_metadata_df.iloc[indexvalue][\n",
    "            \"page_num\"\n",
    "        ]\n",
    "\n",
    "        final_images[matched_imageno][\"page_text\"] = np.unique(\n",
    "            text_metadata_df[\n",
    "                (\n",
    "                    text_metadata_df[\"page_num\"].isin(\n",
    "                        [final_images[matched_imageno][\"page_num\"]]\n",
    "                    )\n",
    "                )\n",
    "                & (\n",
    "                    text_metadata_df[\"file_name\"].isin(\n",
    "                        [final_images[matched_imageno][\"file_name\"]]\n",
    "                    )\n",
    "                )\n",
    "            ][\"text\"].values\n",
    "        )\n",
    "\n",
    "        # Store image description\n",
    "        final_images[matched_imageno][\"image_description\"] = image_metadata_df.iloc[\n",
    "            indexvalue\n",
    "        ][\"img_desc\"]\n",
    "\n",
    "    return final_images\n",
    "\n",
    "\n",
    "def get_similar_text_from_query(\n",
    "    query: str,\n",
    "    text_metadata_df: pd.DataFrame,\n",
    "    column_name: str = \"\",\n",
    "    top_n: int = 3,\n",
    "    chunk_text: bool = True,\n",
    "    print_citation: bool = False,\n",
    ") -> Dict[int, Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Finds the top N most similar text passages from a metadata DataFrame based on a text query.\n",
    "\n",
    "    Args:\n",
    "        query: The text query used for finding similar passages.\n",
    "        text_metadata_df: A Pandas DataFrame containing the text metadata to search.\n",
    "        column_name: The column name in the text_metadata_df containing the text embeddings or text itself.\n",
    "        top_n: The number of most similar text passages to return.\n",
    "        embedding_size: The dimensionality of the text embeddings (only used if text embeddings are stored in the column specified by `column_name`).\n",
    "        chunk_text: Whether to return individual text chunks (True) or the entire page text (False).\n",
    "        print_citation: Whether to immediately print formatted citations for the matched text passages (True) or just return the dictionary (False).\n",
    "\n",
    "    Returns:\n",
    "        A dictionary containing information about the top N most similar text passages, including cosine scores, page numbers, chunk numbers (optional), and chunk text or page text (depending on `chunk_text`).\n",
    "\n",
    "    Raises:\n",
    "        KeyError: If the specified `column_name` is not present in the `text_metadata_df`.\n",
    "    \"\"\"\n",
    "\n",
    "    if column_name not in text_metadata_df.columns:\n",
    "        raise KeyError(f\"Column '{column_name}' not found in the 'text_metadata_df'\")\n",
    "\n",
    "    query_vector = get_user_query_text_embeddings(query)\n",
    "\n",
    "\n",
    "    # Calculate cosine similarity between query text and metadata text\n",
    "    cosine_scores = text_metadata_df.apply(\n",
    "        lambda row: get_cosine_score(\n",
    "            row,\n",
    "            column_name,\n",
    "            query_vector,\n",
    "        ),\n",
    "        axis=1,\n",
    "    )\n",
    "\n",
    "    # Get top N cosine scores and their indices\n",
    "    top_n_indices = cosine_scores.nlargest(top_n).index.tolist()\n",
    "    top_n_scores = cosine_scores.nlargest(top_n).values.tolist()\n",
    "\n",
    "    # Create a dictionary to store matched text and their information\n",
    "    final_text: Dict[int, Dict[str, Any]] = {}\n",
    "\n",
    "    for matched_textno, index in enumerate(top_n_indices):\n",
    "        # Create a sub-dictionary for each matched text\n",
    "        final_text[matched_textno] = {}\n",
    "\n",
    "        #Code subIndex  Desc Longer Desc\n",
    "\n",
    "        # Store page number\n",
    "        final_text[matched_textno][\"clip_name\"] = text_metadata_df.iloc[index][\n",
    "            \"clip_name\"\n",
    "        ]\n",
    "\n",
    "        # Store page number\n",
    "        final_text[matched_textno][\"description\t\"] = text_metadata_df.iloc[index][\n",
    "            \"description\"\n",
    "        ]\n",
    "\n",
    "        # Store cosine score\n",
    "        final_text[matched_textno][\"cosine_score\"] = top_n_scores[matched_textno]\n",
    "\n",
    "\n",
    "    return final_text\n",
    "\n",
    "\n",
    "def display_images(\n",
    "    images: Iterable[Union[str, PIL.Image.Image]], resize_ratio: float = 0.5\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Displays a series of images provided as paths or PIL Image objects.\n",
    "\n",
    "    Args:\n",
    "        images: An iterable of image paths or PIL Image objects.\n",
    "        resize_ratio: The factor by which to resize each image (default 0.5).\n",
    "\n",
    "    Returns:\n",
    "        None (displays images using IPython or Jupyter notebook).\n",
    "    \"\"\"\n",
    "\n",
    "    # Convert paths to PIL images if necessary\n",
    "    pil_images = []\n",
    "    for image in images:\n",
    "        if isinstance(image, str):\n",
    "            pil_images.append(PIL.Image.open(image))\n",
    "        else:\n",
    "            pil_images.append(image)\n",
    "\n",
    "    # Resize and display each image\n",
    "    for img in pil_images:\n",
    "        original_width, original_height = img.size\n",
    "        new_width = int(original_width * resize_ratio)\n",
    "        new_height = int(original_height * resize_ratio)\n",
    "        resized_img = img.resize((new_width, new_height))\n",
    "        display(resized_img)\n",
    "        print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9124340e-795e-41fd-a85e-326adef6811a",
   "metadata": {},
   "source": [
    "## Cut the video into subclips of specified duration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "034d4183-2240-4cd1-ad36-8987c9f7da02",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content/clips/clip_0.mp4 has been processed\n",
      "content/clips/clip_1.mp4 has been processed\n",
      "content/clips/clip_2.mp4 has been processed\n",
      "content/clips/clip_3.mp4 has been processed\n",
      "content/clips/clip_4.mp4 has been processed\n",
      "content/clips/clip_5.mp4 has been processed\n",
      "content/clips/clip_6.mp4 has been processed\n",
      "content/clips/clip_7.mp4 has been processed\n",
      "content/clips/clip_8.mp4 has been processed\n",
      "content/clips/clip_9.mp4 has been processed\n",
      "\n",
      "10 clips have been processed.\n"
     ]
    }
   ],
   "source": [
    "import moviepy.editor as mpe\n",
    "import pandas as pd\n",
    "import math\n",
    "\n",
    "# Load the video file\n",
    "clip = mpe.VideoFileClip(downloaded_video)\n",
    "\n",
    "# first result is retrieved because a single video was processed\n",
    "shots_list = []\n",
    "\n",
    "shots_df = pd.read_csv('shots.csv')\n",
    "\n",
    "# e.g. Cut into 60s clips (Change clip_duration to set the desired duration of subclips)\n",
    "clip_duration = 60\n",
    "clip_no = math.ceil(clip.duration/clip_duration)\n",
    "clip_no = int(clip_no)\n",
    "\n",
    "unnamed_id_list = []\n",
    "start_time_list = []\n",
    "end_time_list = []\n",
    "clip_name_list = []\n",
    "frame_name_list = []\n",
    "\n",
    "for i in range(clip_no):\n",
    "    start_time = i * clip_duration\n",
    "    end_time = min(start_time + clip_duration, clip.duration)\n",
    "\n",
    "    text = \"\"\n",
    "    clip_name = \"content/clips/clip_\"+str(i)+\".mp4\"\n",
    "    clip.subclip(start_time, end_time).write_videofile(clip_name, logger=None)\n",
    "    time = (start_time  + end_time)/2\n",
    "    print(f\"{clip_name} has been processed\")\n",
    "   # saving a frame at 2 second\n",
    "    frame_name = \"content/frames/frame_\"+str(i)+\".png\"\n",
    "    clip.save_frame(frame_name, t = float(time))\n",
    "\n",
    "    unnamed_id_list.append(i-0)\n",
    "    start_time_list.append(start_time)\n",
    "    end_time_list.append(end_time)\n",
    "    clip_name_list.append(clip_name)\n",
    "    frame_name_list.append(frame_name)\n",
    "      \n",
    "print(f\"\\n{clip_no} clips have been processed.\")\n",
    "        \n",
    "shots_df['id'] = unnamed_id_list\n",
    "shots_df['start_time'] = start_time_list\n",
    "shots_df['end_time'] = end_time_list\n",
    "shots_df['clip_name'] = clip_name_list\n",
    "shots_df['frame_name'] = frame_name_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b37e7bd0-e978-40b0-9ff6-856bca402dc1",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af9ba5a1-149a-4c52-9389-aaecadd18169",
   "metadata": {},
   "source": [
    "### Load the Video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "165aa04e-2884-48b6-bd18-5ba87a3d7c5f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import base64\n",
    "\n",
    "from vertexai.preview.generative_models import GenerativeModel, Part\n",
    "import base64\n",
    "\n",
    "\n",
    "text = \"\"\n",
    "with open(\"content/clips/clip_0.mp4\", \"rb\") as videoFile:\n",
    "    text = base64.b64encode(videoFile.read())\n",
    "    print(text)\n",
    "\n",
    "video1 = Part.from_data(data=base64.b64decode(text), mime_type=\"video/mp4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24c29443-dbb0-48d7-a753-d1b079954e5e",
   "metadata": {},
   "source": [
    "### Helper Functions to generate embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c06441bc-faa0-44a4-934a-220ecf55d859",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"At Google we are fully in our Gemini era Today all of our 2 billion user products use Gemini Gemini 1.5 Pro is available today in Workspace Labs Let's see how this comes to life with Google Workspace People are always searching their emails in Gmail We're working to make it much more powerful with Gemini Now you can ask Gemini to summarize all recent emails from the school Maybe you were traveling this week and you couldn't make the PTA meeting The recording of the meeting is an hour long If it's from Google Meet you can ask Gemini to give you the highlights People love using photos to search across their life With Gemini we're making that a whole lot easier And Ask Photos can also help you search your memories in a deeper way For example you might be reminiscing \\n\""
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import base64\n",
    "import vertexai\n",
    "import vertexai.preview.generative_models as generative_models\n",
    "\n",
    "\n",
    "#Generate video description using AI Model\n",
    "#Model used is customisable (default = gemini-1.0-pro)\n",
    "\n",
    "def generate(video):\n",
    "    vertexai.init(project=PROJECT_ID, location=\"us-central1\")\n",
    "    model = GenerativeModel(\"gemini-1.5-pro-001\")\n",
    "    responses = model.generate_content(\n",
    "    [\"\"\"You are provided with a short video. First, watch the whole video in full. Then, \n",
    "    describe the contents of the video in chronological order. Register everything that is said in the video as well.\n",
    "    Then, provide detailed descriptions of each subject present in the video,\n",
    "    include their appearance, emotion, actions if possible. Include any names if mentioned.\n",
    "    Go into as much detail as possible.\"\"\", video1],\n",
    "    generation_config={\n",
    "        \"max_output_tokens\": 2048,\n",
    "        \"temperature\": 0,\n",
    "        \"top_p\": 0.4,\n",
    "        \"top_k\": 32\n",
    "    },\n",
    "    safety_settings={\n",
    "          generative_models.HarmCategory.HARM_CATEGORY_HATE_SPEECH: generative_models.HarmBlockThreshold.BLOCK_MEDIUM_AND_ABOVE,\n",
    "          generative_models.HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT: generative_models.HarmBlockThreshold.BLOCK_MEDIUM_AND_ABOVE,\n",
    "          generative_models.HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT: generative_models.HarmBlockThreshold.BLOCK_MEDIUM_AND_ABOVE,\n",
    "          generative_models.HarmCategory.HARM_CATEGORY_HARASSMENT: generative_models.HarmBlockThreshold.BLOCK_MEDIUM_AND_ABOVE,\n",
    "    },\n",
    "    stream=False,\n",
    "    )\n",
    "\n",
    "  #print(responses.text, end=\"\")\n",
    "    return responses.text\n",
    "\n",
    "def generateText(video):\n",
    "    vertexai.init(project=PROJECT_ID, location=\"us-central1\")\n",
    "    model = GenerativeModel(\"gemini-1.5-pro-001\")\n",
    "    responses = model.generate_content(\n",
    "    [\"\"\"You are provided with a short video. Register everything visible in chronological order.\n",
    "    Do not include audio. Only include what you can see visually.\"\"\", video1],\n",
    "    generation_config={\n",
    "        \"max_output_tokens\": 2048,\n",
    "        \"temperature\": 0,\n",
    "        \"top_p\": 0.4,\n",
    "        \"top_k\": 32\n",
    "    },\n",
    "    safety_settings={\n",
    "          generative_models.HarmCategory.HARM_CATEGORY_HATE_SPEECH: generative_models.HarmBlockThreshold.BLOCK_MEDIUM_AND_ABOVE,\n",
    "          generative_models.HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT: generative_models.HarmBlockThreshold.BLOCK_MEDIUM_AND_ABOVE,\n",
    "          generative_models.HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT: generative_models.HarmBlockThreshold.BLOCK_MEDIUM_AND_ABOVE,\n",
    "          generative_models.HarmCategory.HARM_CATEGORY_HARASSMENT: generative_models.HarmBlockThreshold.BLOCK_MEDIUM_AND_ABOVE,\n",
    "    },\n",
    "    stream=False,\n",
    "    )\n",
    "\n",
    "  #print(responses.text, end=\"\")\n",
    "    return responses.text\n",
    "\n",
    "\n",
    "\n",
    "def generateSpeech(video):\n",
    "    \n",
    "    \n",
    "    vertexai.init(project=PROJECT_ID, location=\"us-central1\")\n",
    "    model = GenerativeModel(\"gemini-1.5-pro-001\")\n",
    "    responses = model.generate_content(\n",
    "    [\"\"\"You are provided with a short video. Watch the video in full, and listen to the audio.\n",
    "    Register everything you hear in chronological order. Convert everything into text, word for word.\"\"\", video1],\n",
    "    generation_config={\n",
    "        \"max_output_tokens\": 2048,\n",
    "        \"temperature\": 0,\n",
    "        \"top_p\": 0.4,\n",
    "        \"top_k\": 32\n",
    "    },\n",
    "    safety_settings={\n",
    "          generative_models.HarmCategory.HARM_CATEGORY_HATE_SPEECH: generative_models.HarmBlockThreshold.BLOCK_MEDIUM_AND_ABOVE,\n",
    "          generative_models.HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT: generative_models.HarmBlockThreshold.BLOCK_MEDIUM_AND_ABOVE,\n",
    "          generative_models.HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT: generative_models.HarmBlockThreshold.BLOCK_MEDIUM_AND_ABOVE,\n",
    "          generative_models.HarmCategory.HARM_CATEGORY_HARASSMENT: generative_models.HarmBlockThreshold.BLOCK_MEDIUM_AND_ABOVE,\n",
    "    },\n",
    "    stream=False,\n",
    "    )\n",
    "\n",
    "  #print(responses.text, end=\"\")\n",
    "    return responses.text\n",
    "\n",
    "def generateObjects(video):\n",
    "    vertexai.init(project=PROJECT_ID, location=\"us-central1\")\n",
    "    model = GenerativeModel(\"gemini-1.5-pro-001\")\n",
    "    responses = model.generate_content(\n",
    "    [\"\"\"You are provided with a short video. Describe all the different people or animals that appear throughout the video. Associate them with\n",
    "    a name if mentioned.\n",
    "    Include their appearance, emotions, actions if possible. Do so in great detail.\"\"\", video1],\n",
    "    generation_config={\n",
    "        \"max_output_tokens\": 2048,\n",
    "        \"temperature\": 0,\n",
    "        \"top_p\": 0.4,\n",
    "        \"top_k\": 32\n",
    "    },\n",
    "    safety_settings={\n",
    "          generative_models.HarmCategory.HARM_CATEGORY_HATE_SPEECH: generative_models.HarmBlockThreshold.BLOCK_MEDIUM_AND_ABOVE,\n",
    "          generative_models.HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT: generative_models.HarmBlockThreshold.BLOCK_MEDIUM_AND_ABOVE,\n",
    "          generative_models.HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT: generative_models.HarmBlockThreshold.BLOCK_MEDIUM_AND_ABOVE,\n",
    "          generative_models.HarmCategory.HARM_CATEGORY_HARASSMENT: generative_models.HarmBlockThreshold.BLOCK_MEDIUM_AND_ABOVE,\n",
    "    },\n",
    "    stream=False,\n",
    "    )\n",
    "\n",
    "  #print(responses.text, end=\"\")\n",
    "    return responses.text\n",
    "\n",
    "generateSpeech(video1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e53b6c49-1796-4a68-aa1b-76afd29a6d20",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>start_time</th>\n",
       "      <th>end_time</th>\n",
       "      <th>clip_name</th>\n",
       "      <th>frame_name</th>\n",
       "      <th>associated_text</th>\n",
       "      <th>associated_speech</th>\n",
       "      <th>associated_object</th>\n",
       "      <th>description</th>\n",
       "      <th>id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>content/clips/clip_0.mp4</td>\n",
       "      <td>content/frames/frame_0.png</td>\n",
       "      <td>The video starts with a white screen and a rec...</td>\n",
       "      <td>At Google we are fully in our Gemini era Today...</td>\n",
       "      <td>The video features Sundar Pichai, CEO of Googl...</td>\n",
       "      <td>The video starts with the Google I/O logo anim...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>60</td>\n",
       "      <td>120.0</td>\n",
       "      <td>content/clips/clip_1.mp4</td>\n",
       "      <td>content/frames/frame_1.png</td>\n",
       "      <td>The video starts with a man with glasses stand...</td>\n",
       "      <td>about your daughter Lucia's early milestones Y...</td>\n",
       "      <td>The video features a single speaker, a man who...</td>\n",
       "      <td>The video starts with a man standing on a stag...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>120</td>\n",
       "      <td>180.0</td>\n",
       "      <td>content/clips/clip_2.mp4</td>\n",
       "      <td>content/frames/frame_2.png</td>\n",
       "      <td>- A man with glasses and a gray shirt is stand...</td>\n",
       "      <td>the opportunities we see with AI agents I thin...</td>\n",
       "      <td>The video features two people.\\n\\nThe first pe...</td>\n",
       "      <td>The video starts with Sundar Pichai, CEO of Go...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NaN</td>\n",
       "      <td>180</td>\n",
       "      <td>240.0</td>\n",
       "      <td>content/clips/clip_3.mp4</td>\n",
       "      <td>content/frames/frame_3.png</td>\n",
       "      <td>The video starts with a person holding a phone...</td>\n",
       "      <td>vector IV Do you remember where you saw my gla...</td>\n",
       "      <td>The video starts with a pair of hands holding ...</td>\n",
       "      <td>The video starts with a person holding a smart...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NaN</td>\n",
       "      <td>240</td>\n",
       "      <td>300.0</td>\n",
       "      <td>content/clips/clip_4.mp4</td>\n",
       "      <td>content/frames/frame_4.png</td>\n",
       "      <td>The video starts with a black screen and five ...</td>\n",
       "      <td>video prompts It can capture the details of yo...</td>\n",
       "      <td>The video starts with a fast-tracking shot thr...</td>\n",
       "      <td>The video starts with a montage of images, inc...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Id  start_time  end_time                 clip_name  \\\n",
       "0  NaN           0      60.0  content/clips/clip_0.mp4   \n",
       "1  NaN          60     120.0  content/clips/clip_1.mp4   \n",
       "2  NaN         120     180.0  content/clips/clip_2.mp4   \n",
       "3  NaN         180     240.0  content/clips/clip_3.mp4   \n",
       "4  NaN         240     300.0  content/clips/clip_4.mp4   \n",
       "\n",
       "                   frame_name  \\\n",
       "0  content/frames/frame_0.png   \n",
       "1  content/frames/frame_1.png   \n",
       "2  content/frames/frame_2.png   \n",
       "3  content/frames/frame_3.png   \n",
       "4  content/frames/frame_4.png   \n",
       "\n",
       "                                     associated_text  \\\n",
       "0  The video starts with a white screen and a rec...   \n",
       "1  The video starts with a man with glasses stand...   \n",
       "2  - A man with glasses and a gray shirt is stand...   \n",
       "3  The video starts with a person holding a phone...   \n",
       "4  The video starts with a black screen and five ...   \n",
       "\n",
       "                                   associated_speech  \\\n",
       "0  At Google we are fully in our Gemini era Today...   \n",
       "1  about your daughter Lucia's early milestones Y...   \n",
       "2  the opportunities we see with AI agents I thin...   \n",
       "3  vector IV Do you remember where you saw my gla...   \n",
       "4  video prompts It can capture the details of yo...   \n",
       "\n",
       "                                   associated_object  \\\n",
       "0  The video features Sundar Pichai, CEO of Googl...   \n",
       "1  The video features a single speaker, a man who...   \n",
       "2  The video features two people.\\n\\nThe first pe...   \n",
       "3  The video starts with a pair of hands holding ...   \n",
       "4  The video starts with a fast-tracking shot thr...   \n",
       "\n",
       "                                         description  id  \n",
       "0  The video starts with the Google I/O logo anim...   0  \n",
       "1  The video starts with a man standing on a stag...   1  \n",
       "2  The video starts with Sundar Pichai, CEO of Go...   2  \n",
       "3  The video starts with a person holding a smart...   3  \n",
       "4  The video starts with a montage of images, inc...   4  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "associated_desc=[]\n",
    "associated_speech=[]\n",
    "associated_text=[]\n",
    "associated_object=[]\n",
    "\n",
    "\n",
    "\n",
    "for i in range(clip_no):\n",
    "    clip_name = f\"clip_{i}\"\n",
    "    text = \"\"\n",
    "    with open(f\"content/clips/{clip_name}.mp4\", \"rb\") as videoFile:\n",
    "        text = base64.b64encode(videoFile.read())\n",
    "\n",
    "        video1 = Part.from_data(data=base64.b64decode(text), mime_type=\"video/mp4\")\n",
    "        desc = generate(video1)\n",
    "        associated_desc.append(desc)\n",
    "        speechDesc = generateSpeech(video1)\n",
    "        associated_speech.append(speechDesc)\n",
    "        textDesc = generateText(video1)\n",
    "        associated_text.append(textDesc)\n",
    "        objDesc = generateObjects(video1)\n",
    "        associated_object.append(objDesc)\n",
    "        \n",
    "        \n",
    "for i in range(clip_no):\n",
    "    clip_name = f\"clip_{i}\"\n",
    "    text = \"\"\n",
    "    with open(f\"content/clips/{clip_name}.mp4\", \"rb\") as videoFile:\n",
    "        text = base64.b64encode(videoFile.read())\n",
    "\n",
    "        video1 = Part.from_data(data=base64.b64decode(text), mime_type=\"video/mp4\")\n",
    "        \n",
    "\n",
    "shots_df['description'] = associated_desc\n",
    "shots_df['associated_speech'] = associated_speech\n",
    "shots_df['associated_text'] = associated_text\n",
    "shots_df['associated_object'] = associated_object\n",
    "shots_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "963b997d-98b7-4967-8811-e4f40356301f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from vertexai.language_models import TextEmbeddingModel\n",
    "text_embedding_model = TextEmbeddingModel.from_pretrained(\"textembedding-gecko@003\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "36a420c9-b1e9-4679-8756-bbbe629f0ebb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "embeddingsList = []\n",
    "for i in range(clip_no):\n",
    "    string = shots_df['description'][i] + \"\\n\" + shots_df['associated_speech'][i] + \"\\n\" + shots_df['associated_text'][i] + \"\\n\" +  shots_df['associated_object'][i]\n",
    "    embeddings = text_embedding_model.get_embeddings([string])\n",
    "    text_embedding = [embedding.values for embedding in embeddings][0]\n",
    "    embeddingsList.append(text_embedding)\n",
    "shots_df[\"embedding\"] = embeddingsList"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7027e27f-785d-4ced-916f-962df4a44fe2",
   "metadata": {},
   "source": [
    "### Preview Dataframe "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f72e79b8-b71c-4784-8cbd-39b6147a5ef9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>start_time</th>\n",
       "      <th>end_time</th>\n",
       "      <th>clip_name</th>\n",
       "      <th>frame_name</th>\n",
       "      <th>associated_text</th>\n",
       "      <th>associated_speech</th>\n",
       "      <th>associated_object</th>\n",
       "      <th>description</th>\n",
       "      <th>id</th>\n",
       "      <th>embedding</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>content/clips/clip_0.mp4</td>\n",
       "      <td>content/frames/frame_0.png</td>\n",
       "      <td>The video starts with a white screen and a rec...</td>\n",
       "      <td>At Google we are fully in our Gemini era Today...</td>\n",
       "      <td>The video features Sundar Pichai, CEO of Googl...</td>\n",
       "      <td>The video starts with the Google I/O logo anim...</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.02301250398159027, 0.01784183457493782, -0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>60</td>\n",
       "      <td>120.0</td>\n",
       "      <td>content/clips/clip_1.mp4</td>\n",
       "      <td>content/frames/frame_1.png</td>\n",
       "      <td>The video starts with a man with glasses stand...</td>\n",
       "      <td>about your daughter Lucia's early milestones Y...</td>\n",
       "      <td>The video features a single speaker, a man who...</td>\n",
       "      <td>The video starts with a man standing on a stag...</td>\n",
       "      <td>1</td>\n",
       "      <td>[0.018422599881887436, 0.025941209867596626, -...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>120</td>\n",
       "      <td>180.0</td>\n",
       "      <td>content/clips/clip_2.mp4</td>\n",
       "      <td>content/frames/frame_2.png</td>\n",
       "      <td>- A man with glasses and a gray shirt is stand...</td>\n",
       "      <td>the opportunities we see with AI agents I thin...</td>\n",
       "      <td>The video features two people.\\n\\nThe first pe...</td>\n",
       "      <td>The video starts with Sundar Pichai, CEO of Go...</td>\n",
       "      <td>2</td>\n",
       "      <td>[-0.0005373490275815129, -0.004592179320752621...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NaN</td>\n",
       "      <td>180</td>\n",
       "      <td>240.0</td>\n",
       "      <td>content/clips/clip_3.mp4</td>\n",
       "      <td>content/frames/frame_3.png</td>\n",
       "      <td>The video starts with a person holding a phone...</td>\n",
       "      <td>vector IV Do you remember where you saw my gla...</td>\n",
       "      <td>The video starts with a pair of hands holding ...</td>\n",
       "      <td>The video starts with a person holding a smart...</td>\n",
       "      <td>3</td>\n",
       "      <td>[0.028424551710486412, -0.034521788358688354, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NaN</td>\n",
       "      <td>240</td>\n",
       "      <td>300.0</td>\n",
       "      <td>content/clips/clip_4.mp4</td>\n",
       "      <td>content/frames/frame_4.png</td>\n",
       "      <td>The video starts with a black screen and five ...</td>\n",
       "      <td>video prompts It can capture the details of yo...</td>\n",
       "      <td>The video starts with a fast-tracking shot thr...</td>\n",
       "      <td>The video starts with a montage of images, inc...</td>\n",
       "      <td>4</td>\n",
       "      <td>[0.01569233275949955, -0.006282717455178499, -...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Id  start_time  end_time                 clip_name  \\\n",
       "0  NaN           0      60.0  content/clips/clip_0.mp4   \n",
       "1  NaN          60     120.0  content/clips/clip_1.mp4   \n",
       "2  NaN         120     180.0  content/clips/clip_2.mp4   \n",
       "3  NaN         180     240.0  content/clips/clip_3.mp4   \n",
       "4  NaN         240     300.0  content/clips/clip_4.mp4   \n",
       "\n",
       "                   frame_name  \\\n",
       "0  content/frames/frame_0.png   \n",
       "1  content/frames/frame_1.png   \n",
       "2  content/frames/frame_2.png   \n",
       "3  content/frames/frame_3.png   \n",
       "4  content/frames/frame_4.png   \n",
       "\n",
       "                                     associated_text  \\\n",
       "0  The video starts with a white screen and a rec...   \n",
       "1  The video starts with a man with glasses stand...   \n",
       "2  - A man with glasses and a gray shirt is stand...   \n",
       "3  The video starts with a person holding a phone...   \n",
       "4  The video starts with a black screen and five ...   \n",
       "\n",
       "                                   associated_speech  \\\n",
       "0  At Google we are fully in our Gemini era Today...   \n",
       "1  about your daughter Lucia's early milestones Y...   \n",
       "2  the opportunities we see with AI agents I thin...   \n",
       "3  vector IV Do you remember where you saw my gla...   \n",
       "4  video prompts It can capture the details of yo...   \n",
       "\n",
       "                                   associated_object  \\\n",
       "0  The video features Sundar Pichai, CEO of Googl...   \n",
       "1  The video features a single speaker, a man who...   \n",
       "2  The video features two people.\\n\\nThe first pe...   \n",
       "3  The video starts with a pair of hands holding ...   \n",
       "4  The video starts with a fast-tracking shot thr...   \n",
       "\n",
       "                                         description  id  \\\n",
       "0  The video starts with the Google I/O logo anim...   0   \n",
       "1  The video starts with a man standing on a stag...   1   \n",
       "2  The video starts with Sundar Pichai, CEO of Go...   2   \n",
       "3  The video starts with a person holding a smart...   3   \n",
       "4  The video starts with a montage of images, inc...   4   \n",
       "\n",
       "                                           embedding  \n",
       "0  [0.02301250398159027, 0.01784183457493782, -0....  \n",
       "1  [0.018422599881887436, 0.025941209867596626, -...  \n",
       "2  [-0.0005373490275815129, -0.004592179320752621...  \n",
       "3  [0.028424551710486412, -0.034521788358688354, ...  \n",
       "4  [0.01569233275949955, -0.006282717455178499, -...  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shots_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "39b69218-7bf9-4586-9823-c45227159e5c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copying file://content/shots.csv [Content-Type=text/csv]...\n",
      "/ [1 files][235.8 KiB/235.8 KiB]                                                \n",
      "Operation completed over 1 objects/235.8 KiB.                                    \n"
     ]
    }
   ],
   "source": [
    "shots_df.to_csv('content/shots.csv')\n",
    "# Copy the file to our new bucket.\n",
    "!gsutil cp content/shots.csv {BUCKET_URI}/shots.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e3b1956-0c36-4a87-8a81-715fad307411",
   "metadata": {},
   "source": [
    "### Convert the Dataframe into a JSON file, which will be uploaded into Vector Search Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6152d8e8-6b82-41ba-9a0e-de72d1f4fcd8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "\n",
    "BUCKET_URI_ME=f\"{BUCKET_URI}/embeddings/\"\n",
    "LOCATION = 'us-central1'\n",
    "\n",
    "from google.cloud import aiplatform\n",
    "\n",
    "aiplatform.init(project=PROJECT_ID, location=LOCATION)\n",
    "\n",
    "jsonl_string = shots_df[['id','start_time','end_time','clip_name', 'frame_name', \"description\", \"embedding\"]].to_json(orient=\"records\", lines=True)\n",
    "with open(f\"./videodata.json\", \"w\") as f:\n",
    "    f.write(jsonl_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "683ba9bf-2079-41c0-a51b-2e11e9bca8ac",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copying file://videodata.json [Content-Type=application/json]...\n",
      "/ [1 files][133.1 KiB/133.1 KiB]                                                \n",
      "Operation completed over 1 objects/133.1 KiB.                                    \n"
     ]
    }
   ],
   "source": [
    "! gsutil cp videodata.json {BUCKET_URI_ME}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf914184-1944-4996-9de9-f7f1150b129e",
   "metadata": {},
   "source": [
    "## Creating Vector Search Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a7fba175-9018-4dea-b4f1-19ea07180066",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating MatchingEngineIndex\n",
      "Create MatchingEngineIndex backing LRO: projects/255766800726/locations/us-central1/indexes/8368299436119425024/operations/2987475519047467008\n",
      "MatchingEngineIndex created. Resource name: projects/255766800726/locations/us-central1/indexes/8368299436119425024\n",
      "To use this MatchingEngineIndex in another session:\n",
      "index = aiplatform.MatchingEngineIndex('projects/255766800726/locations/us-central1/indexes/8368299436119425024')\n"
     ]
    }
   ],
   "source": [
    "# create Index\n",
    "my_index = aiplatform.MatchingEngineIndex.create_tree_ah_index(\n",
    "    display_name=f\"vs-feature-index-{UID}\",\n",
    "    contents_delta_uri=BUCKET_URI_ME,\n",
    "    dimensions=768,\n",
    "    approximate_neighbors_count=10,\n",
    "    project = PROJECT_ID\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab66e05f-22c0-4225-bb12-06286e016252",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Create Index Endpoint and deploy the Index\n",
    "To use the Index, you need to create an Index Endpoint. It works as a server instance accepting query requests for your Index.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "72c81bf3-11f8-4536-ad33-ad3ad6ebfdd3",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating MatchingEngineIndexEndpoint\n",
      "Create MatchingEngineIndexEndpoint backing LRO: projects/255766800726/locations/us-central1/indexEndpoints/8281781065152987136/operations/9076342215252377600\n",
      "MatchingEngineIndexEndpoint created. Resource name: projects/255766800726/locations/us-central1/indexEndpoints/8281781065152987136\n",
      "To use this MatchingEngineIndexEndpoint in another session:\n",
      "index_endpoint = aiplatform.MatchingEngineIndexEndpoint('projects/255766800726/locations/us-central1/indexEndpoints/8281781065152987136')\n"
     ]
    }
   ],
   "source": [
    "# create IndexEndpoint\n",
    "my_index_endpoint = aiplatform.MatchingEngineIndexEndpoint.create(\n",
    "    display_name=f\"darryl-testing-lh-{UID}\", public_endpoint_enabled=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77a5f274-a090-4477-b296-5fa2f02f8b35",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deploying index MatchingEngineIndexEndpoint index_endpoint: projects/255766800726/locations/us-central1/indexEndpoints/8281781065152987136\n",
      "Deploy index MatchingEngineIndexEndpoint index_endpoint backing LRO: projects/255766800726/locations/us-central1/indexEndpoints/8281781065152987136/operations/5333287974954074112\n",
      "MatchingEngineIndexEndpoint index_endpoint Deployed index. Resource name: projects/255766800726/locations/us-central1/indexEndpoints/8281781065152987136\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<google.cloud.aiplatform.matching_engine.matching_engine_index_endpoint.MatchingEngineIndexEndpoint object at 0x7f5e7a9dbe50> \n",
       "resource name: projects/255766800726/locations/us-central1/indexEndpoints/8281781065152987136"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DEPLOYED_INDEX_ID = f\"darryl_testing_lh_{UID}\"\n",
    "# deploy the Index to the Index Endpoint\n",
    "my_index_endpoint.deploy_index(index=my_index, deployed_index_id=DEPLOYED_INDEX_ID)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc16288d-f9f0-4f29-898c-5100fb83284a",
   "metadata": {},
   "source": [
    "#### Go to your Vertex AI console and check that the index is CREATED successfully "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a3ba04c-a5ec-432b-a098-1205d937647f",
   "metadata": {},
   "source": [
    "### Get an existing Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "86531ab4-261a-4f34-924c-cce05d4cb0b9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Ignoring invalid distribution -ryptography (/opt/conda/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -ryptography (/opt/conda/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.1.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install tqdm -q\n",
    "import tqdm\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "fb63a10d-c8ab-455e-a9af-65c609f280d3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "REGION = LOCATION = \"us-central1\"\n",
    "\n",
    "aiplatform.init(project=PROJECT_ID, location=LOCATION)\n",
    "vertexai.init(project=PROJECT_ID, location=LOCATION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "1e1f9a72-6e82-41c5-92d2-3ac3094d1eb8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'my-project-0004-346516'"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_index_endpoint_id= '2987475519047467008' #'8368299436119425024'\n",
    "PROJECT_ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "bbe54c90-9902-4e70-84a1-33f7ef9636c8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "my_index_name = my_index._gca_resource.name\n",
    "my_index_display_name = my_index.display_name\n",
    "my_index_id = my_index.name.split('/')[-1]\n",
    "\n",
    "my_index_endpoint_name = my_index_endpoint._gca_resource.name\n",
    "my_index_endpoint_display_name = my_index_endpoint.display_name\n",
    "my_index_endpoint_id = my_index_endpoint.name.split('/')[-1]\n",
    "my_index_endpoint_public_domain = my_index_endpoint.public_endpoint_domain_name\n",
    "\n",
    "my_index = aiplatform.MatchingEngineIndex(my_index_name)\n",
    "\n",
    "my_index_endpoint = aiplatform.MatchingEngineIndexEndpoint(my_index_endpoint_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "2829325d-9680-445e-9ad1-4eefd4029f8b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "631876179.us-central1-255766800726.vdb.vertexai.goog\n",
      "projects/255766800726/locations/us-central1/indexEndpoints/8281781065152987136\n",
      "darryl_testing_lh_pytorch112kagglewbi\n"
     ]
    }
   ],
   "source": [
    "from google.cloud import aiplatform_v1\n",
    "\n",
    "# Set variables for the current deployed index.\n",
    "API_ENDPOINT=my_index_endpoint_public_domain\n",
    "INDEX_ENDPOINT=my_index_endpoint_name\n",
    "\n",
    "indexendpoint_id=UNIQUE_PREFIX\n",
    "\n",
    "DEPLOYED_INDEX_ID=\"darryl_testing_lh_\" + indexendpoint_id\n",
    "neighbor_count = 3\n",
    "\n",
    "print(API_ENDPOINT)\n",
    "print(INDEX_ENDPOINT)\n",
    "print(DEPLOYED_INDEX_ID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "9453a621-7362-4d82-9450-b78a370bba18",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# %pip install -q langchain_google_vertexai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "d4b9293b-ad30-414f-8e6b-b9009475f2a1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain_google_vertexai import VertexAIEmbeddings\n",
    "\n",
    "embeddings = VertexAIEmbeddings(model=\"textembedding-gecko@003\", model_name=\"textembedding-gecko@003\")\n",
    "\n",
    "text_embedding_model = embeddings #TextEmbeddingModel.from_pretrained(\"textembedding-gecko@003\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2df9812c-c1ee-423a-af56-7235da1e1e9a",
   "metadata": {},
   "source": [
    "### Querying using Vector Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "949c0248-1299-4e33-b0dc-f6f34155ec24",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Set your query\n",
    "query = \"Give me the clips where there are animals present\"\n",
    "\n",
    "#Set the number of results you want\n",
    "neighbor_count = 3\n",
    "\n",
    "test_embeddings = embeddings.embed_query(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "1e74441a-4107-49a3-987f-e8918df3d45b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "neighbor_count 3\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>start_time</th>\n",
       "      <th>end_time</th>\n",
       "      <th>clip_name</th>\n",
       "      <th>frame_name</th>\n",
       "      <th>associated_text</th>\n",
       "      <th>associated_speech</th>\n",
       "      <th>associated_object</th>\n",
       "      <th>description</th>\n",
       "      <th>id</th>\n",
       "      <th>embedding</th>\n",
       "      <th>distance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>120</td>\n",
       "      <td>180.0</td>\n",
       "      <td>content/clips/clip_2.mp4</td>\n",
       "      <td>content/frames/frame_2.png</td>\n",
       "      <td>- A man with glasses and a gray shirt is stand...</td>\n",
       "      <td>the opportunities we see with AI agents I thin...</td>\n",
       "      <td>The video features two people.\\n\\nThe first pe...</td>\n",
       "      <td>The video starts with Sundar Pichai, CEO of Go...</td>\n",
       "      <td>2</td>\n",
       "      <td>[-0.0005373490275815129, -0.004592179320752621...</td>\n",
       "      <td>0.603239</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NaN</td>\n",
       "      <td>240</td>\n",
       "      <td>300.0</td>\n",
       "      <td>content/clips/clip_4.mp4</td>\n",
       "      <td>content/frames/frame_4.png</td>\n",
       "      <td>The video starts with a black screen and five ...</td>\n",
       "      <td>video prompts It can capture the details of yo...</td>\n",
       "      <td>The video starts with a fast-tracking shot thr...</td>\n",
       "      <td>The video starts with a montage of images, inc...</td>\n",
       "      <td>4</td>\n",
       "      <td>[0.01569233275949955, -0.006282717455178499, -...</td>\n",
       "      <td>0.606726</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NaN</td>\n",
       "      <td>180</td>\n",
       "      <td>240.0</td>\n",
       "      <td>content/clips/clip_3.mp4</td>\n",
       "      <td>content/frames/frame_3.png</td>\n",
       "      <td>The video starts with a person holding a phone...</td>\n",
       "      <td>vector IV Do you remember where you saw my gla...</td>\n",
       "      <td>The video starts with a pair of hands holding ...</td>\n",
       "      <td>The video starts with a person holding a smart...</td>\n",
       "      <td>3</td>\n",
       "      <td>[0.028424551710486412, -0.034521788358688354, ...</td>\n",
       "      <td>0.637677</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Id  start_time  end_time                 clip_name  \\\n",
       "2  NaN         120     180.0  content/clips/clip_2.mp4   \n",
       "4  NaN         240     300.0  content/clips/clip_4.mp4   \n",
       "3  NaN         180     240.0  content/clips/clip_3.mp4   \n",
       "\n",
       "                   frame_name  \\\n",
       "2  content/frames/frame_2.png   \n",
       "4  content/frames/frame_4.png   \n",
       "3  content/frames/frame_3.png   \n",
       "\n",
       "                                     associated_text  \\\n",
       "2  - A man with glasses and a gray shirt is stand...   \n",
       "4  The video starts with a black screen and five ...   \n",
       "3  The video starts with a person holding a phone...   \n",
       "\n",
       "                                   associated_speech  \\\n",
       "2  the opportunities we see with AI agents I thin...   \n",
       "4  video prompts It can capture the details of yo...   \n",
       "3  vector IV Do you remember where you saw my gla...   \n",
       "\n",
       "                                   associated_object  \\\n",
       "2  The video features two people.\\n\\nThe first pe...   \n",
       "4  The video starts with a fast-tracking shot thr...   \n",
       "3  The video starts with a pair of hands holding ...   \n",
       "\n",
       "                                         description  id  \\\n",
       "2  The video starts with Sundar Pichai, CEO of Go...   2   \n",
       "4  The video starts with a montage of images, inc...   4   \n",
       "3  The video starts with a person holding a smart...   3   \n",
       "\n",
       "                                           embedding  distance  \n",
       "2  [-0.0005373490275815129, -0.004592179320752621...  0.603239  \n",
       "4  [0.01569233275949955, -0.006282717455178499, -...  0.606726  \n",
       "3  [0.028424551710486412, -0.034521788358688354, ...  0.637677  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "# Configure Vector Search client\n",
    "client_options = {\n",
    "  \"api_endpoint\": API_ENDPOINT\n",
    "}\n",
    "vector_search_client = aiplatform_v1.MatchServiceClient(\n",
    "  client_options=client_options,\n",
    ")\n",
    "# Build FindNeighborsRequest object\n",
    "datapoint = aiplatform_v1.IndexDatapoint(\n",
    "  feature_vector=test_embeddings\n",
    ")\n",
    "\n",
    "query = aiplatform_v1.FindNeighborsRequest.Query(\n",
    "  datapoint=datapoint,\n",
    "  # The number of nearest neighbors to be retrieved\n",
    "  neighbor_count=neighbor_count\n",
    ")\n",
    "\n",
    "request = aiplatform_v1.FindNeighborsRequest(\n",
    "  index_endpoint=INDEX_ENDPOINT,\n",
    "  deployed_index_id=DEPLOYED_INDEX_ID,\n",
    "  # Request can have multiple queries\n",
    "  queries=[query],\n",
    "  return_full_datapoint=False,\n",
    ")\n",
    "\n",
    "# Execute the request\n",
    "response = vector_search_client.find_neighbors(request)\n",
    "\n",
    "df_new = pd.DataFrame()\n",
    "print('neighbor_count', neighbor_count)\n",
    "\n",
    "shots_df['distance'] = None\n",
    "\n",
    "for i in range(0,neighbor_count):\n",
    "    x=response.nearest_neighbors[0]\n",
    "    \n",
    "    df_match = shots_df.loc[shots_df['id'] == int(x.neighbors[i].datapoint.datapoint_id) ]\n",
    "    df_match['distance'] = x.neighbors[i].distance\n",
    "\n",
    "    # Append the matching rows to the new DataFrame\n",
    "    df_new = pd.concat([df_new, df_match])\n",
    "    \n",
    "\n",
    "# Print the new DataFrame\n",
    "df_sorted = df_new.sort_values(by=\"distance\", ascending=True)\n",
    "print(display(df_sorted))\n",
    "\n",
    "#Export DataFrame to CSV file for reference\n",
    "df_new.to_csv(\"results.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "d2a62998-8b5e-48e5-a951-2ac47c8bccbe",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['content/clips/clip_2.mp4', 'content/clips/clip_4.mp4', 'content/clips/clip_3.mp4']\n"
     ]
    }
   ],
   "source": [
    "clipNames = []\n",
    "\n",
    "for i in df_sorted['clip_name']:\n",
    "    clipNames.append(i)\n",
    "    \n",
    "#Display the clips     \n",
    "print(clipNames)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c206cddf-9022-43a1-b0e3-bbbad439bb88",
   "metadata": {},
   "source": [
    "### Display Clips"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "56918019-cd58-4329-b6bb-4eea1bfc224b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<video src=\"content/clips/clip_2.mp4\" controls  width=\"600\" >\n",
       "      Your browser does not support the <code>video</code> element.\n",
       "    </video>"
      ],
      "text/plain": [
       "<IPython.core.display.Video object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<video src=\"content/clips/clip_4.mp4\" controls  width=\"600\" >\n",
       "      Your browser does not support the <code>video</code> element.\n",
       "    </video>"
      ],
      "text/plain": [
       "<IPython.core.display.Video object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<video src=\"content/clips/clip_3.mp4\" controls  width=\"600\" >\n",
       "      Your browser does not support the <code>video</code> element.\n",
       "    </video>"
      ],
      "text/plain": [
       "<IPython.core.display.Video object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import IPython\n",
    "\n",
    "for i in clipNames:\n",
    "    IPython.display.display(IPython.display.Video(i, width=600))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "45df467d-ff6a-47e6-a84d-7908c2af07bd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "PROJECT_ID = !(gcloud config get-value core/project)\n",
    "PROJECT_ID = PROJECT_ID[0]\n",
    "\n",
    "SVC_ACC = !(gcloud config get-value core/account)\n",
    "SVC_ACC = SVC_ACC[0]\n",
    "\n",
    "PROJECT_NUMBER=str(re.search(r'\\d+', SVC_ACC).group())\n",
    "\n",
    "FOLDER_NAME=\".\"\n",
    "\n",
    "dataset_id = \"darryl_video_rag_demo_lh\"\n",
    "table_id = \"video_embeddings\"\n",
    "LOCATION=\"us-central1\"\n",
    "full_table_id = '.'.join([PROJECT_ID,dataset_id,table_id])\n",
    "file_path = \"embeddings_videodata.json\"\n",
    "\n",
    "\n",
    "from google.cloud import bigquery\n",
    "client = bigquery.Client(project=PROJECT_ID)\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "acf91bbd-ee84-4ad2-bf74-ccee45f66811",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def create_bigquery_dataset():\n",
    "    dataset = client.dataset(dataset_id)\n",
    "    dataset = client.create_dataset(dataset, timeout=30)  # Make an API request.\n",
    "    print(\"Created dataset {}.{}\".format(client.project, dataset.dataset_id))\n",
    "\n",
    "def create_bigquery_table():    \n",
    "    # Define the schema for the table\n",
    "    schema = [\n",
    "        bigquery.SchemaField(\"id\", \"STRING\"),\n",
    "        bigquery.SchemaField(\"start_time\", \"STRING\"),\n",
    "        bigquery.SchemaField(\"end_time\", \"STRING\"),\n",
    "        bigquery.SchemaField(\"clip_name\", \"STRING\"),\n",
    "        bigquery.SchemaField(\"frame_name\", \"STRING\"),\n",
    "        bigquery.SchemaField(\"description\", \"STRING\"),\n",
    "        bigquery.SchemaField(\"embedding\", \"FLOAT\", mode=\"REPEATED\")\n",
    "    ]\n",
    "    \n",
    "    # Define the table reference\n",
    "    table_ref = client.dataset(dataset_id).table(table_id)\n",
    "    \n",
    "    # Define the table object\n",
    "    table = bigquery.Table(table_ref, schema=schema)\n",
    "    \n",
    "    # Create the table in BigQuery\n",
    "    try:\n",
    "        client.create_table(table)\n",
    "        print(\"Table created successfully.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error creating table: {e}\")\n",
    "\n",
    "def load_data_into_bigquery_table():\n",
    "    job_config = bigquery.LoadJobConfig(\n",
    "        source_format=bigquery.SourceFormat.NEWLINE_DELIMITED_JSON, autodetect=False)\n",
    "    \n",
    "    table_ref = client.dataset(dataset_id).table(table_id)\n",
    "    \n",
    "    with open(file_path, \"rb\") as source_file:\n",
    "        job = client.load_table_from_file(source_file, table_ref, job_config=job_config)\n",
    "    \n",
    "    job.result()  # Waits for the job to complete.\n",
    "    \n",
    "    table = client.get_table(table_ref)  # Make an API request.\n",
    "    print(\"Loaded {} rows and {} columns to {}\".format(\n",
    "        table.num_rows, len(table.schema), table_ref))\n",
    "    \n",
    "def read_bq_table_dataframe():  \n",
    "    table_ref = client.dataset(dataset_id).table(table_id)\n",
    "    dataframe = client.list_rows(table_ref).to_dataframe(create_bqstorage_client=True)\n",
    "    \n",
    "    return dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "f55fb03b-7402-415d-b2b0-e4042d854487",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error creating table: 409 POST https://bigquery.googleapis.com/bigquery/v2/projects/my-project-0004-346516/datasets/darryl_video_rag_demo_lh/tables?prettyPrint=false: Already Exists: Table my-project-0004-346516:darryl_video_rag_demo_lh.video_embeddings\n",
      "Loaded 10 rows and 7 columns to my-project-0004-346516.darryl_video_rag_demo_lh.video_embeddings\n"
     ]
    }
   ],
   "source": [
    "# Call the function to create the table\n",
    "# create_bigquery_dataset()\n",
    "create_bigquery_table()\n",
    "load_data_into_bigquery_table()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "ea69d483-fd8d-42af-aec1-5eb75238653d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>start_time</th>\n",
       "      <th>end_time</th>\n",
       "      <th>clip_name</th>\n",
       "      <th>frame_name</th>\n",
       "      <th>description</th>\n",
       "      <th>embedding</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>content/clips/clip_0.mp4</td>\n",
       "      <td>content/frames/frame_0.png</td>\n",
       "      <td>The video starts with the Google I/O logo anim...</td>\n",
       "      <td>[0.023012504, 0.0178418346, -0.0278227255, 0.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>60</td>\n",
       "      <td>120.0</td>\n",
       "      <td>content/clips/clip_1.mp4</td>\n",
       "      <td>content/frames/frame_1.png</td>\n",
       "      <td>The video starts with a man standing on a stag...</td>\n",
       "      <td>[0.0184225999, 0.0259412099, -0.0144431163, 0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>120</td>\n",
       "      <td>180.0</td>\n",
       "      <td>content/clips/clip_2.mp4</td>\n",
       "      <td>content/frames/frame_2.png</td>\n",
       "      <td>The video starts with Sundar Pichai, CEO of Go...</td>\n",
       "      <td>[-0.000537349, -0.0045921793, -0.0360602662, -...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>180</td>\n",
       "      <td>240.0</td>\n",
       "      <td>content/clips/clip_3.mp4</td>\n",
       "      <td>content/frames/frame_3.png</td>\n",
       "      <td>The video starts with a person holding a smart...</td>\n",
       "      <td>[0.0284245517, -0.0345217884, -0.029420957, 0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>240</td>\n",
       "      <td>300.0</td>\n",
       "      <td>content/clips/clip_4.mp4</td>\n",
       "      <td>content/frames/frame_4.png</td>\n",
       "      <td>The video starts with a montage of images, inc...</td>\n",
       "      <td>[0.0156923328, -0.0062827175, -0.0356400497, 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>300</td>\n",
       "      <td>360.0</td>\n",
       "      <td>content/clips/clip_5.mp4</td>\n",
       "      <td>content/frames/frame_5.png</td>\n",
       "      <td>The video starts with a woman with short curly...</td>\n",
       "      <td>[0.0248258356, -0.0402916409, -0.0059038997, 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>360</td>\n",
       "      <td>420.0</td>\n",
       "      <td>content/clips/clip_6.mp4</td>\n",
       "      <td>content/frames/frame_6.png</td>\n",
       "      <td>The video starts with a woman standing on a st...</td>\n",
       "      <td>[0.0603798144, -0.0208670031, -0.0023623984, -...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>420</td>\n",
       "      <td>480.0</td>\n",
       "      <td>content/clips/clip_7.mp4</td>\n",
       "      <td>content/frames/frame_7.png</td>\n",
       "      <td>The video starts with a woman standing on a st...</td>\n",
       "      <td>[0.0107160276, 0.0019427086, -0.0039142366, 0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>480</td>\n",
       "      <td>540.0</td>\n",
       "      <td>content/clips/clip_8.mp4</td>\n",
       "      <td>content/frames/frame_8.png</td>\n",
       "      <td>The video is about Google's new AI model, Gemi...</td>\n",
       "      <td>[-0.0237723123, -0.0174860042, -0.0038281605, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>540</td>\n",
       "      <td>598.05</td>\n",
       "      <td>content/clips/clip_9.mp4</td>\n",
       "      <td>content/frames/frame_9.png</td>\n",
       "      <td>The video starts with a man with glasses, wear...</td>\n",
       "      <td>[0.003135087, -0.0236313213, -0.0164693557, 0....</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  id start_time end_time                 clip_name  \\\n",
       "0  0          0     60.0  content/clips/clip_0.mp4   \n",
       "1  1         60    120.0  content/clips/clip_1.mp4   \n",
       "2  2        120    180.0  content/clips/clip_2.mp4   \n",
       "3  3        180    240.0  content/clips/clip_3.mp4   \n",
       "4  4        240    300.0  content/clips/clip_4.mp4   \n",
       "5  5        300    360.0  content/clips/clip_5.mp4   \n",
       "6  6        360    420.0  content/clips/clip_6.mp4   \n",
       "7  7        420    480.0  content/clips/clip_7.mp4   \n",
       "8  8        480    540.0  content/clips/clip_8.mp4   \n",
       "9  9        540   598.05  content/clips/clip_9.mp4   \n",
       "\n",
       "                   frame_name  \\\n",
       "0  content/frames/frame_0.png   \n",
       "1  content/frames/frame_1.png   \n",
       "2  content/frames/frame_2.png   \n",
       "3  content/frames/frame_3.png   \n",
       "4  content/frames/frame_4.png   \n",
       "5  content/frames/frame_5.png   \n",
       "6  content/frames/frame_6.png   \n",
       "7  content/frames/frame_7.png   \n",
       "8  content/frames/frame_8.png   \n",
       "9  content/frames/frame_9.png   \n",
       "\n",
       "                                         description  \\\n",
       "0  The video starts with the Google I/O logo anim...   \n",
       "1  The video starts with a man standing on a stag...   \n",
       "2  The video starts with Sundar Pichai, CEO of Go...   \n",
       "3  The video starts with a person holding a smart...   \n",
       "4  The video starts with a montage of images, inc...   \n",
       "5  The video starts with a woman with short curly...   \n",
       "6  The video starts with a woman standing on a st...   \n",
       "7  The video starts with a woman standing on a st...   \n",
       "8  The video is about Google's new AI model, Gemi...   \n",
       "9  The video starts with a man with glasses, wear...   \n",
       "\n",
       "                                           embedding  \n",
       "0  [0.023012504, 0.0178418346, -0.0278227255, 0.0...  \n",
       "1  [0.0184225999, 0.0259412099, -0.0144431163, 0....  \n",
       "2  [-0.000537349, -0.0045921793, -0.0360602662, -...  \n",
       "3  [0.0284245517, -0.0345217884, -0.029420957, 0....  \n",
       "4  [0.0156923328, -0.0062827175, -0.0356400497, 0...  \n",
       "5  [0.0248258356, -0.0402916409, -0.0059038997, 0...  \n",
       "6  [0.0603798144, -0.0208670031, -0.0023623984, -...  \n",
       "7  [0.0107160276, 0.0019427086, -0.0039142366, 0....  \n",
       "8  [-0.0237723123, -0.0174860042, -0.0038281605, ...  \n",
       "9  [0.003135087, -0.0236313213, -0.0164693557, 0....  "
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataframe = read_bq_table_dataframe()\n",
    "dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80e9e4b2-fbbe-4ab1-b4bf-8e1202866111",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d72b383-eff1-4ad1-b3fd-125b6074fe88",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d26a3037-b981-4cc0-b7c7-bee5e66c00e1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3633bbe8-a90b-4f41-92a0-70ce3ef992cf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fc537471-3163-44d3-83b2-3212977e7419",
   "metadata": {},
   "source": [
    "## Querying using Gemini Model instead of Vector Search"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f1db7cd-19c1-4134-bc3c-28647eac4501",
   "metadata": {},
   "source": [
    "### Defining Functions for Gemini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "56839314-719c-4171-8c36-0baaa1e20905",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import vertexai\n",
    "from vertexai.preview.generative_models import GenerativeModel, Part\n",
    "from vertexai.preview.language_models import TextGenerationModel\n",
    "\n",
    "def generate(input_prompt):\n",
    "    model = GenerativeModel(\"gemini-ultra\")\n",
    "    responses = model.generate_content(\n",
    "        input_prompt ,\n",
    "    generation_config={\n",
    "        \"max_output_tokens\": 2048,\n",
    "        \"temperature\": 0.2,\n",
    "        \"top_p\": 1,\n",
    "        \"top_k\": 32\n",
    "    },\n",
    "        safety_settings=[],\n",
    "        stream=True,\n",
    "    )\n",
    "    \n",
    "    all_response  = []\n",
    "    \n",
    "    for response in responses:\n",
    "        # print(response.text, end=\"\")\n",
    "        all_response.append(response.text)\n",
    "    \n",
    "    return(\" \".join(all_response))\n",
    "    \n",
    "\n",
    "def generate_pro(input_prompt):\n",
    "    model = GenerativeModel(\"gemini-pro\")\n",
    "    responses = model.generate_content(\n",
    "    input_prompt,\n",
    "    generation_config={\n",
    "        \"max_output_tokens\": 2048,\n",
    "        \"temperature\": 0.2,\n",
    "        \"top_p\": 1\n",
    "    },stream=True,)\n",
    "    \n",
    "    all_response  = []\n",
    "    \n",
    "    for response in responses:\n",
    "        all_response.append(response.text)\n",
    "\n",
    "    return(\" \".join(all_response))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7df08a0c-434b-4040-a830-b3d775ce774a",
   "metadata": {},
   "source": [
    "### Query using Gemini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "ed3ccae4-1c2f-4a67-b11d-ac7dc50fa206",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Insert with your own query\n",
    "query = \"In which clip is there a dog?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "7a7a30fb-e0eb-4847-997e-065114ebca8b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def combine_column_to_string(df, column_name):\n",
    "\n",
    "    column_values = df[column_name].tolist()\n",
    "    combined_string = ', '.join(column_values)\n",
    "    return combined_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "0737729f-cbff-4d4c-9f1a-186ffc6d14a7",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your prompt: \n",
      " You are an expert in screening videos and understanding the context and contents of the video.\n",
      "Only answer based on the contents of the video provided here: \n",
      " The video starts with the Google I/O logo animation, transitioning to a man walking on a stage in front of a large audience.\n",
      "\n",
      "The man on the stage says: \"At Google, we are fully in our Gemini era. Today, all of our 2 billion user products use Gemini. Gemini 1.5 Pro is available today in Workspace Labs. Let's see how this comes to life with Google Workspace. People are always searching their emails in Gmail. We are working to make it much more powerful with Gemini. Now, you can ask Gemini to summarize all recent emails from the school. Maybe you were traveling this week and you couldn't make the PTA meeting. The recording of the meeting is an hour long. If it's from Google Meet, you can ask Gemini to give you the highlights. People love using photos to search across their life. With Gemini, we're making that a whole lot easier. And ask photos can also help you search your memories in a deeper way. For example, you might be reminiscing...\"\n",
      "\n",
      "The video ends before the man finishes his sentence.\n",
      "\n",
      "## Subject Descriptions:\n",
      "\n",
      "**Man on Stage:**\n",
      "\n",
      "* **Name:** Not mentioned\n",
      "* **Appearance:**  He appears to be middle-aged, with short dark hair, a short beard, and is wearing glasses. He is dressed casually in a dark gray button-down shirt and blue jeans.\n",
      "* **Emotion:** He appears enthusiastic and passionate about the topic he is presenting.\n",
      "* **Actions:** He walks confidently onto the stage, gestures with his hands while speaking, and maintains eye contact with the audience.\n",
      "\n",
      "**Audience:**\n",
      "\n",
      "* **Appearance:** A large crowd of people of various ages and ethnicities. Many are holding up their phones, presumably taking pictures or videos.\n",
      "* **Emotion:** The audience appears engaged and excited, clapping and cheering as the man speaks.\n",
      "\n",
      "**Note:** The video transitions to screen recordings of the features being described, but no other individuals are visible. \n",
      ", The video starts with a man standing on a stage in front of a large screen. The screen displays four photos of a young girl. The man begins to speak about his daughter, Lucia, and her early milestones. He then transitions to talking about the Google Photos app and how users can ask it questions about their photos. A simulation of the app is shown on the screen, where a user types in the question, \"show me how Lucia's swimming has progressed.\" The app then displays a collection of photos and videos of Lucia swimming. The man explains that this feature, called Gemini, goes beyond a simple search by recognizing different contexts and packaging the photos together in a summary.\n",
      "\n",
      "The scene then shifts to a different stage with a different screen and a different audience. The man on stage is the same as before. He continues talking about Gemini, explaining that it is multimodal from the ground up and has all modalities built in. He mentions that they have been gradually rolling out Gemini 1.5 Pro with long context and preview over the last few months. The screen behind him displays a grid of icons representing different modalities. The man announces that they are expanding the context window to 2 million tokens, which elicits applause from the audience.\n",
      "\n",
      "The scene changes back to the first stage with the first screen. The man recaps the two technical advances they have discussed: multimodality and long context. He emphasizes that while each is powerful on its own, together they unlock deeper capabilities and more intelligence. The screen displays the words \"Multimodality\" and \"Long Context\" alongside their respective icons. The man concludes by asking, \"What if we could go even further?\"\n",
      "\n",
      "**Detailed Descriptions:**\n",
      "\n",
      "**Man on Stage:**\n",
      "\n",
      "* **Appearance:** The man has short, dark hair with a receding hairline and a short, neatly trimmed beard. He is wearing rectangular eyeglasses with dark frames. He is dressed in a casual, long-sleeved shirt with two front pockets and buttons. The shirt is a dark gray-green color.\n",
      "* **Emotion:** The man appears to be enthusiastic and passionate about the topic he is presenting. He speaks with a clear voice and maintains eye contact with the audience. He gestures with his hands to emphasize his points.\n",
      "* **Actions:** The man stands confidently on stage, addressing the audience. He uses hand gestures while speaking and occasionally pauses for emphasis.\n",
      "\n",
      "**Photos on Screen:**\n",
      "\n",
      "* **Photo 1:** A young girl with dark hair and a pink dress is standing in a room, smiling and reaching out with her hand.\n",
      "* **Photo 2:** The same girl, wearing a white sun hat, is being held by an adult as she stands in shallow water at the beach.\n",
      "* **Photo 3:** The girl is sitting at a table, smiling and looking at a plate with a cupcake on it. She is wearing a white shirt.\n",
      "* **Photo 4:** The girl is partially obscured, but she appears to be playing with other children in a brightly lit room.\n",
      "\n",
      "**Gemini App Simulation:**\n",
      "\n",
      "* **Screen:** The simulation shows a smartphone screen with a white background.\n",
      "* **Text:** The text \"show me how Lucia's swimming has progressed\" is displayed in a blue chat bubble. Below the chat bubble is a text box labeled \"Ask a question.\"\n",
      "* **Photos:** Two photos of Lucia swimming are displayed at the top of the screen. In both photos, she is wearing a colorful swimsuit and swimming goggles.\n",
      "\n",
      "**Audience:**\n",
      "\n",
      "* **Appearance:** The audience consists of a large group of people of various ages, genders, and ethnicities. They are seated in rows of red chairs.\n",
      "* **Emotion:** The audience appears to be engaged and interested in the presentation. They applaud and cheer at certain points.\n",
      "* **Actions:** The audience members are seated and listening attentively to the speaker. Some are taking photos or videos with their smartphones.\n",
      "\n",
      "**Screen Content:**\n",
      "\n",
      "* **Multimodality Icon:** A blue icon with four smaller icons inside: a capital letter \"A,\" a square, a video camera, and a sound wave.\n",
      "* **Long Context Icon:** A blue icon depicting three horizontal layers stacked on top of each other.\n",
      "* **2 Million Tokens:** The number \"2M\" is displayed in large, bold font, followed by the word \"tokens\" in smaller font.\n",
      "\n",
      "**Names Mentioned:**\n",
      "\n",
      "* **Lucia:** The name of the speaker's daughter.\n",
      "* **Gemini:** The name of the Google AI model being discussed. \n",
      ", The video starts with Sundar Pichai, CEO of Google, talking about AI agents. He describes them as intelligent systems that show reasoning, planning, and memory. He says they are able to think multiple steps ahead, work across software and systems, all to get something done on your behalf, and most importantly, under your supervision.\n",
      "\n",
      "The next speaker is a bald man with glasses wearing a blue sweater. He announces exciting new progress about the future of AI assistants that they are calling Project Astra. He says they wanted to build a universal AI agent that can be truly helpful in everyday life. He introduces a video of their prototype, which has two parts, each part captured in a single take in real-time.\n",
      "\n",
      "The video shows a person holding a smartphone in front of a computer screen displaying code. The person asks, \"What does that part of the code do?\" A moment later, a response appears on the screen, \"This code defines encryption and decryption functions.\" The person asks another question, \"It seems to use AES-CBC encryption to encode and decode data based on a key and an initialization vector (IV)?\"\n",
      "\n",
      "Here are the details of the subjects in the video:\n",
      "\n",
      "**Sundar Pichai:**\n",
      "* **Appearance:** Wears a gray button-down shirt, glasses, and has short, dark hair and a beard.\n",
      "* **Emotion:** Appears calm and confident.\n",
      "* **Actions:** Stands still with his hands clasped in front of him while speaking.\n",
      "\n",
      "**Second Speaker:**\n",
      "* **Appearance:** Bald head, wearing glasses and a blue sweater. He also wears a watch on his left wrist.\n",
      "* **Emotion:** Appears enthusiastic and passionate about the project.\n",
      "* **Actions:** Uses hand gestures while speaking.\n",
      "\n",
      "**Person in the Prototype Video:**\n",
      "* **Appearance:** Only hands are visible, holding a smartphone.\n",
      "* **Emotion:** Unknown.\n",
      "* **Actions:** Holds the smartphone steady, points at the code on the screen, and presumably speaks the questions. \n",
      ", The video starts with a person holding a smartphone in their hands. The phone is in a video call with an AI assistant named Gemini. The user asks Gemini if it remembers where they saw their glasses. Gemini responds with \"Yes, I do. Your glasses were on the desk near a red apple.\" The user then laughs and puts the phone down. The video then cuts to a different scene with a dog and a person. The person is petting the dog and holding a stuffed tiger toy. The person asks Gemini for a band name for the duo. Gemini responds with \"Golden Stripes.\" The person thanks Gemini and the video cuts to a man standing on a stage in front of a large screen. The man is talking about a new AI model called Gemini 1.5 Flash. He says that it is a lighter-weight model compared to Pro. It's designed to be fast and cost-efficient to serve at scale, while still featuring multi-modal reasoning capabilities and breakthrough long context. The man then goes on to talk about a new generative video model called Veo. He says that Veo creates high-quality 1080p videos from text, image, and video.\n",
      "\n",
      "**Subject Descriptions:**\n",
      "\n",
      "**User 1:**\n",
      "- Not visible in the video, only their hands are shown.\n",
      "- Emotion: Seemingly amused, as they laugh at Gemini's response.\n",
      "- Actions: Holds a smartphone, asks Gemini a question, laughs, puts the phone down.\n",
      "\n",
      "**Gemini (AI Assistant):**\n",
      "- Not physically present, only their voice and responses are heard through the smartphone.\n",
      "- Emotion: Neutral, helpful.\n",
      "- Actions: Responds to the user's questions accurately.\n",
      "\n",
      "**User 2:**\n",
      "- Not fully visible, only their hands and voice are present.\n",
      "- Emotion: Playful, appreciative.\n",
      "- Actions: Pets a dog, holds a stuffed tiger toy, asks Gemini a question, thanks Gemini.\n",
      "\n",
      "**Dog:**\n",
      "- Breed: Golden Retriever\n",
      "- Appearance: Light golden fur, wearing a collar.\n",
      "- Emotion: Calm, happy.\n",
      "- Actions: Sits calmly, looks at the camera.\n",
      "\n",
      "**Man on Stage:**\n",
      "- Appearance: Bald, wearing glasses, a blue sweater, and a watch.\n",
      "- Emotion: Enthusiastic, excited.\n",
      "- Actions: Stands on a stage, talks about Gemini 1.5 Flash and Veo. \n",
      ", The video starts with a montage of images, including a cityscape at night, a mountain lake, a cityscape during the day, and an alpaca. The title card \"Veo\" is displayed in the center of the screen. The text \"Unedited raw output\" is displayed at the bottom of the screen.\n",
      "\n",
      "The scene then transitions to a smartphone displaying a text prompt that reads: \"A fast-tracking shot through a bustling dystopian sprawl with bright neon signs, flying cars and mist, night, lens flare, volumetric lighting\". The text \"Create again\" is displayed below the prompt.\n",
      "\n",
      "The scene then transitions to a computer screen displaying the same text prompt and a video player playing a video generated from the prompt. The video shows a fast-tracking shot through a futuristic city at night with bright neon signs and flying cars. The video is 40 seconds long. The text \"Extend\" is displayed below the video player.\n",
      "\n",
      "The scene then transitions to a large audience clapping in a large auditorium.\n",
      "\n",
      "The scene then transitions to Sundar Pichai, the CEO of Google, standing on a stage in front of a large screen. He is wearing a gray shirt and has his hands clasped in front of him. He says: \"For 25 years we have invested in world-class technical infrastructure. Today, we are excited to announce the sixth generation of TPUs called Trillium.\"\n",
      "\n",
      "The scene then transitions to a man standing on a stage in front of a large screen. He is wearing a gray shirt and blue jeans. The screen displays the word \"Trillium\" and the text \"6th generation TPUs\". He says: \"Trillium delivers a 4.7x improvement in compute performance per chip over the previous generation.\"\n",
      "\n",
      "The scene then transitions back to Sundar Pichai. He says: \"Google Search is Generative AI at the scale of human curiosity and it's our most exciting chapter of Search yet.\"\n",
      "\n",
      "The scene then transitions to a large audience clapping in an outdoor auditorium.\n",
      "\n",
      "The scene then transitions to a woman standing in front of a blue wall. She is wearing a black blazer and a purple shirt. She says: \"All the advancements you'll see today are made possible by a new Gemini model customized for Google Search. What really sets this apart is our three unique strengths. This is Search in the Gemini era. By the end of the year AI overviews will come to over a billion people. We're making AI overviews even more helpful for your most...\"\n",
      "\n",
      "**Detailed Descriptions of Subjects:**\n",
      "\n",
      "**Sundar Pichai:**\n",
      "* **Appearance:**  Wearing a gray button-down shirt, glasses, short, dark hair, and a trimmed beard.\n",
      "* **Emotion:**  Appears to be excited and enthusiastic.\n",
      "* **Actions:**  Standing on a stage, gesturing with his hands while speaking.\n",
      "\n",
      "**Man on Stage (Trillium):**\n",
      "* **Appearance:** Wearing a gray button-down shirt, blue jeans, glasses, short, dark hair, and a trimmed beard.\n",
      "* **Emotion:**  Appears to be confident and informative.\n",
      "* **Actions:**  Standing on a stage, gesturing with his hands while speaking.\n",
      "\n",
      "**Woman in Black Blazer:**\n",
      "* **Appearance:** Wearing a black blazer, a purple shirt, shoulder-length, dark, curly hair.\n",
      "* **Emotion:**  Appears to be passionate and informative.\n",
      "* **Actions:**  Standing in front of a blue wall, clasping her hands in front of her while speaking.\n",
      "\n",
      "**Audiences:**\n",
      "* **Appearance:** Diverse groups of people of various ethnicities and ages.\n",
      "* **Emotion:**  Appear to be engaged, excited, and impressed.\n",
      "* **Actions:**  Clapping enthusiastically. \n",
      ", The video starts with a woman with short curly brown hair wearing a black blazer and a purple shirt. She is standing in front of a white wall with a blue stripe at the top. She is looking at the camera and talking with her hands clasped in front of her. \n",
      "\n",
      "She says: \"complex questions\"\n",
      "\n",
      "The video then cuts to a white background with a Google search bar in the center. The search bar is populated with the following text: \"find the best yoga or pilates studios in boston and show details on their intro offers and walking time from beacon hill\"\n",
      "\n",
      "The woman from the previous shot speaks again, this time off-screen: \"The type that are really more like 10 questions in one.\"\n",
      "\n",
      "The video cuts back to the Google search bar, which is now at the top of a smartphone screen. The rest of the screen is blurred, but text can be seen that reads \"Generating...\" and \"Searching the web...\".\n",
      "\n",
      "The woman off-screen continues: \"You can ask your entire question with all its sub-questions and get an AI overview in seconds.\"\n",
      "\n",
      "The video transitions to a different woman with black hair wearing a denim jacket standing behind a turntable. She is holding a smartphone in her left hand and gesturing with her right hand. The smartphone screen is visible and shows a video of the turntable playing a record. The text \"Speak now to ask about this video\" is displayed at the bottom of the screen.\n",
      "\n",
      "The woman behind the turntable says: \"Why will this not stay in place?\"\n",
      "\n",
      "The video cuts back to the first woman, who is now standing on a stage in front of a large screen. The screen is black with a smaller screen inside of it. The smaller screen shows a video of a man in a white shirt bending over. The woman is looking at the camera and smiling.\n",
      "\n",
      "She says: \"I'm really excited to share that soon you'll be able to ask questions with video.\"\n",
      "\n",
      "The video cuts back to the woman with the smartphone. The text on the screen now reads \"Why will this not stay in place\" in the search bar at the top. Below that, the text \"An Audio Technica LP120 tonearm may move freely if it's unbalanced. To balance the tonearm, you can try these steps:\" is visible.\n",
      "\n",
      "The woman with the smartphone says: \"And in a near instant, Google gives me an AI overview. I get some reasons this might be happening and steps I can take to troubleshoot.\"\n",
      "\n",
      "The video cuts to a third woman with dark hair wearing a pink blazer and a white shirt. She is standing in front of the same white and blue background as the first woman. She is looking at the camera and smiling.\n",
      "\n",
      "She says: \"Since last May, we've been hard at work making Gemini for Workspace even more helpful for businesses and consumers across the world.\"\n",
      "\n",
      "The video cuts to the same woman standing on a stage in front of a large screen. The screen is white with the text \"Gemini for Workspace\" in the center.\n",
      "\n",
      "The woman on stage continues: \"Now, I can simply type out my question right here in the mobile card and say something like 'compare my roof repair bids by price and availability'.\"\n",
      "\n",
      "The video shows a close-up of a smartphone screen. The text \"compare my roof repair bids by price and availability\" is visible in a text box at the bottom of the screen.\n",
      "\n",
      "The woman on stage says: \"This new Q&A feature makes it so easy to get quick answers on anything in my inbox.\"\n",
      "\n",
      "The video cuts to a fourth woman with black hair wearing a beige blazer and a white shirt. She is standing in front of the same white and blue background as the other women. She is looking at the camera and talking.\n",
      "\n",
      "She says: \"Today we'll show you how Gemini...\"\n",
      "\n",
      "The video ends.\n",
      "\n",
      "**Detailed Descriptions of Subjects:**\n",
      "\n",
      "**Woman 1:**\n",
      "* Appearance: Short curly brown hair, wearing a black blazer and a purple shirt.\n",
      "* Emotion: Appears confident and excited.\n",
      "* Actions: Stands with her hands clasped in front of her, talks to the camera.\n",
      "\n",
      "**Woman 2:**\n",
      "* Appearance: Black hair, wearing a denim jacket.\n",
      "* Emotion: Appears curious and inquisitive.\n",
      "* Actions: Stands behind a turntable, holds a smartphone, asks a question using Google Lens.\n",
      "\n",
      "**Woman 3:**\n",
      "* Appearance: Dark hair, wearing a pink blazer and a white shirt, later wearing blue jeans and white sneakers.\n",
      "* Emotion: Appears enthusiastic and helpful.\n",
      "* Actions: Stands on a stage, talks to the camera, gestures with her hands.\n",
      "\n",
      "**Woman 4:**\n",
      "* Appearance: Black hair, wearing a beige blazer and a white shirt.\n",
      "* Emotion: Appears professional and informative.\n",
      "* Actions: Stands in front of a white and blue background, talks to the camera.\n",
      "\n",
      "**Other Subjects:**\n",
      "\n",
      "* **Google Search Bar:** Displays various search queries throughout the video.\n",
      "* **Smartphone Screen:** Shows Google search results, Google Lens interface, and a messaging app.\n",
      "* **Turntable:** A black turntable with a record playing on it.\n",
      "* **Large Screen:** Displays a video of a man bending over, then the \"Gemini for Workspace\" logo.\n",
      ", The video starts with a woman standing on a stage in front of a large screen. She is wearing a light brown blazer over a white shirt and black pants. She is smiling and gesturing with her hands as she speaks.\n",
      "\n",
      "**Woman:** AI is delivering our most intelligent AI experience. We're rolling out a new feature that lets you customize it for your own needs and create personal experts on any topic you want. We're calling these Gems. They're really simple to set up. Just tap to create a Gem, write your instructions once, and come back whenever you need it. Starting today, Gemini Advanced subscribers get access to Gemini 1.5 Pro with 1 million tokens. That is the longest context window of any chatbot in the world. You can upload a PDF up to 1,500 pages long or multiple files to get insights across a project. Now, we all know that chatbots can give you ideas for your next vacation, but there's a lot more that goes into planning a great trip. It requires reasoning that considers space-time logistics and the intelligence...\n",
      "\n",
      "The screen behind her changes to show a blue and purple four-pointed star on a black background. The word \"Gems\" appears in a gradient of blue and purple. A white pill-shaped button with the text \"+ Create Gem\" appears and is pressed, turning blue. A dark gray box with rounded corners appears with the title \"New Gem\" in the top left corner. It has a text field for \"Name\" and a larger text field for \"Instructions\" with the text \"I'd like you to act as a storyteller who specializes in crafting short stories with mysterious twists\" already entered. Below the \"Instructions\" field is a light gray button with the text \"Generate instructions\". The screen then transitions to a grid pattern with three squares in a gradient of blue and purple appearing in different locations on the grid.\n",
      "\n",
      "The woman is then shown standing on a stage in front of a large screen. The screen behind her changes to a black background with the blue and purple four-pointed star. The text \"Gemini 1.5 Pro\" appears to the right of the star in a gradient of blue and purple. The screen then transitions to a black background with the text \"Tokens\" in white at the top. Below the text are four rectangles, each with a different color outline and text inside. The top rectangle has a blue outline, is mostly filled with a pink bar, and has the text \"1M Gemini Advanced\" inside. The second rectangle has a white outline and the text \"200K Claude 3\" inside. The third rectangle has a white outline and the text \"128K GPT-4\" inside. The bottom rectangle has a white outline and the text \"32K Gemini app\" inside.\n",
      "\n",
      "The screen behind the woman changes to show a graphic of a document with the text \"1,500 pages\" below it. The graphic then changes to show four documents stacked on top of each other. The audience is then shown clapping their hands.\n",
      "\n",
      "The woman is then shown standing on a stage in front of a large screen. The screen behind her changes to show a grid pattern with three orange squares appearing in different locations on the grid.\n",
      "\n",
      "**Woman:** ...Now, we all know that chatbots can give you ideas for your next vacation, but there's a lot more that goes into planning a great trip. It requires reasoning that considers space-time logistics and the intelligence...\n",
      "\n",
      "The screen behind her changes to show a collection of images and icons, including a map pin, an airplane, a hotel bed, and various cityscapes.\n",
      "\n",
      "The video ends with the woman still speaking.\n",
      "\n",
      "**Subject Descriptions:**\n",
      "\n",
      "**Woman:** The woman appears to be in her 40s and has long, dark hair. She is of Asian descent. She is wearing a light brown blazer over a white shirt and black pants. She appears to be confident and enthusiastic about the product she is presenting. She speaks clearly and concisely. \n",
      ", The video starts with a woman standing on a stage in front of a large screen. The screen displays five images: a location pin icon, a cityscape, an airplane icon, a sunset over water, and a building.\n",
      "\n",
      "The woman is replaced by a different woman standing in front of a blue and white background. She is wearing a beige blazer over a white shirt. She is smiling and gesturing with her hands as she speaks.\n",
      "\n",
      "**Woman 1:** \"... to prioritize and make decisions. That reasoning and intelligence all come together in the new trip planning experience in Gemini Advanced.\"\n",
      "\n",
      "The woman is replaced by a man with short dark hair and a beard. He is wearing a black hoodie and an orange watch. He is standing in front of the same blue and white background as the previous woman. He is also smiling and gesturing with his hands as he speaks.\n",
      "\n",
      "**Man 1:** \"We've embarked on a multi-year journey to reimagine Android with AI at the core. Now, we're making Gemini context aware so it can anticipate what you're trying to do and provide more helpful suggestions in the moment. Let me show you how this works.\"\n",
      "\n",
      "The man is replaced by a different man with short brown hair. He is wearing a gray shirt and an orange watch. He is standing behind a table with a laptop on it. He is holding a green smartphone in his right hand. A large screen behind him shows the content of the smartphone.\n",
      "\n",
      "**Man 2:** \"So, my friend Pete is asking me if I want to play pickleball this weekend. But I'm new to this pickleball thing and I can bring up Gemini to help with that. Gemini knows I'm looking at a video, so it proactively shows me an 'ask this video' chip. So let me tap on that and now I can ask specific questions about the video. So for example, uh, what is the two bounce rule?\"\n",
      "\n",
      "The man taps on the screen of his smartphone. The large screen behind him shows the question \"what is the two bounce rule\" being typed into the phone.\n",
      "\n",
      "**Man 2:** \"So give it a moment and there. I get a nice distinct answer.\"\n",
      "\n",
      "The large screen shows the phone displaying the answer to the question.\n",
      "\n",
      "**Man 2:** \"Starting with Pixel later this year, we'll be expanding...\"\n",
      "\n",
      "## Subject Descriptions:\n",
      "\n",
      "**Woman 1:**\n",
      "\n",
      "* **Appearance:**  Asian woman with long black hair. She is wearing a beige blazer over a white shirt.\n",
      "* **Emotion:**  Smiling and appears enthusiastic.\n",
      "* **Actions:**  Standing on a stage, gesturing with her hands as she speaks.\n",
      "\n",
      "**Man 1:**\n",
      "\n",
      "* **Appearance:**  Man with short dark hair and a beard. He is wearing a black hoodie and an orange watch.\n",
      "* **Emotion:**  Smiling and appears enthusiastic.\n",
      "* **Actions:**  Standing in front of a blue and white background, gesturing with his hands as he speaks.\n",
      "\n",
      "**Man 2:**\n",
      "\n",
      "* **Appearance:**  Man with short brown hair. He is wearing a gray shirt and an orange watch.\n",
      "* **Emotion:**  Smiling and appears enthusiastic.\n",
      "* **Actions:**  Standing behind a table, holding a green smartphone, and interacting with it.\n",
      "\n",
      "**Other:**\n",
      "\n",
      "* The video features a large screen that displays images and the content of the smartphone being used by Man 2.\n",
      "* The video showcases a new feature called \"Gemini Advanced\" that uses AI to provide helpful suggestions and answer questions based on the user's context.\n",
      ", The video is about Google's new AI model, Gemini Nano.\n",
      "\n",
      "The video starts with a man standing on a stage in front of a large screen. He is talking about the new Gemini Nano model with multimodality. He says that this means your phone can understand the world the way you understand it, not just through text input, but also through sights, sounds, and spoken language.\n",
      "\n",
      "The video then cuts to another man standing on a stage in front of a large screen. He is talking about Gemma, Google's family of open models which are crucial for driving AI innovation and responsibility. He announces that today's newest member, PaliGemma, is their first vision language open model and it's available right now. He is also excited to announce that they have Gemma 2 coming. It's the next generation of Gemma and it will be available in June. He says that in a few weeks they'll be adding a new 27 billion parameter model to Gemma 2. The audience applauds.\n",
      "\n",
      "The video then cuts to a third man standing on a stage in front of a large screen. He is talking about building AI responsibly. He says that to them, building AI responsibly means both addressing the risks and maximizing the benefits.\n",
      "\n",
      "**Subject Descriptions:**\n",
      "\n",
      "**Subject 1:**\n",
      "\n",
      "* **Name:** Unknown\n",
      "* **Appearance:**  A man with short brown hair and a light beard. He is wearing a brown long-sleeved shirt and dark blue jeans. He has a watch on his left wrist and a ring on his left ring finger.\n",
      "* **Emotion:** He appears to be excited and passionate about the topic he is presenting.\n",
      "* **Actions:** He is standing on a stage in front of a large screen, gesturing with his hands as he speaks.\n",
      "\n",
      "**Subject 2:**\n",
      "\n",
      "* **Name:** Unknown\n",
      "* **Appearance:** A man with short blond hair and glasses. He is wearing a dark gray hoodie and dark blue jeans.\n",
      "* **Emotion:** He appears to be excited and proud of the work they are doing.\n",
      "* **Actions:** He is standing on a stage in front of a large screen, gesturing with his hands as he speaks.\n",
      "\n",
      "**Subject 3:**\n",
      "\n",
      "* **Name:** Unknown\n",
      "* **Appearance:** A dark-skinned man with short, shaved hair and glasses. He is wearing a light brown button-down shirt over a black t-shirt.\n",
      "* **Emotion:** He appears to be serious and thoughtful about the topic he is discussing.\n",
      "* **Actions:** He is standing on a stage in front of a large screen, gesturing with his hands as he speaks.\n",
      "\n",
      "**Other Observations:**\n",
      "\n",
      "* The video appears to be filmed at a Google event.\n",
      "* The audience is diverse and engaged in the presentations.\n",
      ", The video starts with a man with glasses, wearing a beige overshirt and a black shirt underneath, standing on a stage in front of a large screen and a seated audience. The screen displays the text \"Building AI responsibly\".\n",
      "\n",
      "The man starts talking about how Google is improving its AI models with an industry-standard practice called \"Red Teaming\". The screen behind him changes to display the text \"Red Teaming\". He explains that this practice involves testing their own models and trying to break them to identify weaknesses.\n",
      "\n",
      "The screen briefly displays a grid with some colored squares, then cuts back to the man.\n",
      "\n",
      "He then introduces \"LearnAlam\", a new family of models based on Gemini and fine-tuned for learning. The screen behind him changes to display the text \"LearnLM\".\n",
      "\n",
      "He gives an example of a new feature on YouTube that uses LearnAlam to make educational videos more interactive. The screen displays a smartphone with an example of the feature. The feature allows users to ask clarifying questions, get helpful explanations, or take a quiz.\n",
      "\n",
      "The video then cuts to a different man with glasses, wearing a gray shirt, standing in front of the same blue background. He speaks about the important progress Google has made in making AI helpful for everyone.\n",
      "\n",
      "The video cuts to a wide shot of a large outdoor amphitheater with a stage and a large screen. The man in the gray shirt is on the stage, speaking to a large audience.\n",
      "\n",
      "The video cuts back to the man in the gray shirt in front of the blue background. He continues speaking, expressing excitement about the possibilities ahead and creating them together. He thanks the audience, waves, smiles, and walks off-screen.\n",
      "\n",
      "The video ends with the Google logo displayed on a white background.\n",
      "\n",
      "**Detailed Descriptions:**\n",
      "\n",
      "**Man 1:**\n",
      "* Appearance: Dark skin, bald head, wearing glasses, a beige overshirt, and a black shirt underneath.\n",
      "* Emotion: He appears enthusiastic and passionate about the topic of AI.\n",
      "* Actions: He speaks to the audience, gesturing with his hands.\n",
      "\n",
      "**Man 2:**\n",
      "* Name: Sundar Pichai (implied, based on context and appearance)\n",
      "* Appearance: Dark skin, short black hair, wearing glasses, and a gray shirt.\n",
      "* Emotion: He appears confident and optimistic about the future of AI.\n",
      "* Actions: He speaks to the audience, gesturing with his hands, waves, and smiles.\n",
      "\n",
      "**Audience:**\n",
      "* Appearance: A diverse group of people of different ages and ethnicities.\n",
      "* Emotion: They appear attentive and engaged with the speakers.\n",
      "* Actions: They are seated in an auditorium and an outdoor amphitheater, listening to the presentations.\n",
      "\n",
      "**Dialogue:**\n",
      "\n",
      "**Man 1:** \"...products for people and society. We're improving our models with an industry-standard practice called Red Teaming, in which we test our own models and try to break them to identify weaknesses. I'm excited to introduce LearnAlam, our new family of models based on Gemini and fine-tuned for learning. Another example is a new feature on YouTube that uses LearnAlam to make educational videos more interactive, allowing you to ask a clarifying question, get a helpful explanation, or take a quiz.\"\n",
      "\n",
      "**Man 2:** \"All of this shows the important progress we have made as we take a bold and responsible approach to making AI helpful for everyone. To everyone here in Shoreline and the millions more watching around the world, here's to the possibilities ahead and creating them together. Thank you.\"\n",
      "The video starts with a white screen and a rectangle and circle in the center. The rectangle is filled with a gradient of yellow, orange, and green. The circle is filled with a gradient of blue, purple, and red. The rectangle and circle move slightly to the left.\n",
      "\n",
      "The screen transitions to a wide shot of a stage at an event. A man in a black shirt and blue jeans is walking across the stage. There is a large screen behind him with the same rectangle and circle logo. The rectangle and circle are now filled with solid colors. The audience is visible in the foreground, clapping and cheering.\n",
      "\n",
      "The screen transitions again to a close-up shot of the man on stage. He is now wearing a gray shirt and blue jeans. He is speaking, but there is no audio.\n",
      "\n",
      "The screen transitions to a white screen with the text \"Gemini\" in blue and pink at the top. Below it, the text \"Integrated in all 2B user products\" is displayed in black.\n",
      "\n",
      "The screen transitions back to the man speaking.\n",
      "\n",
      "The screen transitions again to the man on stage. The screen behind him now displays \"Gemini 1.5 Pro available today in Workspace Labs\" in black text on a white background.\n",
      "\n",
      "The screen transitions back to the man speaking.\n",
      "\n",
      "The screen transitions back to the man on stage. The screen behind him now displays \"Google Workspace\" in blue text on a white background. There are various colorful icons surrounding the text.\n",
      "\n",
      "The screen transitions back to the man speaking.\n",
      "\n",
      "The screen behind the man transitions to a white background with a large \"M\" in the center, designed with the Gmail color scheme of red and white.\n",
      "\n",
      "The screen transitions back to the man speaking.\n",
      "\n",
      "The screen transitions to a computer screen displaying a Gmail inbox. The left side shows a list of emails, while the right side features a sidebar titled \"Gemini\" with a summary of recent emails related to \"Maywood Park Elementary School.\"\n",
      "\n",
      "The screen transitions to a close-up of the Gemini sidebar, highlighting a specific email about a PTA meeting.\n",
      "\n",
      "The screen transitions to a full-screen view of the email, showing the subject \"[Recap] PTA Meeting - May 13\" and the sender \"Gloria Hill.\"\n",
      "\n",
      "The screen transitions to a video player interface, displaying a recorded PTA meeting titled \"2024 PTA Meeting - Maywood Park Elementary School.\" Two individuals, Andrew Teoh and Nicole Westlake, are visible in the video. A sidebar on the right side, labeled \"Gemini,\" offers options to interact with the video, including \"Summarize this video,\" \"Ask a question about this video,\" and \"Show action items from this video.\"\n",
      "\n",
      "The screen transitions to a close-up of the Gemini sidebar, highlighting the \"Summarize this video\" option.\n",
      "\n",
      "The screen transitions to a white background with a grid pattern. Three squares within the grid are filled with gradients of blue and red. The squares move in different directions.\n",
      "\n",
      "The screen transitions back to the man speaking.\n",
      "\n",
      "The screen behind the man transitions to a display of numerous photos scattered across a white background.\n",
      "\n",
      "The screen transitions back to the man speaking.\n",
      "\n",
      "The screen fades to black.\n",
      ", The video starts with a man with glasses standing on a stage in front of a large screen. The screen displays four photos of a young girl. The man is gesturing with his hands as he speaks.\n",
      "\n",
      "The screen then transitions to show a smartphone displaying two photos of a child in a swimming pool. The text \"show me how Lucia's swimming has progressed\" is visible in a chat bubble.\n",
      "\n",
      "The screen fades to black and the text \"show me how Lucia's swimming has progressed\" is displayed in white.\n",
      "\n",
      "A series of photos and videos of the child swimming in various locations are shown in a diagonal pattern against a blue background. The photos and videos are arranged in a timeline, with the oldest at the bottom and the most recent at the top.\n",
      "\n",
      "The photos and videos then assemble into a grid of eight, showcasing the child's swimming progress.\n",
      "\n",
      "The screen transitions back to the smartphone, now displaying the grid of photos and a text summary of the child's swimming journey.\n",
      "\n",
      "The man on stage reappears, partially obscured by a white grid overlay.\n",
      "\n",
      "The screen behind the man displays a blue and white abstract design.\n",
      "\n",
      "The screen transitions to show four icons: \"Aa\" representing text, a square representing images, a video camera representing videos, and a sound wave representing audio.\n",
      "\n",
      "The man is now standing on a different stage, this one outdoors with a large crowd of people seated in front of him. The screen behind him displays the same four icons.\n",
      "\n",
      "The man reappears on the indoor stage, the screen behind him now displaying a pattern of blue and white squares with the letters \"Aa\" inside.\n",
      "\n",
      "The squares part to reveal the number \"2M\" in the center of the screen.\n",
      "\n",
      "The text \"Gemini 1.5 Pro\" appears above the \"2M\", and the word \"tokens\" appears below it.\n",
      "\n",
      "The crowd applauds.\n",
      "\n",
      "The screen behind the man turns black, then a blue and white star-shaped design appears in the center.\n",
      "\n",
      "The screen turns black again, then displays the word \"Multimodality\" on the left and \"Long Context\" on the right. The word \"Multimodality\" is accompanied by the four icons previously shown. The words \"Long Context\" are accompanied by an illustration of three stacked layers.\n",
      "\n",
      "The four icons move towards the illustration of stacked layers, then disappear as the screen fades to black once more.\n",
      "\n",
      "The blue and white star-shaped design reappears on the screen.\n",
      "\n",
      "The man reappears on stage, partially obscured by the white grid overlay.\n",
      "\n",
      "The video ends with the man speaking to the camera against a blue background.\n",
      ", - A man with glasses and a gray shirt is standing in front of a blue and white background.\n",
      "- The screen behind him shows the word \"Agents\" in blue letters.\n",
      "- The screen then shows the words \"Reasoning, planning, memory\" in green, yellow, and blue letters, respectively.\n",
      "- The screen then shows the words \"Think multiple steps ahead\" in yellow letters.\n",
      "- The screen then shows the words \"Work across software and systems\" in orange letters.\n",
      "- The screen then shows the words \"Under your supervision\" in gray letters.\n",
      "- A man with glasses and a blue shirt is standing in front of a blue and white background.\n",
      "- The screen behind him is black with the words \"Project Astra\" in white letters.\n",
      "- The screen then shows the words \"A universal AI agent helpful in everyday life.\"\n",
      "- A person is holding a smartphone in their hands. The screen of the phone shows a video call with a person on the other end. The person on the other end of the call is asking a question about a piece of code.\n",
      "- The person holding the phone asks the AI assistant on the other end of the call what a specific part of the code does.\n",
      "- The AI assistant responds by explaining that the code defines encryption and decryption functions.\n",
      "- The AI assistant further explains that the code seems to use AES-CBC encryption to encode and decode data based on a key and an initialization vector (IV)., The video starts with a person holding a phone in their hands. The phone is displaying a video call and the person is moving the phone around a room. The phone is then placed down on a desk.\n",
      "The video then cuts to a wide shot of an office. A golden retriever walks into the shot and a person pets it.\n",
      "The video then cuts to a man with glasses standing in front of a blue and white background. He is wearing a blue sweater and a watch.\n",
      "The video then cuts to a man standing on a stage in front of a large screen. The screen displays the text \"Gemini 1.5 Flash\".\n",
      "The video then cuts back to the man with glasses.\n",
      "The video then cuts back to the man on the stage. The screen behind him displays a logo of a tree with the text \"Speed and efficiency\" and four squares with the text \"Multimodal reasoning\".\n",
      "The video then cuts back to the man with glasses.\n",
      "The video then cuts back to the man on the stage. The screen behind him is displaying a video of a city at night.\n",
      "The video then cuts to a black screen with the text \"Veo\" in the center. Surrounding the text are six smaller videos. The videos show a city at night, a lake in the mountains, a llama, a space shuttle, a city street, and a lake.\n",
      "The video then cuts back to the man on the stage., The video starts with a black screen and five images in rounded rectangles with a white text \"Veo\" in the center. The text \"Unedited raw output\" is below the images.\n",
      "Then, a black smartphone is shown with a text prompt on the screen.\n",
      "The next shot is a computer screen with a video playing on the right side. The video shows a car driving on a highway at night.\n",
      "The video is paused, and the cursor clicks on the \"Extend\" button.\n",
      "The next shot is of a large crowd of people sitting in an auditorium. They are all looking up at something.\n",
      "The screen then changes to a grid with three orange squares.\n",
      "A man in a gray shirt and glasses is standing in front of a blue and white background. He is talking.\n",
      "The background changes to a large screen with a white background and a blue border. The screen is blank.\n",
      "The man is now standing on a stage in front of the screen. The screen now shows a diagram of a computer server.\n",
      "The man is still talking.\n",
      "The screen changes to show the word \"Trillium\" in large blue letters.\n",
      "The man is now standing in front of a large screen that says \"Trillium\" on it. There is a crowd of people sitting in front of him.\n",
      "The screen changes to show the text \"Delivers a 4.7x improvement in compute performance per chip over the previous generation.\"\n",
      "The man with glasses is talking again in front of a blue and white background.\n",
      "The background changes to a large screen with a white background and a blue border. The screen shows the text \"Generative AI at the scale of human curiosity\".\n",
      "A man in a blue shirt and jeans is standing on a stage in front of the screen. He is talking.\n",
      "The next shot is of a large crowd of people sitting in an auditorium. They are all looking and clapping.\n",
      "The screen then changes to a grid with three orange squares.\n",
      "A woman in a black blazer and purple shirt is standing in front of a blue and white background. She is talking.\n",
      "The background changes to a black screen with a 3D animation of a network of blue cubes.\n",
      "The animation disappears, and three blue icons with text below them appear on a white background. The icons are: a clock with an arrow, a stack of three squares, and a star. The text below the icons reads: \"Real-time Information\", \"Ranking and Quality Systems\", and \"New Agentive Capabilities\".\n",
      "The woman in a black blazer and purple shirt is talking again in front of a blue and white background.\n",
      "The background changes to a large screen with a white background and a blue border. The screen shows the text \"Search in the Gemini era\".\n",
      "The woman is now standing on a stage in front of the screen. She is talking.\n",
      "The screen then changes to a grid with a blue rectangle.\n",
      "The woman in a black blazer and purple shirt is talking again in front of a blue and white background.\n",
      "The background changes to a white screen with the text \"Coming to over 1B people by end of year\".\n",
      "The woman in a black blazer and purple shirt is talking again in front of a blue and white background.\n",
      "The video ends.\n",
      ", The video starts with a woman in a black blazer and a purple shirt talking.\n",
      "Then, a white search bar appears with the text \"find the best yoga or pilates studios in boston and show details on their intro offers and walking ti\" written inside. The text \"ti\" then changes to \"time from beacon hill\".\n",
      "The search bar is then shown inside a phone as part of the Google search engine. The search results then appear.\n",
      "The video cuts back to the woman in the black blazer.\n",
      "The video then cuts to a woman standing on a stage in front of a large screen. The screen shows a video of a man doing push-ups with a soccer ball on his back.\n",
      "The video then cuts to a split screen. On the left side, a woman with black hair is standing in front of a record player. On the right side, a phone is shown with the text \"Speak now to ask about this video\" written on the screen. The woman then holds the phone up to the record player.\n",
      "The text \"Why will this not stay in place\" then appears on the phone screen.\n",
      "The phone screen then shows search results from Google.\n",
      "The video cuts back to the woman on the stage.\n",
      "The video cuts back to the woman in the black blazer.\n",
      "The video cuts to a woman in a pink blazer standing on a stage in front of a large screen. The screen is white with a blue and purple logo in the center. The logo then changes to say \"Gemini for Workspace\".\n",
      "The video cuts back to the woman in the pink blazer.\n",
      "A phone screen then appears with a text box that says \"Enter a prompt here\". The text \"compare my roof repair bids by price and availability\" is then typed into the text box.\n",
      "The video cuts back to the woman in the pink blazer, who is now standing on a different stage. The screen behind her shows a phone with the text \"compare my roof repair bids by price and availability\" written on it.\n",
      "The video cuts back to the woman in the pink blazer.\n",
      "The video cuts to a woman in a beige blazer talking. \n",
      ", The video starts with a woman in a beige suit jacket standing on a stage in front of a large screen. The screen is black with a blue and pink four-pointed star in the center. \n",
      "\n",
      "The screen then transitions to a grid pattern with blue, pink, and purple squares.\n",
      "\n",
      "The screen changes back to the black screen with the four-pointed star, and the words \"Gemini 1.5 Pro\" appear in white text below the star.\n",
      "\n",
      "The screen then changes to a black background with the word \"Tokens\" in white text at the top. Below the word \"Tokens\" is a horizontal bar with a blue outline and a pink outline that fills the bar from left to right. The bar is labeled \"Gemini Advanced\" on the right side. Below the bar are three more bars with the same design, but they are not filled. The second bar is labeled \"Claude 3\", the third bar is labeled \"GPT-4\", and the fourth bar is labeled \"Gemini app\".\n",
      "\n",
      "The screen then changes back to the woman in the beige suit jacket. She is now standing in front of a large screen that is displaying a grid of images. The images are of various sizes and shapes, and they are arranged in a random order. Some of the images are of people, some are of places, and some are of things.\n",
      "\n",
      "The video ends with the screen displaying a grid pattern with orange squares. \n",
      ", The video starts with a woman standing on a stage in front of a large screen. The screen displays five images: a location pin icon, a cityscape, an airplane icon, a sunset, and a building. The woman is cut off at the knees by the bottom of the screen.\n",
      "\n",
      "The scene changes to a woman with long black hair wearing a beige blazer and a white shirt. She is standing in front of a blue and white background.\n",
      "\n",
      "The scene changes to a man with short black hair and a beard wearing a black hoodie over a white shirt. He is standing in front of the same blue and white background. He is wearing an orange watch on his left wrist and a beaded bracelet on his right.\n",
      "\n",
      "The scene changes back to the stage. A man in a gray shirt is standing behind a podium. He is wearing a watch on his left wrist. The screen behind him is white with a colored stripe pattern at the bottom. The screen displays a dialogue box that reads \"Good morning\" and \"Type, talk, or share a photo\". Below the text are three icons: a keyboard, a microphone, and a photo.\n",
      "\n",
      "The scene changes back to the man in the black hoodie.\n",
      "\n",
      "The scene changes back to the stage. The screen now displays a smartphone screen showing a messaging app. The app shows a conversation between two users, \"Pete\" and the viewer. The time is 11:23.\n",
      "\n",
      "The scene changes back to the man in the gray shirt. He is now holding a smartphone in his left hand.\n",
      "\n",
      "The scene changes to show both the man and the phone screen enlarged. The phone screen shows a video titled \"How to Play Pickleball (THE BASICS)\". The video is paused, and the timestamp reads 3:29. Below the video, a text box reads \"Ask this video\".\n",
      "\n",
      "The man taps the screen.\n",
      "\n",
      "The scene changes to show the phone screen again. The \"Ask this video\" text box is now open and displays the text \"What would you like to do today? Type, talk, or share a photo to Gemini Advanced\". Below the text are a microphone icon and a camera icon.\n",
      "\n",
      "The video in the background continues to play.\n",
      "\n",
      "The scene changes to show the phone screen again. The \"Ask this video\" text box is still open. The man types the question \"what is the two bounce rule\" into the text box.\n",
      "\n",
      "The scene changes to show the phone screen again. The video is still playing. A text box at the bottom of the screen reads \"YouTube\".\n",
      "\n",
      "The scene changes to show the phone screen again. The video is still playing. The \"YouTube\" text box has been replaced by a text box that reads \"According to the video, the two bounce rule is standard to serving in pickleball. After the serving is done the pickleball must bounce once on the returner's side then bounce one more time on the server's side before you can start volleying. You can volley the ball from anywhere on the court except for from the kitchen.\"\n",
      "\n",
      "The scene changes back to the man in the gray shirt.\n",
      "\n",
      "The scene changes back to the man in the black hoodie.\n",
      ", The video starts with a man in a grey shirt and blue jeans standing on a stage in front of a large screen. The screen displays the text \"Gemini Nano with Multimodality\" and \"Coming to Pixel later this year\". The man is gesturing with his hands as he speaks.\n",
      "\n",
      "The screen then changes to show a grid of four black rectangles. Below each rectangle is a white icon: a letter \"A\" with a plus sign, a camera with an up arrow, two vertical lines with a right arrow, and a speech bubble with a plus sign.\n",
      "\n",
      "The screen then changes again to show a grid of six rectangles. The rectangles contain a mix of text in a foreign language and images of a person standing in a doorway, a guitar, and a person riding a skateboard.\n",
      "\n",
      "The scene then cuts to a different man in a black sweatshirt and blue jeans standing on a stage in front of a large screen. The screen is black. The man is clapping his hands together.\n",
      "\n",
      "The screen then displays a blue logo that says \"Gemma\".\n",
      "\n",
      "The scene cuts back to the man in the black sweatshirt. He is now standing behind a podium.\n",
      "\n",
      "The screen behind the man changes to display a grid of four rectangles. The rectangles contain images of a microscopic organism, a dog, some flowers, and an aerial view of a city. The text \"PaliGemma\" is displayed above the rectangles.\n",
      "\n",
      "The scene cuts back to the man in the black sweatshirt.\n",
      "\n",
      "The screen behind the man changes to display a white logo that says \"Gemma 2\". Below the logo is a stylized image of a neural network.\n",
      "\n",
      "The text \"27B parameters\" appears to the right of the neural network image.\n",
      "\n",
      "The scene cuts to a wide shot of the audience, who are clapping and cheering.\n",
      "\n",
      "The scene cuts back to the man in the black sweatshirt.\n",
      "\n",
      "The scene cuts to a different man in a tan shirt and black pants standing on a stage in front of a large screen. The screen is black. The man is gesturing with his hands as he speaks.\n",
      "\n",
      "The screen behind the man changes to display the text \"Building AI responsibly\".\n",
      ", The video starts with a man with glasses, a beige shirt, and black pants standing on a stage in front of a large screen and an audience. The screen displays the words \"Building AI responsibly\". The screen then turns black and the words \"Red Teaming\" appear in white. The screen turns white and the words \"LearnLM\" appear in blue and green. The screen then shows a smartphone screen with a chat window open. The chat window has a white background and the text is in black. The chat window is asking the user if they want to learn more about the video they are watching. The user can choose to have the video summarized, learn why the topic is important, or take a quiz. The user selects \"Quiz me about this topic\" and a multiple choice question appears. The screen then changes to a man with glasses and a gray shirt standing in front of a blue and white background. He is clapping his hands together. The camera pans out to show that he is on a stage in front of a large audience seated in red chairs. The screen behind him shows a presentation slide with the words \"Platforms: enabling everyone\" on it. The man waves to the audience. The video ends with a white screen with the Google logo in the center. \n",
      "At Google we are fully in our Gemini era Today all of our 2 billion user products use Gemini Gemini 1.5 Pro is available today in Workspace Labs Let's see how this comes to life with Google Workspace People are always searching their emails in Gmail We're working to make it much more powerful with Gemini Now you can ask Gemini to summarize all recent emails from the school Maybe you were traveling this week and you couldn't make the PTA meeting The recording of the meeting is an hour long If it's from Google Meet you can ask Gemini to give you the highlights People love using photos to search across their life With Gemini we're making that a whole lot easier And Ask Photos can also help you search your memories in a deeper way For example you might be reminiscing \n",
      ", about your daughter Lucia's early milestones You can ask photos show me how Lucia's swimming has progressed show me how Lucia's swimming has progressed Here Gemini goes beyond a simple search recognizing different contexts and photos packages it up all together in a summary Unlocking knowledge across formats is why we built Gemini to be multimodal from the ground up It's one model with all the modalities built in We've been rolling out Gemini 1.5 Pro with long context in preview over the last few months So today we are expanding the context window to 2 million tokens So far we have talked about two technical advances multimodality and long context Each is powerful on its own but together they unlock deeper capabilities and more intelligence But what if you could go even further That's one of the P \n",
      ", the opportunities we see with AI agents I think about them as intelligent systems that show reasoning planning and memory or able to think multiple steps ahead work across software and systems all to get something done on your behalf and most importantly under your supervision Today we have some exciting new progress to share about the future of AI assistants that we're calling Project Astra For a long time we wanted to build a universal AI agent that can be truly helpful in everyday life Here's a video of our prototype which you'll see has two parts Each part was captured in a single take in real time What does that part of the code do This code defines encryption and decryption functions It seems to use AES-CBC encryption to encode and decode data based on a key and an initialization \n",
      ", vector IV Do you remember where you saw my glasses Yes I do Your glasses were on the desk near a red apple Uh give me a band name for this duo Golden Stripes Nice Thanks Gemini Today we're introducing Gemini 1.5 Flash Flash is a lightweight model compared to Pro It's designed to be fast and cost-efficient to serve at scale while still featuring multimodal reasoning capabilities and breakthrough long context There's one more area I'm really excited to share with you Our teams have made some incredible progress in generative video Today I'm excited to announce our newest most capable generative video model called Veo Veo creates high-quality 1080p videos from text image and , video prompts It can capture the details of your instructions in different visual and cinematic styles For 25 years we have invested in world-class technical infrastructure Today we are excited to announce the sixth generation of TPUs called Trillium Trillium delivers a 4.7x improvement in compute performance per chip over the previous generation Google Search is Generative AI at the scale of human curiosity and it's our most exciting chapter of search yet All the advancements you'll see today are made possible by a new Gemini model customized for Google Search But really sets this apart is our three unique strengths This is search in the Gemini era By the end of the year AI overviews will come to over a billion people We're making AI overviews even more helpful for your most \n",
      ", complex questions The types that are really more like 10 questions in one You can ask your entire question with all its sub questions and get an AI overview in seconds I'm really excited to share that soon you'll be able to ask questions with video Why will this not stay in place And in near instant Google gives me an AI overview I get some reasons this might be happening and steps I can take to troubleshoot Since last May we've been hard at work making Gemini for Workspace even more helpful for businesses and consumers across the world Now I can simply type out my question right here in the mobile card and say something like compare my roof repair bids by price and availability This new Q&A feature makes it so easy to get quick answers on anything in my inbox Today we'll show you how Gemini \n",
      ", AI is delivering our most intelligent AI experience We're rolling out a new feature that lets you customize it for your own needs and create personal experts on any topic you want We're calling these Gems They're really simple to set up Just tap to create a Gem write your instructions once and come back whenever you need it Starting today Gemini Advanced subscribers get access to Gemini 1.5 Pro with 1 million tokens That is the longest context window of any chatbot in the world You can upload a PDF up to 1,500 pages long or multiple files to get insights across a project Now we all know that chatbots can give you ideas for your next vacation but there's a lot more that goes into planning a great trip It requires reasoning that considers space time logistics and the intelligence , to prioritize and make decisions That reasoning and intelligence all come together in the new trip planning experience in Gemini Advanced We've embarked on a multi-year journey to reimagine Android with AI at the core Now we're making Gemini context aware So it can anticipate what you're trying to do and provide more helpful suggestions in the moment Let me show you how this works So my friend Pete is asking if I want to play pickleball this weekend But I'm new to this pickleball thing And I can bring up Gemini to help with that Gemini knows I'm looking at a video So it proactively shows me an ask this video chip So let me tap on that And now I can ask specific questions about the video So for example uh what is the two bounce rule So give it a moment and there I get a nice distinct answer Starting with Pixel later this year will be expand \n",
      ", what's possible with our latest model Gemini Nano with multimodality This means your phone can understand the world the way you understand it So not just through text input but also through sights sounds and spoken language Now let's shift gears and talk about Gemma our family of open models which are crucial for driving AI innovation and responsibility Today's newest member PaLiGemma our first vision language open model and it's available right now I'm also excited to announce that we have Gemma 2 coming It's the next generation of Gemma and it will be available in June So in a few weeks we'll be adding a new 27 billion parameter model to Gemma 2 To us building AI responsibly means both addressing the risks and maximizing the benefits \n",
      ", for people and society We're improving our models with an industry standard practice called red teaming in which we test our own models and try to break them to identify weaknesses I'm excited to introduce Learn Alam our new family of models based on Gemini and fine-tuned for learning Another example is a new feature in YouTube that uses Learn Alam to make educational videos more interactive allowing you to ask a clarifying question get a helpful explanation or take a quiz All of this shows the important progress we have made as we take a bold and responsible approach to making AI helpful for everyone To everyone here in Shoreline and the millions more watching around the world Here's to the possibilities ahead and creating them together Thank you The video features Sundar Pichai, CEO of Google, speaking at a Google event. \n",
      "\n",
      "Sundar is a man of Indian descent with short, dark hair and a short beard and mustache. He is wearing glasses and a dark gray button-down shirt with the collar open. He is standing in front of a blue and white background. He appears to be excited and passionate about the topic he is discussing. He speaks with his hands clasped in front of him. \n",
      "\n",
      "The video then cuts to a screen displaying a Google Meet recording of a PTA meeting. The recording shows two people:\n",
      "\n",
      "* **Andrew Yoon:** A man of Asian descent with short, dark hair and glasses. He is wearing a white collared shirt.\n",
      "* **Nicole Westlaw:** A woman with blonde hair, wearing glasses and a black shirt. \n",
      "\n",
      "The video does not show any other people or animals. \n",
      ", The video features a single speaker, a man who appears to be in his 40s with short dark hair, a short beard, and glasses. He is wearing a gray button-down shirt. He is standing on a stage in front of a large screen. He is gesturing with his hands as he speaks. He appears to be giving a presentation about a new product or feature. He seems to be passionate and excited about the topic he is presenting. \n",
      "\n",
      "Behind the speaker, the screen shows various images throughout the video. First, it shows a series of photos of a young girl named Lucia. Lucia has dark hair and is wearing different outfits in each photo. She is smiling in most of the photos. In one photo, she is being held by a man, presumably her father, in a swimming pool. \n",
      "\n",
      "The screen then transitions to show a series of images related to swimming, including photos and text. The images are shown in a rapid, dynamic way. \n",
      "\n",
      "After the images related to swimming, the screen transitions to show a large crowd of people sitting in an auditorium. The people are of various ages and ethnicities. Most of them are looking at the stage and appear to be engaged in the presentation. Some of them are clapping their hands. \n",
      "\n",
      "The screen then goes black for a moment before showing a blue star-shaped animation. The animation expands and transforms into a pattern of blue lines. \n",
      "\n",
      "The screen then transitions back to the speaker, who continues his presentation. \n",
      ", The video features two people.\n",
      "\n",
      "The first person is a man named Sundar Pichai. He has short, dark hair with a receding hairline, a short beard, and a mustache. He is wearing glasses and a dark gray button-down shirt. He is standing in front of a blue and white wall and appears to be giving a presentation. He is looking directly at the camera and speaking with a serious expression. His hands are clasped in front of him.\n",
      "\n",
      "The second person is a man with short, dark hair, a short beard, and a goatee. He is wearing glasses and a navy blue sweater. He is also standing in front of a blue and white wall, looking directly at the camera, and speaking with a serious expression. His hands are moving as he speaks.\n",
      "\n",
      "The video then cuts to a close-up shot of a person's hands holding a smartphone. The person is wearing a white long-sleeved shirt. The smartphone is displaying a video call with a person named Gemini. The background of the video call shows a desk with a laptop, a picture frame with a dog in it, an apple, and a plant. The person in the video call is out of frame, but their voice can be heard. The hands are interacting with the smartphone, moving the screen up and down. \n",
      ", The video starts with a pair of hands holding a smartphone. The hands appear to belong to a light-skinned person. The hands are moving the phone around, showing different angles of a desk. The phone is on a video call with a person named Gemini. Gemini is a computer program, so we can't see them, but we can hear their voice. Gemini sounds calm and helpful. Gemini tells the user that their glasses are on the desk near a red apple. The user then laughs and puts the phone down.\n",
      "\n",
      "The video then cuts to a different scene, this time in an office. A pair of hands enters the frame and starts petting a golden retriever dog. The hands are holding a stuffed tiger toy. The dog is looking at the camera with a neutral expression. The person holding the camera asks Gemini for a band name for the duo. Gemini suggests \"Golden Stripes\". The person thanks Gemini and the dog walks away.\n",
      "\n",
      "The video then cuts to a man standing in front of a blue background. He is bald and wearing glasses and a blue sweater. He is speaking in a clear and concise voice. He is talking about a new product called Gemini 1.5 Flash. He explains that Flash is a lighter-weight model compared to Pro. He says that it is designed to be fast and cost-efficient to serve at scale, while still featuring multimodal reasoning capabilities and breakthrough long context.\n",
      "\n",
      "The video then cuts to the man standing on a stage in front of a large screen. He is wearing a black shirt and gray pants. He is still speaking in a clear and concise voice. He is talking about a new generative video model called Veo. He explains that Veo creates high-quality 1080p videos from text, image, and code. The screen behind him shows a variety of images and videos, including a cityscape at night, a mountain lake, a busy street, a satellite in space, and a llama. \n",
      ", The video starts with a fast-tracking shot through a bustling dystopian sprawl with bright neon signs, flying cars and mist, night, lens flare, volumetric lighting.\n",
      "\n",
      "Then it cuts to a large crowd of people sitting in an auditorium. They are all clapping their hands and looking at something in front of them. Most of them are smiling and appear to be happy.\n",
      "\n",
      "Then it cuts to Sundar Pichai, CEO of Google, standing on a stage in front of a large screen. He is wearing a gray shirt and has short black hair and a beard. He is gesturing with his hands as he speaks. He appears to be excited.\n",
      "\n",
      "Then it cuts to a man standing on a stage in front of a large screen. He is wearing a gray shirt and blue jeans. He is gesturing with his hands as he speaks. He appears to be excited.\n",
      "\n",
      "Then it cuts back to Sundar Pichai.\n",
      "\n",
      "Then it cuts back to the man in the gray shirt and blue jeans.\n",
      "\n",
      "Then it cuts back to Sundar Pichai.\n",
      "\n",
      "Then it cuts to a large crowd of people sitting in an auditorium. They are all clapping their hands and looking at something in front of them. Most of them are smiling and appear to be happy.\n",
      "\n",
      "Then it cuts to a woman standing on a stage in front of a large screen. She is wearing a black blazer over a purple shirt. She is gesturing with her hands as she speaks. She appears to be excited.\n",
      "\n",
      "Then it cuts back to the woman in the black blazer over a purple shirt.\n",
      ", The video starts with a woman with short curly brown hair, wearing a black blazer and a purple shirt. She seems to be giving a presentation and appears to be excited. She is standing in front of a white wall with a blue stripe at the top.\n",
      "\n",
      "The next woman has black hair with bangs and is wearing a blue denim shirt. She is also giving a presentation and is standing behind a turntable. She seems to be curious and asks a question about why something on the turntable won't stay in place.\n",
      "\n",
      "The third woman has dark brown hair and is wearing a pink blazer and blue jeans. She is giving a presentation on a stage with a large screen behind her. She seems to be enthusiastic and is explaining something related to email and getting quick answers.\n",
      "\n",
      "The last woman has black hair and is wearing a beige blazer over a white shirt. She is also giving a presentation and appears to be confident. She is standing in front of a white wall with a blue stripe at the top. \n",
      ", The video features a single person, a woman who appears to be giving a presentation. She is wearing a light brown blazer over a white button-up shirt and dark pants. She has shoulder-length black hair and is wearing light makeup. She appears to be speaking to an audience, gesturing with her hands as she speaks. She appears to be excited and passionate about the topic she is presenting on. Behind her is a large screen with graphics related to her presentation. The audience, visible briefly, is clapping. \n",
      ", The video features three speakers at a Google event.\n",
      "\n",
      "The first speaker is a woman with long black hair wearing a light brown blazer over a white shirt. She is standing on a stage with a large screen behind her. She appears to be excited and is gesturing with her hands as she speaks.\n",
      "\n",
      "The second speaker is a man with short dark hair and a beard. He is wearing a black hoodie and an orange watch. He is standing in front of a blue and white background. He is also excited and uses hand gestures as he speaks.\n",
      "\n",
      "The third speaker is a man with short brown hair. He is wearing a brown t-shirt and an orange watch. He is standing behind a table with a laptop on it. He is explaining something on his phone and appears to be excited. He is also using hand gestures as he speaks. \n",
      "\n",
      "The man speaking is named Pete. He is mentioned by the third speaker when he is talking about a text message he received from Pete asking him to play pickleball. \n",
      ", The video features two men speaking in front of an audience at a technology conference. \n",
      "\n",
      "The first man has short brown hair and is wearing a brown long-sleeved shirt and dark-wash jeans. He wears a watch on each wrist, one black and one orange. He is standing on a stage in front of a large screen. He appears to be in his 40s. He is gesturing with his hands as he speaks, indicating enthusiasm about the topic. The screen behind him shows text and images related to the new technology he is discussing.\n",
      "\n",
      "The second man has short blonde hair and is wearing a dark gray hoodie and dark-wash jeans. He wears glasses and has a smile on his face, suggesting he is excited about the technology he is presenting. He is standing on a stage in front of a large screen. He is gesturing with his hands as he speaks. The screen behind him shows text and images related to the new technology he is discussing. \n",
      "\n",
      "The audience is a diverse group of people of different races and ages. They are sitting in rows of red chairs and are listening attentively to the speakers. Some members of the audience are clapping their hands, showing their appreciation for the presentations. \n",
      ", The video features two speakers at a Google event.\n",
      "\n",
      "The first speaker is a dark-skinned man with a shaved head. He is wearing glasses, a light brown button-up shirt over a black t-shirt, and dark pants. He appears to be excited about the topic he is presenting. He stands in the middle of a stage, gesturing with his hands as he speaks. Behind him is a large screen that displays the topics he is discussing.\n",
      "\n",
      "The second speaker is a brown-skinned man with short, dark hair and a short beard. He is wearing glasses and a gray button-up shirt. He appears to be happy and smiles at the end of his presentation. He stands in the middle of a stage, gesturing with his hands as he speaks. Behind him is a blue and white background. He thanks the audience before leaving the stage. \n",
      "  -- Based on video provided, answer the following query as accurately as possible:\n",
      "In which clip is there a dog?\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "System_Prompts = \"\"\" You are an expert in screening videos and understanding the context and contents of the video.\n",
    "Only answer based on the contents of the video provided here: \n",
    "\"\"\"\n",
    "\n",
    "Question_Prompts = \"\"\" -- Based on video provided, answer the following query as accurately as possible:\n",
    "\"\"\"\n",
    "\n",
    "videoDesc = combine_column_to_string(shots_df,'description') + combine_column_to_string(shots_df,'associated_text') + combine_column_to_string(shots_df,'associated_speech') + combine_column_to_string(shots_df,'associated_object')\n",
    "\n",
    "combined_prompt = System_Prompts + ' ' + videoDesc + ' ' + Question_Prompts + query\n",
    "\n",
    "\n",
    "print(\"Your prompt: \\n\" + combined_prompt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "fdd553d4-4b21-4f5b-a2f5-5b402e760397",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response from AI model: \n",
      "\n",
      "The  dog appears in the clip with the woman wearing a denim jacket and holding a stuffed tiger toy.  \n",
      " \n"
     ]
    }
   ],
   "source": [
    "print(\"Response from AI model: \\n\")\n",
    "print(generate_pro(combined_prompt))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c286cd01-4c9c-4187-9ee5-2aad75b236b4",
   "metadata": {},
   "source": [
    "## The End"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-root-py",
   "name": "workbench-notebooks.m113",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/workbench-notebooks:m113"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel) (Local)",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
